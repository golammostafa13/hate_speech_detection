{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/golammostafa13/hate_speech_detection/blob/main/English_and_Bangla_Hate_Speech_5_fold.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7t8QZNxpcipD"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPiDwjanMA4n",
        "outputId": "f5fffbbf-7587-42c6-b6d9-777eee6c7d66"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-04-23 05:56:42.146742: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "2022-04-23 05:56:42.146870: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/mostafa/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.layers import Embedding,LSTM,Dense,Dropout\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "import string as st\n",
        "import re\n",
        "import nltk\n",
        "stemmer = nltk.SnowballStemmer(\"english\")\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "nltk.download('stopwords')\n",
        "stopword=set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "e-E5FfrBc0Ww",
        "outputId": "23ca1dd7-6ffb-45dd-a980-3c3ff06fda20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_siWNqaks8x"
      },
      "source": [
        "# Dataset Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvodr3XCdNw6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "file = 'english/FinalBalancedDataset.csv'\n",
        "data = pd.read_csv(file, encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "collapsed": true,
        "id": "acT557TFdiDM",
        "outputId": "899b2b3a-f22c-4dbf-f948-117cf4acdbca"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Toxicity</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>@user when a father is dysfunctional and is s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>bihday your majesty</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>#model   i love u take with u all the time in ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>factsguide: society now    #motivation</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  Toxicity                                              tweet\n",
              "0           0         0   @user when a father is dysfunctional and is s...\n",
              "1           1         0  @user @user thanks for #lyft credit i can't us...\n",
              "2           2         0                                bihday your majesty\n",
              "3           3         0  #model   i love u take with u all the time in ...\n",
              "4           4         0             factsguide: society now    #motivation"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYuCj2MaeNwU"
      },
      "outputs": [],
      "source": [
        "data.drop(['Unnamed: 0'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "collapsed": true,
        "id": "NxjdglCCiA8K",
        "outputId": "25a3913e-22e2-4913-95d3-e592544e0062"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<AxesSubplot:xlabel='class', ylabel='count'>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEGCAYAAACkQqisAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAS+klEQVR4nO3df6xfd33f8ecLOwG6wuIkt1lqmzoDa5NDWwNW8Nr9wYKWOEir0wpQopW4LIqZmqxD6iYC2hoayAQqFBEKkdzFjY0obgqlcSt3npWislbkx00JSZwU5c6ExVZ+mDghMNowZ+/98f1c+t3NtXPzsb/fry/3+ZCO7jnv8znnfI5k+aVzzuecb6oKSZJ6vGzSHZAkLV6GiCSpmyEiSepmiEiSuhkikqRuyyfdgXE7++yza82aNZPuhiQtKvfcc8+3q2pqbn3JhciaNWuYnp6edDckaVFJ8q356t7OkiR1M0QkSd0MEUlSN0NEktTNEJEkdTNEJEndDBFJUjdDRJLUzRCRJHVbcm+sn6g3/cedk+6CTkH3/NYVk+6CNBFeiUiSuhkikqRuhogkqdvIQiTJK5LcleTrSfYn+c1WPy/JnUlmkvxBktNb/eVteaatXzO0r/e3+jeSXDxU39RqM0muHdW5SJLmN8orkeeAC6vqZ4H1wKYkG4GPAp+oqtcBTwNXtvZXAk+3+idaO5KsAy4Dzgc2AZ9JsizJMuDTwCXAOuDy1laSNCYjC5Ea+F5bPK1NBVwIfKHVdwCXtvnNbZm2/q1J0uq7quq5qvomMANc0KaZqjpQVT8AdrW2kqQxGekzkXbFcC/wJLAP+J/AM1V1tDU5CKxs8yuBRwHa+u8AZw3X52xzrPp8/diaZDrJ9OHDh0/CmUmSYMQhUlXPV9V6YBWDK4d/OsrjHacf26pqQ1VtmJp6wa87SpI6jWV0VlU9A3wZ+GfAGUlmX3JcBRxq84eA1QBt/T8Enhquz9nmWHVJ0piMcnTWVJIz2vwrgX8JPMQgTN7emm0Bbmvzu9sybf2fV1W1+mVt9NZ5wFrgLuBuYG0b7XU6g4fvu0d1PpKkFxrlZ0/OBXa0UVQvA26tqj9N8iCwK8mHga8BN7f2NwOfTTIDHGEQClTV/iS3Ag8CR4Grq+p5gCTXAHuBZcD2qto/wvORJM0xshCpqvuAN8xTP8Dg+cjc+t8B7zjGvm4AbpinvgfYc8KdlSR18Y11SVI3Q0SS1M0QkSR1M0QkSd0MEUlSN0NEktTNEJEkdTNEJEndDBFJUjdDRJLUzRCRJHUzRCRJ3QwRSVI3Q0SS1M0QkSR1M0QkSd0MEUlSN0NEktTNEJEkdTNEJEndDBFJUjdDRJLUzRCRJHUzRCRJ3ZaPasdJVgM7gXOAArZV1SeTfBC4Cjjcmn6gqva0bd4PXAk8D/xaVe1t9U3AJ4FlwH+tqo+0+nnALuAs4B7gXVX1g1Gdk3Sq+1/X//Sku6BT0Gt+4/6R7XuUVyJHgV+vqnXARuDqJOvauk9U1fo2zQbIOuAy4HxgE/CZJMuSLAM+DVwCrAMuH9rPR9u+Xgc8zSCAJEljMrIQqarHquqv2/x3gYeAlcfZZDOwq6qeq6pvAjPABW2aqaoD7SpjF7A5SYALgS+07XcAl47kZCRJ8xrLM5Eka4A3AHe20jVJ7kuyPcmKVlsJPDq02cFWO1b9LOCZqjo6py5JGpORh0iSHwe+CLy3qp4FbgJeC6wHHgM+PoY+bE0ynWT68OHDL76BJGlBRhoiSU5jECCfq6o/AqiqJ6rq+ar6v8DvMrhdBXAIWD20+apWO1b9KeCMJMvn1F+gqrZV1Yaq2jA1NXVyTk6SNLoQac8sbgYeqqrfHqqfO9TsF4EH2vxu4LIkL2+jrtYCdwF3A2uTnJfkdAYP33dXVQFfBt7ett8C3Daq85EkvdDIhvgCPw+8C7g/yb2t9gEGo6vWMxj2+wjwHoCq2p/kVuBBBiO7rq6q5wGSXAPsZTDEd3tV7W/7ex+wK8mHga8xCC1J0piMLESq6i+BzLNqz3G2uQG4YZ76nvm2q6oD/P3tMEnSmPnGuiSpmyEiSepmiEiSuhkikqRuhogkqZshIknqZohIkroZIpKkboaIJKmbISJJ6maISJK6GSKSpG6GiCSpmyEiSepmiEiSuhkikqRuhogkqZshIknqZohIkroZIpKkboaIJKmbISJJ6maISJK6GSKSpG6GiCSp28hCJMnqJF9O8mCS/Un+faufmWRfkofb3xWtniQ3JplJcl+SNw7ta0tr/3CSLUP1NyW5v21zY5KM6nwkSS80yiuRo8CvV9U6YCNwdZJ1wLXA7VW1Fri9LQNcAqxt01bgJhiEDnAd8GbgAuC62eBpba4a2m7TCM9HkjTHyEKkqh6rqr9u898FHgJWApuBHa3ZDuDSNr8Z2FkDdwBnJDkXuBjYV1VHquppYB+wqa17dVXdUVUF7BzalyRpDMbyTCTJGuANwJ3AOVX1WFv1OHBOm18JPDq02cFWO1794Dz1+Y6/Ncl0kunDhw+f2MlIkn5o5CGS5MeBLwLvrapnh9e1K4gadR+qaltVbaiqDVNTU6M+nCQtGSMNkSSnMQiQz1XVH7XyE+1WFO3vk61+CFg9tPmqVjtefdU8dUnSmIxydFaAm4GHquq3h1btBmZHWG0BbhuqX9FGaW0EvtNue+0FLkqyoj1QvwjY29Y9m2RjO9YVQ/uSJI3B8hHu++eBdwH3J7m31T4AfAS4NcmVwLeAd7Z1e4C3ATPA94F3A1TVkSQfAu5u7a6vqiNt/leBW4BXAn/WJknSmIwsRKrqL4Fjvbfx1nnaF3D1Mfa1Hdg+T30aeP0JdFOSdAJ8Y12S1M0QkSR1M0QkSd0MEUlSN0NEktTNEJEkdTNEJEndFhQiSW5fSE2StLQc92XDJK8Afgw4u31yZPblwVdzjC/mSpKWjhd7Y/09wHuBnwTu4e9D5Fngd0bXLUnSYnDcEKmqTwKfTPLvqupTY+qTJGmRWNC3s6rqU0l+DlgzvE1V7RxRvyRJi8CCQiTJZ4HXAvcCz7fy7E/SSpKWqIV+xXcDsK59aVeSJGDh74k8APyjUXZEkrT4LPRK5GzgwSR3Ac/NFqvqF0bSK0nSorDQEPngKDshSVqcFjo66y9G3RFJ0uKz0NFZ32UwGgvgdOA04H9X1atH1TFJ0qlvoVcir5qdTxJgM7BxVJ2SJC0OL/krvjXwx8DFJ787kqTFZKG3s35paPFlDN4b+buR9EiStGgsdHTWvxqaPwo8wuCWliRpCVvoM5F3j7ojkqTFZ6E/SrUqyZeSPNmmLyZZ9SLbbG9tHxiqfTDJoST3tultQ+ven2QmyTeSXDxU39RqM0muHaqfl+TOVv+DJKe/tFOXJJ2ohT5Y/z1gN4PfFflJ4E9a7XhuATbNU/9EVa1v0x6AJOuAy4Dz2zafSbIsyTLg08AlwDrg8tYW4KNtX68DngauXOC5SJJOkoWGyFRV/V5VHW3TLcDU8Taoqq8ARxa4/83Arqp6rqq+CcwAF7RppqoOVNUPgF3A5jbM+ELgC237HcClCzyWJOkkWWiIPJXkl2evDpL8MvBU5zGvSXJfu921otVWAo8OtTnYaseqnwU8U1VH59TnlWRrkukk04cPH+7stiRproWGyL8B3gk8DjwGvB34lY7j3cTgd0nWt/18vGMfL1lVbauqDVW1YWrquBdQkqSXYKFDfK8HtlTV0wBJzgQ+xiBcFqyqnpidT/K7wJ+2xUPA6qGmq1qNY9SfAs5IsrxdjQy3lySNyUKvRH5mNkAAquoI8IaXerAk5w4t/iKD3ymBwUP7y5K8PMl5wFrgLuBuYG0biXU6g4fvu9uPY32ZwRURwBbgtpfaH0nSiVnolcjLkqyYcyVy3G2TfB54C3B2koPAdcBbkqxn8DHHR4D3AFTV/iS3Ag8yeJnx6qp6vu3nGmAvsAzYXlX72yHeB+xK8mHga8DNCzwXSdJJstAQ+Tjw1SR/2JbfAdxwvA2q6vJ5ysf8j76qbphvn20Y8J556gcYjN6SJE3IQt9Y35lkmsGwWoBfqqoHR9ctSdJisNArEVpoGBySpB96yZ+ClyRpliEiSepmiEiSuhkikqRuhogkqZshIknqZohIkroZIpKkboaIJKmbISJJ6maISJK6GSKSpG6GiCSpmyEiSepmiEiSuhkikqRuhogkqZshIknqZohIkroZIpKkboaIJKmbISJJ6jayEEmyPcmTSR4Yqp2ZZF+Sh9vfFa2eJDcmmUlyX5I3Dm2zpbV/OMmWofqbktzftrkxSUZ1LpKk+Y3ySuQWYNOc2rXA7VW1Fri9LQNcAqxt01bgJhiEDnAd8GbgAuC62eBpba4a2m7usSRJIzayEKmqrwBH5pQ3Azva/A7g0qH6zhq4AzgjybnAxcC+qjpSVU8D+4BNbd2rq+qOqipg59C+JEljMu5nIudU1WNt/nHgnDa/Enh0qN3BVjte/eA89Xkl2ZpkOsn04cOHT+wMJEk/NLEH6+0KosZ0rG1VtaGqNkxNTY3jkJK0JIw7RJ5ot6Jof59s9UPA6qF2q1rtePVV89QlSWM07hDZDcyOsNoC3DZUv6KN0toIfKfd9toLXJRkRXugfhGwt617NsnGNirriqF9SZLGZPmodpzk88BbgLOTHGQwyuojwK1JrgS+BbyzNd8DvA2YAb4PvBugqo4k+RBwd2t3fVXNPqz/VQYjwF4J/FmbJEljNLIQqarLj7HqrfO0LeDqY+xnO7B9nvo08PoT6aMk6cT4xrokqZshIknqZohIkroZIpKkboaIJKmbISJJ6maISJK6GSKSpG6GiCSpmyEiSepmiEiSuhkikqRuhogkqZshIknqZohIkroZIpKkboaIJKmbISJJ6maISJK6GSKSpG6GiCSpmyEiSepmiEiSuhkikqRuEwmRJI8kuT/JvUmmW+3MJPuSPNz+rmj1JLkxyUyS+5K8cWg/W1r7h5NsmcS5SNJSNskrkX9RVeurakNbvha4varWAre3ZYBLgLVt2grcBIPQAa4D3gxcAFw3GzySpPE4lW5nbQZ2tPkdwKVD9Z01cAdwRpJzgYuBfVV1pKqeBvYBm8bcZ0la0iYVIgX89yT3JNnaaudU1WNt/nHgnDa/Enh0aNuDrXasuiRpTJZP6Lj/vKoOJfkJYF+SvxleWVWVpE7WwVpQbQV4zWtec7J2K0lL3kSuRKrqUPv7JPAlBs80nmi3qWh/n2zNDwGrhzZf1WrHqs93vG1VtaGqNkxNTZ3MU5GkJW3sIZLkHyR51ew8cBHwALAbmB1htQW4rc3vBq5oo7Q2At9pt732AhclWdEeqF/UapKkMZnE7axzgC8lmT3+71fVf0tyN3BrkiuBbwHvbO33AG8DZoDvA+8GqKojST4E3N3aXV9VR8Z3GpKksYdIVR0Afnae+lPAW+epF3D1Mfa1Hdh+svsoSVqYU2mIryRpkTFEJEndDBFJUjdDRJLUzRCRJHUzRCRJ3QwRSVI3Q0SS1M0QkSR1M0QkSd0MEUlSN0NEktTNEJEkdTNEJEndDBFJUjdDRJLUzRCRJHUzRCRJ3QwRSVI3Q0SS1M0QkSR1M0QkSd0MEUlSN0NEktTNEJEkdVv0IZJkU5JvJJlJcu2k+yNJS8miDpEky4BPA5cA64DLk6ybbK8kaelY1CECXADMVNWBqvoBsAvYPOE+SdKSsXzSHThBK4FHh5YPAm+e2yjJVmBrW/xekm+MoW9LwdnAtyfdiVNBPrZl0l3QC/nvc9Z1ORl7+an5ios9RBakqrYB2ybdjx81SaarasOk+yHNx3+f47HYb2cdAlYPLa9qNUnSGCz2ELkbWJvkvCSnA5cBuyfcJ0laMhb17ayqOprkGmAvsAzYXlX7J9ytpcRbhDqV+e9zDFJVk+6DJGmRWuy3syRJE2SISJK6GSLq4udmdKpKsj3Jk0kemHRflgJDRC+Zn5vRKe4WYNOkO7FUGCLq4edmdMqqqq8ARybdj6XCEFGP+T43s3JCfZE0QYaIJKmbIaIefm5GEmCIqI+fm5EEGCLqUFVHgdnPzTwE3OrnZnSqSPJ54KvAP0lyMMmVk+7TjzI/eyJJ6uaViCSpmyEiSepmiEiSuhkikqRuhogkqZshIo1Rkg8m+Q+T7od0shgikqRuhog0QkmuSHJfkq8n+eycdVclubut+2KSH2v1dyR5oNW/0mrnJ7kryb1tf2sncT7SXL5sKI1IkvOBLwE/V1XfTnIm8GvA96rqY0nOqqqnWtsPA09U1aeS3A9sqqpDSc6oqmeSfAq4o6o+1z41s6yq/nZS5ybN8kpEGp0LgT+sqm8DVNXc37h4fZL/0ULjXwPnt/pfAbckuQpY1mpfBT6Q5H3ATxkgOlUYItLk3AJcU1U/Dfwm8AqAqvq3wH9i8KXke9oVy+8DvwD8LbAnyYWT6bL0/zNEpNH5c+AdSc4CaLezhr0KeCzJaQyuRGjtXltVd1bVbwCHgdVJ/jFwoKpuBG4DfmYsZyC9iOWT7oD0o6qq9ie5AfiLJM8DXwMeGWryn4E7GQTFnQxCBeC32oPzALcDXwfeB7wryf8BHgf+y1hOQnoRPliXJHXzdpYkqZshIknqZohIkroZIpKkboaIJKmbISJJ6maISJK6/T8FSGwTcbZtnQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import seaborn as sns\n",
        "sns.countplot('class',data=data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZO3yFTCi7SQ",
        "outputId": "2d005554-8cb0-4b3c-d76e-e7d7b4029bb4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /home/mostafa/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /home/mostafa/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "stemmer = nltk.SnowballStemmer(\"english\")\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "stopword=set(stopwords.words('english'))\n",
        "new_word = ['amp']\n",
        "stopword = stopword.union(new_word)\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r96rqvIWjAK9"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    # text = re.sub('\\[.*?\\]', '', text)\n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub('<.*?>+', '', text)\n",
        "    text = re.sub(r'@[^\\s]+', ' ', text)\n",
        "    text = re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])\", \" \",text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub('\\n', '', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    text = [word for word in text.split(' ') if word not in stopword]\n",
        "    text=\" \".join(text)\n",
        "    text = [stemmer.stem(word) for word in text.split(' ')]\n",
        "    text=\" \".join(text)\n",
        "    text = [lemmatizer.lemmatize(word) for word in text.split(' ')]\n",
        "    text = \" \".join(text)\n",
        "    text = [word for word in text.split(' ') if len(word)>2]\n",
        "    text = \" \".join(text)\n",
        "    return text\n",
        "    # return \" \".join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])\", \" \",text).split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SByvoODPjT0M"
      },
      "outputs": [],
      "source": [
        "data['tweet']=data['tweet'].apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 722
        },
        "collapsed": true,
        "id": "PjbYTor0jfqF",
        "outputId": "fd6a4108-4778-4cf1-8f5c-cf6464ef374c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Toxicity</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>father dysfunct selfish drag kid dysfunct run</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>thank lyft credit use caus offer wheelchair va...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>bihday majesti</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>model love take time</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>factsguid societi motiv</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Toxicity                                              tweet\n",
              "0         0      father dysfunct selfish drag kid dysfunct run\n",
              "1         0  thank lyft credit use caus offer wheelchair va...\n",
              "2         0                                     bihday majesti\n",
              "3         0                               model love take time\n",
              "4         0                            factsguid societi motiv"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "collapsed": true,
        "id": "vLDVF4fPts_0",
        "outputId": "6ae12ce0-1715-419b-fe88-fb29a9928793"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "#Importing Dataset\n",
        "\n",
        "df = data\n",
        "\n",
        "#Checking the Data\n",
        "\n",
        "df.head()\n",
        "\n",
        "#Creating the text variable\n",
        "\n",
        "text2 = \" \".join(tweet for tweet in df.tweet)\n",
        "\n",
        "# Creating word_cloud with text as argument in .generate() method\n",
        "\n",
        "word_cloud2 = WordCloud(collocations = False, background_color = 'white').generate(text2)\n",
        "\n",
        "# Display the generated Word Cloud\n",
        "\n",
        "plt.imshow(word_cloud2, interpolation='bilinear')\n",
        "\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fb7YwUoqTFEv"
      },
      "outputs": [],
      "source": [
        "path = 'english/hatePreprocessed.csv'\n",
        "\n",
        "with open(path, 'w', encoding='utf-8') as f:\n",
        "    data.to_csv(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlBi4xvvuJax"
      },
      "source": [
        "# Train test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLzkd9k6uBhr",
        "outputId": "4e4d5307-e7be-4a8c-c56a-e29eefbf57dd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((39652,), (9914,), (39652,), (9914,))"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['tweet'], data['Toxicity'],random_state = 0,test_size = 0.20)\n",
        "X_train.shape,X_test.shape,y_train.shape,y_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uV8RrDeuVQc"
      },
      "source": [
        "# ML models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldpQ5mJNlvdd",
        "outputId": "17efc09d-96a9-4e11-bd34-a895ca09c8e0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((39721,), (17024,), (39721,), (17024,))"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X, y = data.tweet.fillna(' '), data['class']\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,random_state = 0,test_size = 0.2)\n",
        "X_train.shape,X_test.shape,y_train.shape,y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnYSJRvUuY5f"
      },
      "outputs": [],
      "source": [
        "# x = v.fit_transform(data['t'].values.astype('U'))\n",
        "Tfidf_vect = TfidfVectorizer(max_features=None)\n",
        "Tfidf_vect.fit(X)\n",
        "Train_X_Tfidf = Tfidf_vect.transform(X_train)\n",
        "Test_X_Tfidf = Tfidf_vect.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ki0DbiUYutXS",
        "outputId": "5e6a3837-1ed9-4ac7-9f12-1d3bc94d0a81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Naive Bayes Training Accuracy Score ->  93.02887641298054\n",
            "Naive Bayes Test Accuracy Score ->  88.68068609022556\n"
          ]
        }
      ],
      "source": [
        "model = MultinomialNB()\n",
        "model.fit(Train_X_Tfidf,y_train)\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "predictions_train_NB = model.predict(Train_X_Tfidf)\n",
        "predictions_NB = model.predict(Test_X_Tfidf)\n",
        "print(\"Naive Bayes Training Accuracy Score -> \",accuracy_score(predictions_train_NB, y_train)*100)\n",
        "print(\"Naive Bayes Test Accuracy Score -> \",accuracy_score(predictions_NB, y_test)*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b34fyCl6uyE8",
        "outputId": "91192114-e9e8-45de-cbb0-967cb10d8a42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVM Train Accuracy Score ->  96.30673950806879\n",
            "SVM Test Accuracy Score ->  93.51503759398496\n"
          ]
        }
      ],
      "source": [
        "from sklearn import svm\n",
        "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
        "SVM.fit(Train_X_Tfidf,y_train)\n",
        "pred_y_train = SVM.predict(Train_X_Tfidf)\n",
        "pred_y = SVM.predict(Test_X_Tfidf)\n",
        "# Use accuracy_score function to get the accuracy\n",
        "print(\"SVM Train Accuracy Score -> \",accuracy_score(pred_y_train, y_train)*100)\n",
        "print(\"SVM Test Accuracy Score -> \",accuracy_score(pred_y, y_test)*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LO5_uPSowC9o",
        "outputId": "652ca136-7ff6-41db-f455-ed4d75b06561"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Accuracy Score ->  99.85398152110973\n",
            "Test Accuracy Score ->  92.60455827067669\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "RF=RandomForestClassifier(n_estimators=100)\n",
        "RF.fit(Train_X_Tfidf,y_train)\n",
        "pred_y_train = RF.predict(Train_X_Tfidf)\n",
        "pred_y = RF.predict(Test_X_Tfidf)\n",
        "# Use accuracy_score function to get the accuracy\n",
        "print(\"Train Accuracy Score -> \",accuracy_score(pred_y_train, y_train)*100)\n",
        "print(\"Test Accuracy Score -> \",accuracy_score(pred_y, y_test)*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ON4sUbefxB9S",
        "outputId": "b7c5ed2f-a7f7-42b0-d763-bca45b57daf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Accuracy Score ->  99.85398152110973\n",
            "Test Accuracy Score ->  92.30498120300751\n"
          ]
        }
      ],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "clf = DecisionTreeClassifier()\n",
        "\n",
        "clf = clf.fit(Train_X_Tfidf,y_train)\n",
        "\n",
        "pred_y_train = clf.predict(Train_X_Tfidf)\n",
        "pred_y = clf.predict(Test_X_Tfidf)\n",
        "# Use accuracy_score function to get the accuracy\n",
        "print(\"Train Accuracy Score -> \",accuracy_score(pred_y_train, y_train)*100)\n",
        "print(\"Test Accuracy Score -> \",accuracy_score(pred_y, y_test)*100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uk99_ApDWDwq"
      },
      "source": [
        "# Deep Learning Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSpmZrJuLITR"
      },
      "source": [
        "# Train-Test Split, Tokenizing, Padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfZp-cvOJLXh",
        "outputId": "62c18747-6d42-4eff-cc12-7ee10d36a52f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary Size : 33196\n",
            "Training X Shape: (45396, 60)\n",
            "Testing X Shape: (11349, 60)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "data.tweet=data.tweet.astype(str)\n",
        "X = data.tweet\n",
        "y = data['Toxicity']\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,random_state = 0,test_size = 0.2)\n",
        "# X_train.shape,X_test.shape,y_train.shape,y_test.shape\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(\"Vocabulary Size :\", vocab_size)\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 60\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "x_train = pad_sequences(tokenizer.texts_to_sequences(X_train),\n",
        "                        maxlen = MAX_SEQUENCE_LENGTH)\n",
        "x_test = pad_sequences(tokenizer.texts_to_sequences(X_test),\n",
        "                       maxlen = MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "print(\"Training X Shape:\",x_train.shape)\n",
        "print(\"Testing X Shape:\",x_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-W0_NuxDJ7WH",
        "outputId": "be3f7037-8096-4752-993f-452fd8195107"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(45396, 1) (11349, 1)\n"
          ]
        }
      ],
      "source": [
        "y_train = y_train.values.reshape(-1, 1)\n",
        "y_test = y_test.values.reshape(-1, 1)\n",
        "print(y_train.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOWLB4kwKB0g",
        "outputId": "8df26424-6de8-4a53-9848-1e0661aef4fc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((56745, 60), (56745, 1))"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define per-fold score containers\n",
        "import numpy as np\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "# Merge inputs and targets\n",
        "inputs = np.concatenate((x_train, x_test), axis=0)\n",
        "targets = np.concatenate((y_train, y_test), axis=0)\n",
        "inputs.shape, targets.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tX3IZPLCLRdl"
      },
      "source": [
        "# Attention Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0Wi4sEmKD7x"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Layer\n",
        "from keras import backend as K\n",
        "class attention(Layer):\n",
        "    def __init__(self,**kwargs):\n",
        "        super(attention,self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\")\n",
        "        self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),initializer=\"zeros\")        \n",
        "        super(attention, self).build(input_shape)\n",
        "\n",
        "    def call(self,x):\n",
        "        et=K.squeeze(K.tanh(K.dot(x,self.W)+self.b),axis=-1)\n",
        "        at=K.softmax(et)\n",
        "        at=K.expand_dims(at,axis=-1)\n",
        "        output=x*at\n",
        "        return K.sum(output,axis=1)\n",
        "\n",
        "    def compute_output_shape(self,input_shape):\n",
        "        return (input_shape[0],input_shape[-1])\n",
        "\n",
        "    def get_config(self):\n",
        "        return super(attention,self).get_config()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83gbUbRaLUyG"
      },
      "source": [
        "# Word2vec and 5-fold cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8OWuH99nKF_I",
        "outputId": "5b077ddd-65bb-4f19-fd3c-ac62d61cd4e6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([[    0,     0,     0, ...,   392,   493,   661],\n",
              "        [    0,     0,     0, ...,   153,  2072,    48],\n",
              "        [    0,     0,     0, ...,     1,   116,    13],\n",
              "        ...,\n",
              "        [    0,     0,     0, ..., 14245,    87,  1286],\n",
              "        [    0,     0,     0, ...,   984,     5,   750],\n",
              "        [    0,     0,     0, ...,  2980,   768,     5]], dtype=int32),\n",
              " array([[1],\n",
              "        [1],\n",
              "        [1],\n",
              "        ...,\n",
              "        [1],\n",
              "        [1],\n",
              "        [1]]))"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs, targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGkcLi9zKLD_",
        "outputId": "68883f81-78ee-4f87-e854-48a7adf24727"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['best', 'essentialoil', 'anxieti', 'healthi', 'peac', 'altwaystoh']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mostafa/env/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "Word2vec_train_data = list(map(lambda x: x.split(), X_train))\n",
        "print(Word2vec_train_data[5])\n",
        "EMBEDDING_DIM = 100\n",
        "from gensim.models import Word2Vec\n",
        "word2vec_model = Word2Vec(Word2vec_train_data,\n",
        "                 vector_size=EMBEDDING_DIM,\n",
        "                 workers=3,\n",
        "                 min_count=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-oq3ITuKNb0",
        "outputId": "67833d4f-3724-4c1e-d596-4e9705ab9fc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding Matrix Shape: (33196, 100)\n"
          ]
        }
      ],
      "source": [
        "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
        "\n",
        "for word, token in tokenizer.word_index.items():\n",
        "    if word2vec_model.wv.__contains__(word):\n",
        "        embedding_matrix[token] = word2vec_model.wv.__getitem__(word)\n",
        "\n",
        "print(\"Embedding Matrix Shape:\", embedding_matrix.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HATE DETECT W2V"
      ],
      "metadata": {
        "id": "SPmUYq_d6c3M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Qkz-kI706WE8",
        "outputId": "9c98f6c0-bfe0-451e-8b00-706640cd1036"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "620/620 [==============================] - 543s 831ms/step - loss: 0.1975 - accuracy: 0.9121\n",
            "Epoch 2/10\n",
            "620/620 [==============================] - 556s 896ms/step - loss: 0.1554 - accuracy: 0.9337\n",
            "Epoch 3/10\n",
            "620/620 [==============================] - 581s 937ms/step - loss: 0.1502 - accuracy: 0.9360\n",
            "Epoch 4/10\n",
            "620/620 [==============================] - 580s 935ms/step - loss: 0.1486 - accuracy: 0.9371\n",
            "Epoch 5/10\n",
            "620/620 [==============================] - 582s 939ms/step - loss: 0.1474 - accuracy: 0.9375\n",
            "Epoch 6/10\n",
            "620/620 [==============================] - 581s 936ms/step - loss: 0.1427 - accuracy: 0.9393\n",
            "Epoch 7/10\n",
            "620/620 [==============================] - 582s 938ms/step - loss: 0.1407 - accuracy: 0.9400\n",
            "Epoch 8/10\n",
            "620/620 [==============================] - 580s 935ms/step - loss: 0.1398 - accuracy: 0.9408\n",
            "Epoch 9/10\n",
            "620/620 [==============================] - 580s 936ms/step - loss: 0.1379 - accuracy: 0.9403\n",
            "Epoch 10/10\n",
            "620/620 [==============================] - 579s 934ms/step - loss: 0.1356 - accuracy: 0.9421\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-04-18 22:54:57.059696: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: TwitterHateModels_word2vec/1model/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f6017be8580> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f6017be8d60> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f601ebac6d0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f6014a708b0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 1: loss of 0.12517638504505157; accuracy of 94.73471641540527%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "620/620 [==============================] - 603s 950ms/step - loss: 0.1793 - accuracy: 0.9187\n",
            "Epoch 2/10\n",
            "620/620 [==============================] - 592s 956ms/step - loss: 0.1508 - accuracy: 0.9356\n",
            "Epoch 3/10\n",
            "620/620 [==============================] - 584s 941ms/step - loss: 0.1442 - accuracy: 0.9384\n",
            "Epoch 4/10\n",
            "620/620 [==============================] - 585s 944ms/step - loss: 0.1433 - accuracy: 0.9399\n",
            "Epoch 5/10\n",
            "620/620 [==============================] - 621s 1s/step - loss: 0.1399 - accuracy: 0.9415\n",
            "Epoch 6/10\n",
            "620/620 [==============================] - 638s 1s/step - loss: 0.1382 - accuracy: 0.9411\n",
            "Epoch 7/10\n",
            "620/620 [==============================] - 692s 1s/step - loss: 0.1380 - accuracy: 0.9415\n",
            "Epoch 8/10\n",
            "620/620 [==============================] - 626s 1s/step - loss: 0.1345 - accuracy: 0.9444\n",
            "Epoch 9/10\n",
            "620/620 [==============================] - 605s 975ms/step - loss: 0.1333 - accuracy: 0.9440\n",
            "Epoch 10/10\n",
            "620/620 [==============================] - 601s 969ms/step - loss: 0.1312 - accuracy: 0.9447\n",
            "INFO:tensorflow:Assets written to: TwitterHateModels_word2vec/2model/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: TwitterHateModels_word2vec/2model/assets\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f601633efa0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f5fec141a90> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f6016075790> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f5fecf14bb0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 2: loss of 0.13567915558815002; accuracy of 94.65348720550537%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "620/620 [==============================] - 658s 1s/step - loss: 0.1795 - accuracy: 0.9210\n",
            "Epoch 2/10\n",
            "620/620 [==============================] - 637s 1s/step - loss: 0.1524 - accuracy: 0.9347\n",
            "Epoch 3/10\n",
            "620/620 [==============================] - 598s 963ms/step - loss: 0.1463 - accuracy: 0.9372\n",
            "Epoch 4/10\n",
            "620/620 [==============================] - 624s 1s/step - loss: 0.1443 - accuracy: 0.9387\n",
            "Epoch 5/10\n",
            "620/620 [==============================] - 646s 1s/step - loss: 0.1410 - accuracy: 0.9394\n",
            "Epoch 6/10\n",
            "620/620 [==============================] - 574s 926ms/step - loss: 0.1384 - accuracy: 0.9414\n",
            "Epoch 7/10\n",
            "620/620 [==============================] - 626s 1s/step - loss: 0.1374 - accuracy: 0.9425\n",
            "Epoch 8/10\n",
            "620/620 [==============================] - 633s 1s/step - loss: 0.1344 - accuracy: 0.9426\n",
            "Epoch 9/10\n",
            "620/620 [==============================] - 638s 1s/step - loss: 0.1335 - accuracy: 0.9429\n",
            "Epoch 10/10\n",
            "620/620 [==============================] - 545s 879ms/step - loss: 0.1310 - accuracy: 0.9445\n",
            "INFO:tensorflow:Assets written to: TwitterHateModels_word2vec/3model/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: TwitterHateModels_word2vec/3model/assets\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f5fedac38e0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f5fed88b2b0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f5fed639610> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f5fed5d8fa0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 3: loss of 0.12342187762260437; accuracy of 95.10743618011475%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/10\n",
            "620/620 [==============================] - 531s 829ms/step - loss: 0.1861 - accuracy: 0.9147\n",
            "Epoch 2/10\n",
            "620/620 [==============================] - 514s 829ms/step - loss: 0.1517 - accuracy: 0.9353\n",
            "Epoch 3/10\n",
            "620/620 [==============================] - 511s 825ms/step - loss: 0.1455 - accuracy: 0.9380\n",
            "Epoch 4/10\n",
            "620/620 [==============================] - 595s 960ms/step - loss: 0.1431 - accuracy: 0.9394\n",
            "Epoch 5/10\n",
            "620/620 [==============================] - 1109s 2s/step - loss: 0.1408 - accuracy: 0.9414\n",
            "Epoch 6/10\n",
            "620/620 [==============================] - 977s 2s/step - loss: 0.1392 - accuracy: 0.9409\n",
            "Epoch 7/10\n",
            "620/620 [==============================] - 1010s 2s/step - loss: 0.1374 - accuracy: 0.9428\n",
            "Epoch 8/10\n",
            "620/620 [==============================] - 1219s 2s/step - loss: 0.1348 - accuracy: 0.9435\n",
            "Epoch 9/10\n",
            "620/620 [==============================] - 1055s 2s/step - loss: 0.1328 - accuracy: 0.9447\n",
            "Epoch 10/10\n",
            "620/620 [==============================] - 634s 1s/step - loss: 0.1301 - accuracy: 0.9446\n",
            "INFO:tensorflow:Assets written to: TwitterHateModels_word2vec/4model/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: TwitterHateModels_word2vec/4model/assets\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f5fd5e7aac0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f5fd5de0670> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f5faba3fd90> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f5fd5bb95b0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 4: loss of 0.12245946377515793; accuracy of 94.88550424575806%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/10\n",
            "620/620 [==============================] - 687s 1s/step - loss: 0.1769 - accuracy: 0.9223\n",
            "Epoch 2/10\n",
            "620/620 [==============================] - 631s 1s/step - loss: 0.1462 - accuracy: 0.9373\n",
            "Epoch 3/10\n",
            "620/620 [==============================] - 519s 837ms/step - loss: 0.1433 - accuracy: 0.9398\n",
            "Epoch 4/10\n",
            "620/620 [==============================] - 518s 836ms/step - loss: 0.1405 - accuracy: 0.9402\n",
            "Epoch 5/10\n",
            "620/620 [==============================] - 518s 836ms/step - loss: 0.1383 - accuracy: 0.9416\n",
            "Epoch 6/10\n",
            "620/620 [==============================] - 584s 943ms/step - loss: 0.1358 - accuracy: 0.9422\n",
            "Epoch 7/10\n",
            "620/620 [==============================] - 589s 950ms/step - loss: 0.1341 - accuracy: 0.9448\n",
            "Epoch 8/10\n",
            "620/620 [==============================] - 587s 947ms/step - loss: 0.1322 - accuracy: 0.9444\n",
            "Epoch 9/10\n",
            "620/620 [==============================] - 587s 947ms/step - loss: 0.1303 - accuracy: 0.9444\n",
            "Epoch 10/10\n",
            "620/620 [==============================] - 587s 946ms/step - loss: 0.1282 - accuracy: 0.9465\n",
            "INFO:tensorflow:Assets written to: TwitterHateModels_word2vec/5model/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: TwitterHateModels_word2vec/5model/assets\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f6015da2970> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f5fd6b86610> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f6017401a90> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f5fd7e1e3d0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 0.13185422122478485; accuracy of 94.60304379463196%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.12517638504505157 - Accuracy: 94.73471641540527%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.13567915558815002 - Accuracy: 94.65348720550537%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.12342187762260437 - Accuracy: 95.10743618011475%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.12245946377515793 - Accuracy: 94.88550424575806%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.13185422122478485 - Accuracy: 94.60304379463196%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 94.79683756828308 (+- 0.18237196506893288)\n",
            "> Loss: 0.12771822065114974\n",
            "------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Model configuration\n",
        "from tensorflow.keras.layers import Dense , Flatten ,Embedding,Input,LSTM, Bidirectional, Dense, Dropout,GRU,SpatialDropout1D, Conv1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.layers import SpatialDropout1D\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "\n",
        "batch_size = 64\n",
        "no_classes = 100\n",
        "no_epochs = 10\n",
        "optimizer = Adam()\n",
        "verbosity = 1\n",
        "num_folds = 5\n",
        "\n",
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "for train, test in kfold.split(inputs, targets):\n",
        "\n",
        "    # Define the model architecture\n",
        "    embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
        "                                          100,\n",
        "                                          weights=[embedding_matrix],\n",
        "                                          input_length=60,\n",
        "                                          trainable=False)\n",
        "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "    embedding_sequences = embedding_layer(sequence_input)\n",
        "    x = SpatialDropout1D(0.2)(embedding_sequences)\n",
        "    x = Bidirectional(GRU(units=128, return_sequences = True, dropout=0.25, recurrent_dropout=0.25))(x)\n",
        "    x = Bidirectional(GRU(units=128, return_sequences = True, dropout=0.25, recurrent_dropout=0.25))(x)\n",
        "    x = attention()(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dropout(0.25)(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "    model = tf.keras.Model(sequence_input, outputs)\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  # model.summary()\n",
        "\n",
        "  # Generate a print\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  # Fit data to model\n",
        "    history = model.fit(inputs[train], targets[train],\n",
        "              batch_size=batch_size,\n",
        "              epochs=no_epochs,\n",
        "              verbose=verbosity)\n",
        "    model_path = \"TwitterHateModels_word2vec/\" + str(fold_no) + \"model\"\n",
        "    \n",
        "    # convert the history.history dict to a pandas DataFrame:     \n",
        "    hist_df = pd.DataFrame(history.history) \n",
        "\n",
        "    # save to json:  \n",
        "    hist_json_file = \"TwitterHateHistory_word2vec/\" + str(fold_no) + 'history.json' \n",
        "    with open(hist_json_file, mode='w') as f:\n",
        "        hist_df.to_json(f)\n",
        "            \n",
        "    #save model\n",
        "    model.save(model_path)\n",
        "    \n",
        "  # Generate generalization metrics\n",
        "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
        "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "  # Increase fold number\n",
        "    fold_no = fold_no + 1\n",
        "\n",
        "  # == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BI-GRU W2V"
      ],
      "metadata": {
        "id": "Q1pP3Sap6i4T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jQSANL7z6WE9",
        "outputId": "a850c067-3bb0-40e1-dafc-21eb9709ea81"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-04-21 21:39:52.254256: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
            "2022-04-21 21:39:52.254321: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
            "2022-04-21 21:39:52.254377: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (mostafa): /proc/driver/nvidia/version does not exist\n",
            "2022-04-21 21:39:52.276311: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/5\n",
            "710/710 [==============================] - 303s 409ms/step - loss: 0.2782 - accuracy: 0.8852\n",
            "Epoch 2/5\n",
            "710/710 [==============================] - 292s 411ms/step - loss: 0.2438 - accuracy: 0.8982\n",
            "Epoch 3/5\n",
            "710/710 [==============================] - 290s 408ms/step - loss: 0.2339 - accuracy: 0.9032\n",
            "Epoch 4/5\n",
            "710/710 [==============================] - 291s 410ms/step - loss: 0.2269 - accuracy: 0.9065\n",
            "Epoch 5/5\n",
            "710/710 [==============================] - 289s 406ms/step - loss: 0.2219 - accuracy: 0.9088\n",
            "Score for fold 1: loss of 0.20849475264549255; accuracy of 91.3331687450409%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/5\n",
            "710/710 [==============================] - 294s 403ms/step - loss: 0.2651 - accuracy: 0.8890\n",
            "Epoch 2/5\n",
            "710/710 [==============================] - 286s 403ms/step - loss: 0.2308 - accuracy: 0.9040\n",
            "Epoch 3/5\n",
            "710/710 [==============================] - 286s 402ms/step - loss: 0.2236 - accuracy: 0.9065\n",
            "Epoch 4/5\n",
            "710/710 [==============================] - 286s 403ms/step - loss: 0.2182 - accuracy: 0.9090\n",
            "Epoch 5/5\n",
            "710/710 [==============================] - 281s 396ms/step - loss: 0.2149 - accuracy: 0.9106\n",
            "Score for fold 2: loss of 0.2059709131717682; accuracy of 91.61338806152344%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/5\n",
            "710/710 [==============================] - 290s 397ms/step - loss: 0.2678 - accuracy: 0.8904\n",
            "Epoch 2/5\n",
            "710/710 [==============================] - 287s 404ms/step - loss: 0.2329 - accuracy: 0.9027\n",
            "Epoch 3/5\n",
            "710/710 [==============================] - 287s 404ms/step - loss: 0.2252 - accuracy: 0.9046\n",
            "Epoch 4/5\n",
            "710/710 [==============================] - 285s 402ms/step - loss: 0.2193 - accuracy: 0.9073\n",
            "Epoch 5/5\n",
            "710/710 [==============================] - 287s 404ms/step - loss: 0.2167 - accuracy: 0.9093\n",
            "Score for fold 3: loss of 0.20462150871753693; accuracy of 91.49219393730164%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/5\n",
            "710/710 [==============================] - 295s 404ms/step - loss: 0.2637 - accuracy: 0.8888\n",
            "Epoch 2/5\n",
            "710/710 [==============================] - 294s 414ms/step - loss: 0.2294 - accuracy: 0.9048\n",
            "Epoch 3/5\n",
            "710/710 [==============================] - 287s 404ms/step - loss: 0.2224 - accuracy: 0.9079\n",
            "Epoch 4/5\n",
            "710/710 [==============================] - 282s 398ms/step - loss: 0.2160 - accuracy: 0.9101\n",
            "Epoch 5/5\n",
            "710/710 [==============================] - 291s 410ms/step - loss: 0.2138 - accuracy: 0.9114\n",
            "Score for fold 4: loss of 0.2038232982158661; accuracy of 91.33802056312561%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/5\n",
            "710/710 [==============================] - 296s 401ms/step - loss: 0.2671 - accuracy: 0.8902\n",
            "Epoch 2/5\n",
            "710/710 [==============================] - 286s 403ms/step - loss: 0.2334 - accuracy: 0.9030\n",
            "Epoch 3/5\n",
            "710/710 [==============================] - 284s 400ms/step - loss: 0.2233 - accuracy: 0.9074\n",
            "Epoch 4/5\n",
            "710/710 [==============================] - 290s 408ms/step - loss: 0.2186 - accuracy: 0.9094\n",
            "Epoch 5/5\n",
            "710/710 [==============================] - 286s 403ms/step - loss: 0.2153 - accuracy: 0.9111\n",
            "Score for fold 5: loss of 0.2032395303249359; accuracy of 91.40673279762268%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.20849475264549255 - Accuracy: 91.3331687450409%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.2059709131717682 - Accuracy: 91.61338806152344%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.20462150871753693 - Accuracy: 91.49219393730164%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.2038232982158661 - Accuracy: 91.33802056312561%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.2032395303249359 - Accuracy: 91.40673279762268%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 91.43670082092285 (+- 0.1055013112642262)\n",
            "> Loss: 0.20523000061511992\n",
            "------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Model configuration\n",
        "from tensorflow.keras.layers import Dense , Flatten ,Embedding,Input,LSTM, Bidirectional, Dense, Dropout,GRU,SpatialDropout1D, Conv1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.layers import SpatialDropout1D\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "\n",
        "batch_size = 64\n",
        "no_classes = 100\n",
        "no_epochs = 5\n",
        "optimizer = Adam()\n",
        "verbosity = 1\n",
        "num_folds = 5\n",
        "\n",
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "for train, test in kfold.split(inputs, targets):\n",
        "\n",
        "    # Define the model architecture\n",
        "    embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
        "                                          100,\n",
        "                                          weights=[embedding_matrix],\n",
        "                                          input_length=60,\n",
        "                                          trainable=False)\n",
        "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "    embedding_sequences = embedding_layer(sequence_input)\n",
        "    x = SpatialDropout1D(0.2)(embedding_sequences)\n",
        "    x = Bidirectional(GRU(units=128, return_sequences = True, dropout=0.25, recurrent_dropout=0.25))(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dropout(0.25)(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "    model = tf.keras.Model(sequence_input, outputs)\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  # model.summary()\n",
        "\n",
        "  # Generate a print\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  # Fit data to model\n",
        "    history = model.fit(inputs[train], targets[train],\n",
        "              batch_size=batch_size,\n",
        "              epochs=no_epochs,\n",
        "              verbose=verbosity)\n",
        "#     model_path = \"TwitterHateModels_word2vec/\" + str(fold_no) + \"model\"\n",
        "    \n",
        "#     # convert the history.history dict to a pandas DataFrame:     \n",
        "#     hist_df = pd.DataFrame(history.history) \n",
        "\n",
        "#     # save to json:  \n",
        "#     hist_json_file = \"TwitterHateHistory_word2vec/\" + str(fold_no) + 'history.json' \n",
        "#     with open(hist_json_file, mode='w') as f:\n",
        "#         hist_df.to_json(f)\n",
        "            \n",
        "    #save model\n",
        "#     model.save(model_path)\n",
        "    \n",
        "  # Generate generalization metrics\n",
        "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
        "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "  # Increase fold number\n",
        "    fold_no = fold_no + 1\n",
        "\n",
        "  # == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM W2V"
      ],
      "metadata": {
        "id": "S6oqabeq6mTN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "MMG4UaVB6WE_",
        "outputId": "4ad65502-3f52-4e32-8c57-74e5a3dab6ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/5\n",
            "710/710 [==============================] - 196s 269ms/step - loss: 0.6511 - accuracy: 0.5992\n",
            "Epoch 2/5\n",
            "710/710 [==============================] - 193s 271ms/step - loss: 0.6483 - accuracy: 0.6003\n",
            "Epoch 3/5\n",
            "710/710 [==============================] - 185s 260ms/step - loss: 0.6472 - accuracy: 0.6010\n",
            "Epoch 4/5\n",
            "710/710 [==============================] - 171s 241ms/step - loss: 0.6464 - accuracy: 0.6013\n",
            "Epoch 5/5\n",
            "710/710 [==============================] - 173s 243ms/step - loss: 0.6458 - accuracy: 0.6017\n",
            "Score for fold 1: loss of 0.6445356607437134; accuracy of 60.6897234916687%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/5\n",
            "710/710 [==============================] - 176s 243ms/step - loss: 0.6495 - accuracy: 0.6015\n",
            "Epoch 2/5\n",
            "710/710 [==============================] - 179s 252ms/step - loss: 0.6466 - accuracy: 0.6029\n",
            "Epoch 3/5\n",
            "710/710 [==============================] - 228s 321ms/step - loss: 0.6458 - accuracy: 0.6034\n",
            "Epoch 4/5\n",
            "710/710 [==============================] - 302s 425ms/step - loss: 0.6454 - accuracy: 0.6036\n",
            "Epoch 5/5\n",
            "710/710 [==============================] - 226s 318ms/step - loss: 0.6451 - accuracy: 0.6039\n",
            "Score for fold 2: loss of 0.6453626751899719; accuracy of 59.92184281349182%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/5\n",
            "710/710 [==============================] - 237s 327ms/step - loss: 0.6497 - accuracy: 0.6011\n",
            "Epoch 2/5\n",
            "710/710 [==============================] - 238s 336ms/step - loss: 0.6470 - accuracy: 0.6028\n",
            "Epoch 3/5\n",
            "710/710 [==============================] - 243s 342ms/step - loss: 0.6461 - accuracy: 0.6033\n",
            "Epoch 4/5\n",
            "710/710 [==============================] - 247s 348ms/step - loss: 0.6456 - accuracy: 0.6037\n",
            "Epoch 5/5\n",
            "710/710 [==============================] - 225s 316ms/step - loss: 0.6453 - accuracy: 0.6038\n",
            "Score for fold 3: loss of 0.6441829800605774; accuracy of 59.94154214859009%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/5\n",
            "710/710 [==============================] - 256s 354ms/step - loss: 0.6501 - accuracy: 0.5992\n",
            "Epoch 2/5\n",
            "710/710 [==============================] - 211s 298ms/step - loss: 0.6474 - accuracy: 0.6004\n",
            "Epoch 3/5\n",
            "710/710 [==============================] - 224s 315ms/step - loss: 0.6464 - accuracy: 0.6009\n",
            "Epoch 4/5\n",
            "710/710 [==============================] - 172s 243ms/step - loss: 0.6459 - accuracy: 0.6012\n",
            "Epoch 5/5\n",
            "710/710 [==============================] - 166s 233ms/step - loss: 0.6456 - accuracy: 0.6013\n",
            "Score for fold 4: loss of 0.6428480744361877; accuracy of 60.911959409713745%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/5\n",
            "710/710 [==============================] - 188s 233ms/step - loss: 0.6495 - accuracy: 0.6011\n",
            "Epoch 2/5\n",
            "710/710 [==============================] - 167s 235ms/step - loss: 0.6467 - accuracy: 0.6024\n",
            "Epoch 3/5\n",
            "710/710 [==============================] - 173s 244ms/step - loss: 0.6457 - accuracy: 0.6029\n",
            "Epoch 4/5\n",
            "710/710 [==============================] - 187s 263ms/step - loss: 0.6454 - accuracy: 0.6031\n",
            "Epoch 5/5\n",
            "710/710 [==============================] - 218s 308ms/step - loss: 0.6449 - accuracy: 0.6033\n",
            "Score for fold 5: loss of 0.6457172632217407; accuracy of 60.16755700111389%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.20849475264549255 - Accuracy: 91.3331687450409%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.2059709131717682 - Accuracy: 91.61338806152344%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.20462150871753693 - Accuracy: 91.49219393730164%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.2038232982158661 - Accuracy: 91.33802056312561%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.2032395303249359 - Accuracy: 91.40673279762268%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 6 - Loss: 0.6445356607437134 - Accuracy: 60.6897234916687%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 7 - Loss: 0.6453626751899719 - Accuracy: 59.92184281349182%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 8 - Loss: 0.6441829800605774 - Accuracy: 59.94154214859009%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 9 - Loss: 0.6428480744361877 - Accuracy: 60.911959409713745%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 10 - Loss: 0.6457172632217407 - Accuracy: 60.16755700111389%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 75.88161289691925 (+- 15.55787630993791)\n",
            "> Loss: 0.42487966567277907\n",
            "------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Model configuration\n",
        "from tensorflow.keras.layers import Dense , Flatten ,Embedding,Input,LSTM, Bidirectional, Dense, Dropout,GRU,SpatialDropout1D, Conv1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.layers import SpatialDropout1D\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "\n",
        "batch_size = 64\n",
        "no_classes = 100\n",
        "no_epochs = 5\n",
        "optimizer = Adam()\n",
        "verbosity = 1\n",
        "num_folds = 5\n",
        "\n",
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "for train, test in kfold.split(inputs, targets):\n",
        "\n",
        "    # Define the model architecture\n",
        "    embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
        "                                          100,\n",
        "                                          weights=[embedding_matrix],\n",
        "                                          input_length=60,\n",
        "                                          trainable=False)\n",
        "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "    embedding_sequences = embedding_layer(sequence_input)\n",
        "    x = SpatialDropout1D(0.2)(embedding_sequences)\n",
        "    x = LSTM(units=128, return_sequences = True, dropout=0.25, recurrent_dropout=0.25)(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dropout(0.25)(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "    model = tf.keras.Model(sequence_input, outputs)\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  # model.summary()\n",
        "\n",
        "  # Generate a print\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  # Fit data to model\n",
        "    history = model.fit(inputs[train], targets[train],\n",
        "              batch_size=batch_size,\n",
        "              epochs=no_epochs,\n",
        "              verbose=verbosity)\n",
        "#     model_path = \"TwitterHateModels_word2vec/\" + str(fold_no) + \"model\"\n",
        "    \n",
        "#     # convert the history.history dict to a pandas DataFrame:     \n",
        "#     hist_df = pd.DataFrame(history.history) \n",
        "\n",
        "#     # save to json:  \n",
        "#     hist_json_file = \"TwitterHateHistory_word2vec/\" + str(fold_no) + 'history.json' \n",
        "#     with open(hist_json_file, mode='w') as f:\n",
        "#         hist_df.to_json(f)\n",
        "            \n",
        "    #save model\n",
        "#     model.save(model_path)\n",
        "    \n",
        "  # Generate generalization metrics\n",
        "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
        "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "  # Increase fold number\n",
        "    fold_no = fold_no + 1\n",
        "\n",
        "  # == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Xmma9IhLcyF"
      },
      "source": [
        "# Fasttext and 5-fold cross validatioin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TD0JtKbTKX14",
        "outputId": "1587c29c-c750-4084-c891-76c2b45ff164"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['best', 'essentialoil', 'anxieti', 'healthi', 'peac', 'altwaystoh']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mostafa/env/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "fastText_train_data = list(map(lambda x: x.split(), X_train))\n",
        "print(fastText_train_data[5])\n",
        "EMBEDDING_DIM = 100\n",
        "from gensim.models import FastText\n",
        "fastText_model = FastText(fastText_train_data,\n",
        "                 vector_size=EMBEDDING_DIM,\n",
        "                 workers=3,\n",
        "                 min_count=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjK4EMbSKhxV",
        "outputId": "6e8614e5-881d-45c8-9f02-0923980a0cb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding Matrix Shape: (33196, 100)\n"
          ]
        }
      ],
      "source": [
        "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
        "\n",
        "for word, token in tokenizer.word_index.items():\n",
        "    if fastText_model.wv.__contains__(word):\n",
        "        embedding_matrix[token] = fastText_model.wv.__getitem__(word)\n",
        "\n",
        "print(\"Embedding Matrix Shape:\", embedding_matrix.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HATE DETECT FT"
      ],
      "metadata": {
        "id": "RVKoqK9W6q9_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8g_qRamcKlhr",
        "outputId": "55c59906-542c-40af-fa07-84a000700d92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "620/620 [==============================] - 584s 911ms/step - loss: 0.2248 - accuracy: 0.8974\n",
            "Epoch 2/10\n",
            "620/620 [==============================] - 506s 817ms/step - loss: 0.1759 - accuracy: 0.9222\n",
            "Epoch 3/10\n",
            "620/620 [==============================] - 507s 818ms/step - loss: 0.1664 - accuracy: 0.9273\n",
            "Epoch 4/10\n",
            "620/620 [==============================] - 510s 822ms/step - loss: 0.1605 - accuracy: 0.9301\n",
            "Epoch 5/10\n",
            "620/620 [==============================] - 660s 1s/step - loss: 0.1574 - accuracy: 0.9313\n",
            "Epoch 6/10\n",
            "620/620 [==============================] - 513s 827ms/step - loss: 0.1549 - accuracy: 0.9331\n",
            "Epoch 7/10\n",
            "620/620 [==============================] - 509s 820ms/step - loss: 0.1512 - accuracy: 0.9342\n",
            "Epoch 8/10\n",
            "620/620 [==============================] - 509s 821ms/step - loss: 0.1504 - accuracy: 0.9349\n",
            "Epoch 9/10\n",
            "620/620 [==============================] - 507s 817ms/step - loss: 0.1484 - accuracy: 0.9342\n",
            "Epoch 10/10\n",
            "620/620 [==============================] - 528s 851ms/step - loss: 0.1471 - accuracy: 0.9353\n",
            "Score for fold 1: loss of 0.14894868433475494; accuracy of 93.46379041671753%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "620/620 [==============================] - 687s 956ms/step - loss: 0.2018 - accuracy: 0.9073\n",
            "Epoch 2/10\n",
            "620/620 [==============================] - 601s 970ms/step - loss: 0.1687 - accuracy: 0.9249\n",
            "Epoch 3/10\n",
            "620/620 [==============================] - 594s 958ms/step - loss: 0.1609 - accuracy: 0.9301\n",
            "Epoch 4/10\n",
            "620/620 [==============================] - 606s 978ms/step - loss: 0.1571 - accuracy: 0.9314\n",
            "Epoch 5/10\n",
            "620/620 [==============================] - 598s 964ms/step - loss: 0.1536 - accuracy: 0.9332\n",
            "Epoch 6/10\n",
            "620/620 [==============================] - 615s 992ms/step - loss: 0.1525 - accuracy: 0.9325\n",
            "Epoch 7/10\n",
            "620/620 [==============================] - 627s 1s/step - loss: 0.1492 - accuracy: 0.9341\n",
            "Epoch 8/10\n",
            "620/620 [==============================] - 657s 1s/step - loss: 0.1488 - accuracy: 0.9345\n",
            "Epoch 9/10\n",
            "620/620 [==============================] - 630s 1s/step - loss: 0.1474 - accuracy: 0.9345\n",
            "Epoch 10/10\n",
            "620/620 [==============================] - 684s 1s/step - loss: 0.1462 - accuracy: 0.9362\n",
            "Score for fold 2: loss of 0.13977986574172974; accuracy of 93.96751523017883%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "620/620 [==============================] - 686s 1s/step - loss: 0.2095 - accuracy: 0.9060\n",
            "Epoch 2/10\n",
            "620/620 [==============================] - 720s 1s/step - loss: 0.1728 - accuracy: 0.9219\n",
            "Epoch 3/10\n",
            "620/620 [==============================] - 743s 1s/step - loss: 0.1633 - accuracy: 0.9292\n",
            "Epoch 4/10\n",
            "620/620 [==============================] - 653s 1s/step - loss: 0.1586 - accuracy: 0.9309\n",
            "Epoch 5/10\n",
            "620/620 [==============================] - 512s 826ms/step - loss: 0.1555 - accuracy: 0.9315\n",
            "Epoch 6/10\n",
            "620/620 [==============================] - 511s 824ms/step - loss: 0.1534 - accuracy: 0.9327\n",
            "Epoch 7/10\n",
            "620/620 [==============================] - 512s 825ms/step - loss: 0.1513 - accuracy: 0.9349\n",
            "Epoch 8/10\n",
            "620/620 [==============================] - 709s 1s/step - loss: 0.1521 - accuracy: 0.9342\n",
            "Epoch 9/10\n",
            "620/620 [==============================] - 663s 1s/step - loss: 0.1501 - accuracy: 0.9339\n",
            "Epoch 10/10\n",
            "620/620 [==============================] - 585s 944ms/step - loss: 0.1483 - accuracy: 0.9351\n",
            "Score for fold 3: loss of 0.1333039402961731; accuracy of 94.11883354187012%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/10\n",
            "620/620 [==============================] - 1169s 1s/step - loss: 0.2055 - accuracy: 0.9033\n",
            "Epoch 2/10\n",
            "620/620 [==============================] - 838s 1s/step - loss: 0.1693 - accuracy: 0.9255\n",
            "Epoch 3/10\n",
            "620/620 [==============================] - 943s 2s/step - loss: 0.1609 - accuracy: 0.9286\n",
            "Epoch 4/10\n",
            "620/620 [==============================] - 722s 1s/step - loss: 0.1560 - accuracy: 0.9320\n",
            "Epoch 5/10\n",
            "620/620 [==============================] - 511s 824ms/step - loss: 0.1553 - accuracy: 0.9316\n",
            "Epoch 6/10\n",
            "620/620 [==============================] - 510s 823ms/step - loss: 0.1508 - accuracy: 0.9339\n",
            "Epoch 7/10\n",
            "620/620 [==============================] - 511s 823ms/step - loss: 0.1502 - accuracy: 0.9340\n",
            "Epoch 8/10\n",
            "620/620 [==============================] - 516s 833ms/step - loss: 0.1483 - accuracy: 0.9345\n",
            "Epoch 9/10\n",
            "620/620 [==============================] - 514s 829ms/step - loss: 0.1461 - accuracy: 0.9358\n",
            "Epoch 10/10\n",
            "620/620 [==============================] - 518s 836ms/step - loss: 0.1461 - accuracy: 0.9359\n",
            "Score for fold 4: loss of 0.14634692668914795; accuracy of 93.81620287895203%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/10\n",
            "620/620 [==============================] - 627s 937ms/step - loss: 0.2013 - accuracy: 0.9085\n",
            "Epoch 2/10\n",
            "620/620 [==============================] - 588s 947ms/step - loss: 0.1672 - accuracy: 0.9277\n",
            "Epoch 3/10\n",
            "620/620 [==============================] - 712s 1s/step - loss: 0.1608 - accuracy: 0.9302\n",
            "Epoch 4/10\n",
            "620/620 [==============================] - 616s 993ms/step - loss: 0.1574 - accuracy: 0.9317\n",
            "Epoch 5/10\n",
            "620/620 [==============================] - 658s 1s/step - loss: 0.1549 - accuracy: 0.9321\n",
            "Epoch 6/10\n",
            "620/620 [==============================] - 739s 1s/step - loss: 0.1524 - accuracy: 0.9328\n",
            "Epoch 7/10\n",
            "620/620 [==============================] - 548s 882ms/step - loss: 0.1507 - accuracy: 0.9349\n",
            "Epoch 8/10\n",
            "620/620 [==============================] - 514s 828ms/step - loss: 0.1482 - accuracy: 0.9358\n",
            "Epoch 9/10\n",
            "620/620 [==============================] - 514s 829ms/step - loss: 0.1484 - accuracy: 0.9344\n",
            "Epoch 10/10\n",
            "620/620 [==============================] - 512s 826ms/step - loss: 0.1453 - accuracy: 0.9379\n",
            "Score for fold 5: loss of 0.13511087000370026; accuracy of 93.92716884613037%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.11638794839382172 - Accuracy: 95.38027048110962%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.12165696173906326 - Accuracy: 94.68374848365784%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.1270059496164322 - Accuracy: 94.89558935165405%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.1259249895811081 - Accuracy: 94.68374848365784%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.12776987254619598 - Accuracy: 94.5626974105835%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 6 - Loss: 0.14894868433475494 - Accuracy: 93.46379041671753%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 7 - Loss: 0.13977986574172974 - Accuracy: 93.96751523017883%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 8 - Loss: 0.1333039402961731 - Accuracy: 94.11883354187012%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 9 - Loss: 0.14634692668914795 - Accuracy: 93.81620287895203%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 10 - Loss: 0.13511087000370026 - Accuracy: 93.92716884613037%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 94.34995651245117 (+- 0.5546139902907413)\n",
            "> Loss: 0.13222360089421273\n",
            "------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Model configuration\n",
        "from tensorflow.keras.layers import Dense , Flatten ,Embedding,Input,LSTM, Bidirectional, Dense, Dropout,GRU,SpatialDropout1D, Conv1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.layers import SpatialDropout1D\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "\n",
        "batch_size = 64\n",
        "no_classes = 100\n",
        "no_epochs = 10\n",
        "optimizer = Adam()\n",
        "verbosity = 1\n",
        "num_folds = 5\n",
        "\n",
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "for train, test in kfold.split(inputs, targets):\n",
        "\n",
        "    # Define the model architecture\n",
        "    embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
        "                                          100,\n",
        "                                          weights=[embedding_matrix],\n",
        "                                          input_length=60,\n",
        "                                          trainable=False)\n",
        "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "    embedding_sequences = embedding_layer(sequence_input)\n",
        "    x = SpatialDropout1D(0.2)(embedding_sequences)\n",
        "    x = Bidirectional(GRU(units=128, return_sequences = True, dropout=0.25, recurrent_dropout=0.25))(x)\n",
        "    x = Bidirectional(GRU(units=128, return_sequences = True, dropout=0.25, recurrent_dropout=0.25))(x)\n",
        "    x = attention()(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dropout(0.25)(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "    model = tf.keras.Model(sequence_input, outputs)\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  # model.summary()\n",
        "\n",
        "  # Generate a print\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  # Fit data to model\n",
        "    history = model.fit(inputs[train], targets[train],\n",
        "              batch_size=batch_size,\n",
        "              epochs=no_epochs,\n",
        "              verbose=verbosity)\n",
        "    # model_path = \"content_english/TwitterHateModels_word2vec/\" + str(fold_no) + \"model\"\n",
        "    \n",
        "    # convert the history.history dict to a pandas DataFrame:     \n",
        "    # hist_df = pd.DataFrame(history.history) \n",
        "\n",
        "    # save to json:  \n",
        "    # hist_json_file = \"content_english/TwitterHateHistory_word2vec/\" + str(fold_no) + 'history.json' \n",
        "    # with open(hist_json_file, mode='w') as f:\n",
        "    #     hist_df.to_json(f)\n",
        "            \n",
        "    #save model\n",
        "    # model.save(model_path)\n",
        "    \n",
        "  # Generate generalization metrics\n",
        "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
        "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "  # Increase fold number\n",
        "    fold_no = fold_no + 1\n",
        "\n",
        "  # == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BI-GRU FT"
      ],
      "metadata": {
        "id": "fRhthajK6twf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "eTBbairv6WFF",
        "outputId": "6b9b80f9-7e6e-4899-a354-11434173f4ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/5\n",
            "710/710 [==============================] - 336s 364ms/step - loss: 0.3050 - accuracy: 0.8738\n",
            "Epoch 2/5\n",
            "710/710 [==============================] - 242s 341ms/step - loss: 0.2612 - accuracy: 0.8908\n",
            "Epoch 3/5\n",
            "710/710 [==============================] - 243s 342ms/step - loss: 0.2523 - accuracy: 0.8950\n",
            "Epoch 4/5\n",
            "710/710 [==============================] - 243s 342ms/step - loss: 0.2463 - accuracy: 0.8963\n",
            "Epoch 5/5\n",
            "710/710 [==============================] - 247s 349ms/step - loss: 0.2415 - accuracy: 0.8995\n",
            "Score for fold 1: loss of 0.2259172648191452; accuracy of 90.44882655143738%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/5\n",
            "710/710 [==============================] - 296s 389ms/step - loss: 0.2926 - accuracy: 0.8785\n",
            "Epoch 2/5\n",
            "710/710 [==============================] - 265s 373ms/step - loss: 0.2535 - accuracy: 0.8942\n",
            "Epoch 3/5\n",
            "710/710 [==============================] - 245s 345ms/step - loss: 0.2444 - accuracy: 0.8978\n",
            "Epoch 4/5\n",
            "710/710 [==============================] - 239s 337ms/step - loss: 0.2402 - accuracy: 0.9003\n",
            "Epoch 5/5\n",
            "710/710 [==============================] - 241s 339ms/step - loss: 0.2372 - accuracy: 0.9017\n",
            "Score for fold 2: loss of 0.22301651537418365; accuracy of 90.69668650627136%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/5\n",
            "710/710 [==============================] - 306s 408ms/step - loss: 0.2926 - accuracy: 0.8769\n",
            "Epoch 2/5\n",
            "710/710 [==============================] - 296s 417ms/step - loss: 0.2505 - accuracy: 0.8945\n",
            "Epoch 3/5\n",
            "710/710 [==============================] - 278s 391ms/step - loss: 0.2411 - accuracy: 0.8999\n",
            "Epoch 4/5\n",
            "710/710 [==============================] - 259s 365ms/step - loss: 0.2366 - accuracy: 0.9014\n",
            "Epoch 5/5\n",
            "710/710 [==============================] - 278s 391ms/step - loss: 0.2302 - accuracy: 0.9045\n",
            "Score for fold 3: loss of 0.22086210548877716; accuracy of 90.85898995399475%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/5\n",
            "710/710 [==============================] - 315s 420ms/step - loss: 0.2979 - accuracy: 0.8734\n",
            "Epoch 2/5\n",
            "710/710 [==============================] - 256s 360ms/step - loss: 0.2507 - accuracy: 0.8942\n",
            "Epoch 3/5\n",
            "710/710 [==============================] - 248s 349ms/step - loss: 0.2437 - accuracy: 0.8985\n",
            "Epoch 4/5\n",
            "710/710 [==============================] - 371s 522ms/step - loss: 0.2373 - accuracy: 0.9009\n",
            "Epoch 5/5\n",
            "710/710 [==============================] - 316s 446ms/step - loss: 0.2337 - accuracy: 0.9040\n",
            "Score for fold 4: loss of 0.22361372411251068; accuracy of 90.94356894493103%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/5\n",
            "710/710 [==============================] - 318s 434ms/step - loss: 0.2924 - accuracy: 0.8795\n",
            "Epoch 2/5\n",
            "710/710 [==============================] - 271s 382ms/step - loss: 0.2520 - accuracy: 0.8945\n",
            "Epoch 3/5\n",
            "710/710 [==============================] - 271s 382ms/step - loss: 0.2440 - accuracy: 0.8978\n",
            "Epoch 4/5\n",
            "710/710 [==============================] - 347s 488ms/step - loss: 0.2386 - accuracy: 0.9008\n",
            "Epoch 5/5\n",
            "710/710 [==============================] - 360s 507ms/step - loss: 0.2335 - accuracy: 0.9025\n",
            "Score for fold 5: loss of 0.2174864411354065; accuracy of 91.13238453865051%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.20849475264549255 - Accuracy: 91.3331687450409%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.2059709131717682 - Accuracy: 91.61338806152344%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.20462150871753693 - Accuracy: 91.49219393730164%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.2038232982158661 - Accuracy: 91.33802056312561%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.2032395303249359 - Accuracy: 91.40673279762268%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 6 - Loss: 0.6445356607437134 - Accuracy: 60.6897234916687%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 7 - Loss: 0.6453626751899719 - Accuracy: 59.92184281349182%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 8 - Loss: 0.6441829800605774 - Accuracy: 59.94154214859009%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 9 - Loss: 0.6428480744361877 - Accuracy: 60.911959409713745%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 10 - Loss: 0.6457172632217407 - Accuracy: 60.16755700111389%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 11 - Loss: 0.2259172648191452 - Accuracy: 90.44882655143738%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 12 - Loss: 0.22301651537418365 - Accuracy: 90.69668650627136%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 13 - Loss: 0.22086210548877716 - Accuracy: 90.85898995399475%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 14 - Loss: 0.22361372411251068 - Accuracy: 90.94356894493103%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 15 - Loss: 0.2174864411354065 - Accuracy: 91.13238453865051%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 80.85977236429851 (+- 14.524013624317929)\n",
            "> Loss: 0.3573128471771876\n",
            "------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Model configuration\n",
        "from tensorflow.keras.layers import Dense , Flatten ,Embedding,Input,LSTM, Bidirectional, Dense, Dropout,GRU,SpatialDropout1D, Conv1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.layers import SpatialDropout1D\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "\n",
        "batch_size = 64\n",
        "no_classes = 100\n",
        "no_epochs = 5\n",
        "optimizer = Adam()\n",
        "verbosity = 1\n",
        "num_folds = 5\n",
        "\n",
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "for train, test in kfold.split(inputs, targets):\n",
        "\n",
        "    # Define the model architecture\n",
        "    embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
        "                                          100,\n",
        "                                          weights=[embedding_matrix],\n",
        "                                          input_length=60,\n",
        "                                          trainable=False)\n",
        "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "    embedding_sequences = embedding_layer(sequence_input)\n",
        "    x = SpatialDropout1D(0.2)(embedding_sequences)\n",
        "    x = Bidirectional(GRU(units=128, return_sequences = True, dropout=0.25, recurrent_dropout=0.25))(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dropout(0.25)(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "    model = tf.keras.Model(sequence_input, outputs)\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  # model.summary()\n",
        "\n",
        "  # Generate a print\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  # Fit data to model\n",
        "    history = model.fit(inputs[train], targets[train],\n",
        "              batch_size=batch_size,\n",
        "              epochs=no_epochs,\n",
        "              verbose=verbosity)\n",
        "    # model_path = \"content_english/TwitterHateModels_word2vec/\" + str(fold_no) + \"model\"\n",
        "    \n",
        "    # convert the history.history dict to a pandas DataFrame:     \n",
        "    # hist_df = pd.DataFrame(history.history) \n",
        "\n",
        "    # save to json:  \n",
        "    # hist_json_file = \"content_english/TwitterHateHistory_word2vec/\" + str(fold_no) + 'history.json' \n",
        "    # with open(hist_json_file, mode='w') as f:\n",
        "    #     hist_df.to_json(f)\n",
        "            \n",
        "    #save model\n",
        "    # model.save(model_path)\n",
        "    \n",
        "  # Generate generalization metrics\n",
        "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
        "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "  # Increase fold number\n",
        "    fold_no = fold_no + 1\n",
        "\n",
        "  # == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM FT"
      ],
      "metadata": {
        "id": "tHLtYPg66zJ9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "rMw3uaKw6WFG",
        "outputId": "947f0057-1455-4b77-e4cf-1a2dfb088578"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-04-22 14:27:17.512482: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
            "2022-04-22 14:27:17.512608: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
            "2022-04-22 14:27:17.512693: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (mostafa): /proc/driver/nvidia/version does not exist\n",
            "2022-04-22 14:27:17.523294: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/5\n",
            "710/710 [==============================] - 140s 189ms/step - loss: 0.2832 - accuracy: 0.8812\n",
            "Epoch 2/5\n",
            "710/710 [==============================] - 129s 181ms/step - loss: 0.2480 - accuracy: 0.8959\n",
            "Epoch 3/5\n",
            "710/710 [==============================] - 128s 180ms/step - loss: 0.2390 - accuracy: 0.8993\n",
            "Epoch 4/5\n",
            "710/710 [==============================] - 128s 181ms/step - loss: 0.2339 - accuracy: 0.9023\n",
            "Epoch 5/5\n",
            "710/710 [==============================] - 128s 181ms/step - loss: 0.2306 - accuracy: 0.9033\n",
            "Score for fold 1: loss of 0.22522643208503723; accuracy of 90.65997004508972%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/5\n",
            "710/710 [==============================] - 132s 180ms/step - loss: 0.2684 - accuracy: 0.8885\n",
            "Epoch 2/5\n",
            "710/710 [==============================] - 129s 181ms/step - loss: 0.2401 - accuracy: 0.8988\n",
            "Epoch 3/5\n",
            "710/710 [==============================] - 135s 191ms/step - loss: 0.2331 - accuracy: 0.9026\n",
            "Epoch 4/5\n",
            "710/710 [==============================] - 146s 205ms/step - loss: 0.2303 - accuracy: 0.9052\n",
            "Epoch 5/5\n",
            "710/710 [==============================] - 145s 205ms/step - loss: 0.2251 - accuracy: 0.9061\n",
            "Score for fold 2: loss of 0.21138709783554077; accuracy of 91.3560688495636%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/5\n",
            "710/710 [==============================] - 153s 207ms/step - loss: 0.2718 - accuracy: 0.8872\n",
            "Epoch 2/5\n",
            "710/710 [==============================] - 145s 205ms/step - loss: 0.2415 - accuracy: 0.8981\n",
            "Epoch 3/5\n",
            "710/710 [==============================] - 147s 208ms/step - loss: 0.2353 - accuracy: 0.9002\n",
            "Epoch 4/5\n",
            "710/710 [==============================] - 147s 208ms/step - loss: 0.2320 - accuracy: 0.9021\n",
            "Epoch 5/5\n",
            "710/710 [==============================] - 148s 209ms/step - loss: 0.2285 - accuracy: 0.9050\n",
            "Score for fold 3: loss of 0.21662138402462006; accuracy of 90.68640470504761%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/5\n",
            "710/710 [==============================] - 148s 203ms/step - loss: 0.2672 - accuracy: 0.8888\n",
            "Epoch 2/5\n",
            "710/710 [==============================] - 145s 205ms/step - loss: 0.2400 - accuracy: 0.8994\n",
            "Epoch 3/5\n",
            "710/710 [==============================] - 144s 203ms/step - loss: 0.2333 - accuracy: 0.9023\n",
            "Epoch 4/5\n",
            "710/710 [==============================] - 148s 208ms/step - loss: 0.2285 - accuracy: 0.9042\n",
            "Epoch 5/5\n",
            "710/710 [==============================] - 146s 205ms/step - loss: 0.2232 - accuracy: 0.9070\n",
            "Score for fold 4: loss of 0.21943388879299164; accuracy of 90.59829115867615%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/5\n",
            "710/710 [==============================] - 151s 207ms/step - loss: 0.2726 - accuracy: 0.8849\n",
            "Epoch 2/5\n",
            "710/710 [==============================] - 146s 206ms/step - loss: 0.2421 - accuracy: 0.8968\n",
            "Epoch 3/5\n",
            "710/710 [==============================] - 146s 205ms/step - loss: 0.2348 - accuracy: 0.9003\n",
            "Epoch 4/5\n",
            "710/710 [==============================] - 149s 210ms/step - loss: 0.2297 - accuracy: 0.9044\n",
            "Epoch 5/5\n",
            "710/710 [==============================] - 150s 211ms/step - loss: 0.2275 - accuracy: 0.9041\n",
            "Score for fold 5: loss of 0.2214421033859253; accuracy of 90.95955491065979%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.22522643208503723 - Accuracy: 90.65997004508972%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.21138709783554077 - Accuracy: 91.3560688495636%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.21662138402462006 - Accuracy: 90.68640470504761%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.21943388879299164 - Accuracy: 90.59829115867615%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.2214421033859253 - Accuracy: 90.95955491065979%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 90.85205793380737 (+- 0.2808269675742984)\n",
            "> Loss: 0.218822181224823\n",
            "------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Model configuration\n",
        "from tensorflow.keras.layers import Dense , Flatten ,Embedding,Input,LSTM, Bidirectional, Dense, Dropout,GRU,SpatialDropout1D, Conv1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.layers import SpatialDropout1D\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "\n",
        "batch_size = 64\n",
        "no_classes = 100\n",
        "no_epochs = 5\n",
        "optimizer = Adam()\n",
        "verbosity = 1\n",
        "num_folds = 5\n",
        "\n",
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "for train, test in kfold.split(inputs, targets):\n",
        "\n",
        "    # Define the model architecture\n",
        "    embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
        "                                          100,\n",
        "                                          weights=[embedding_matrix],\n",
        "                                          input_length=60,\n",
        "                                          trainable=False)\n",
        "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "    embedding_sequences = embedding_layer(sequence_input)\n",
        "    x = SpatialDropout1D(0.2)(embedding_sequences)\n",
        "    x = LSTM(units=128, dropout=0.25, recurrent_dropout=0.25)(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dropout(0.25)(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "    model = tf.keras.Model(sequence_input, outputs)\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  # model.summary()\n",
        "\n",
        "  # Generate a print\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  # Fit data to model\n",
        "    history = model.fit(inputs[train], targets[train],\n",
        "              batch_size=batch_size,\n",
        "              epochs=no_epochs,\n",
        "              verbose=verbosity)\n",
        "    # model_path = \"content_english/TwitterHateModels_word2vec/\" + str(fold_no) + \"model\"\n",
        "    \n",
        "    # convert the history.history dict to a pandas DataFrame:     \n",
        "    # hist_df = pd.DataFrame(history.history) \n",
        "\n",
        "    # save to json:  \n",
        "    # hist_json_file = \"content_english/TwitterHateHistory_word2vec/\" + str(fold_no) + 'history.json' \n",
        "    # with open(hist_json_file, mode='w') as f:\n",
        "    #     hist_df.to_json(f)\n",
        "            \n",
        "    #save model\n",
        "    # model.save(model_path)\n",
        "    \n",
        "  # Generate generalization metrics\n",
        "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
        "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "  # Increase fold number\n",
        "    fold_no = fold_no + 1\n",
        "\n",
        "  # == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1LzedeE1eDa",
        "outputId": "3b601ab9-2a27-4d75-fa96-85f252c52bc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hello\n"
          ]
        }
      ],
      "source": [
        "print(\"hello\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn0pAUZnLiKY"
      },
      "source": [
        "# Glove and 5-fold cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "axB5aS_JK6xL",
        "outputId": "05c5ff06-d8a5-46a8-85fa-10ce036eb066"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary Size : 33196\n"
          ]
        }
      ],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(\"Vocabulary Size :\", vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6cfaY656WFK",
        "outputId": "f31d872a-d049-4b23-bbab-429ab1896a41"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mostafa/env/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
            "  warnings.warn(msg)\n",
            "/tmp/ipykernel_2383/19554426.py:4: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
            "  glove2word2vec(glove_input_file, word2vec_output_file)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(400000, 100)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "glove_input_file = 'glove.6B.100d.txt'\n",
        "word2vec_output_file = 'glove.6B.100d.txt.word2vec'\n",
        "glove2word2vec(glove_input_file, word2vec_output_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y059NkH-6WFL",
        "outputId": "33eca3c9-cdaf-4a40-db96-391692d78dc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('queen', 0.7698540687561035)]\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "# load the Stanford GloVe model\n",
        "filename = 'glove.6B.100d.txt.word2vec'\n",
        "model = KeyedVectors.load_word2vec_format(filename, binary=False)\n",
        "# calculate: (king - man) + woman = ?\n",
        "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6WSdZVU6WFM",
        "outputId": "e3dd2314-5d95-4762-b50d-70abf02c1f10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding Matrix Shape: (33196, 100)\n"
          ]
        }
      ],
      "source": [
        "embedding_matrix = np.zeros((vocab_size, 100))\n",
        "\n",
        "for word, token in tokenizer.word_index.items():\n",
        "    if model.__contains__(word):\n",
        "        embedding_matrix[token] = model.__getitem__(word)\n",
        "\n",
        "print(\"Embedding Matrix Shape:\", embedding_matrix.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1cPIGty6WFN",
        "outputId": "3b905c5a-31d1-4141-d8e4-66625729a045"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.33096999,  0.082649  ,  0.66940999, ...,  0.14552   ,\n",
              "         0.096472  , -0.28279001],\n",
              "       [-0.1644    , -0.27706   , -0.33697   , ...,  0.17838   ,\n",
              "         0.67962003, -0.13873   ],\n",
              "       ...,\n",
              "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [-0.48414001, -0.094258  ,  0.0594    , ..., -0.25029999,\n",
              "         0.13060001, -0.27077001],\n",
              "       [ 0.30818   ,  0.56160003,  1.10650003, ..., -0.54808998,\n",
              "        -0.044094  ,  0.79635   ]])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedding_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HATE DETECT GLOVE"
      ],
      "metadata": {
        "id": "wvD-bko464_B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2D0PrQ5K9Lz",
        "outputId": "3aa86551-4b54-411e-e6a4-cd4c60d71e30"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-04-23 06:02:31.969968: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
            "2022-04-23 06:02:31.986863: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
            "2022-04-23 06:02:31.997544: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (mostafa): /proc/driver/nvidia/version does not exist\n",
            "2022-04-23 06:02:32.112148: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/5\n",
            "710/710 [==============================] - 840s 1s/step - loss: 0.3217 - accuracy: 0.8644\n",
            "Epoch 2/5\n",
            "710/710 [==============================] - 933s 1s/step - loss: 0.2760 - accuracy: 0.8856\n",
            "Epoch 3/5\n",
            "710/710 [==============================] - 625s 879ms/step - loss: 0.2593 - accuracy: 0.8944\n",
            "Epoch 4/5\n",
            "710/710 [==============================] - 625s 880ms/step - loss: 0.2490 - accuracy: 0.8970\n",
            "Epoch 5/5\n",
            "710/710 [==============================] - 717s 1s/step - loss: 0.2384 - accuracy: 0.9019\n",
            "Score for fold 1: loss of 0.2301158607006073; accuracy of 90.64234495162964%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/5\n",
            "710/710 [==============================] - 718s 938ms/step - loss: 0.3068 - accuracy: 0.8731\n",
            "Epoch 2/5\n",
            "710/710 [==============================] - 661s 931ms/step - loss: 0.2606 - accuracy: 0.8935\n",
            "Epoch 3/5\n",
            "710/710 [==============================] - 665s 936ms/step - loss: 0.2454 - accuracy: 0.8991\n",
            "Epoch 4/5\n",
            "710/710 [==============================] - 664s 935ms/step - loss: 0.2346 - accuracy: 0.9036\n",
            "Epoch 5/5\n",
            "710/710 [==============================] - 668s 941ms/step - loss: 0.2266 - accuracy: 0.9050\n",
            "Score for fold 2: loss of 0.22255489230155945; accuracy of 90.43087363243103%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/5\n",
            "710/710 [==============================] - 744s 937ms/step - loss: 0.3061 - accuracy: 0.8718\n",
            "Epoch 2/5\n",
            "710/710 [==============================] - 671s 945ms/step - loss: 0.2600 - accuracy: 0.8928\n",
            "Epoch 3/5\n",
            "710/710 [==============================] - 669s 942ms/step - loss: 0.2419 - accuracy: 0.8995\n",
            "Epoch 4/5\n",
            "710/710 [==============================] - 666s 939ms/step - loss: 0.2323 - accuracy: 0.9026\n",
            "Epoch 5/5\n",
            "710/710 [==============================] - 671s 945ms/step - loss: 0.2256 - accuracy: 0.9053\n",
            "Score for fold 3: loss of 0.22727316617965698; accuracy of 90.52780270576477%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/5\n",
            "710/710 [==============================] - 688s 947ms/step - loss: 0.3047 - accuracy: 0.8747\n",
            "Epoch 2/5\n",
            "710/710 [==============================] - 669s 942ms/step - loss: 0.2617 - accuracy: 0.8930\n",
            "Epoch 3/5\n",
            "710/710 [==============================] - 673s 948ms/step - loss: 0.2445 - accuracy: 0.8990\n",
            "Epoch 4/5\n",
            "710/710 [==============================] - 668s 941ms/step - loss: 0.2340 - accuracy: 0.9039\n",
            "Epoch 5/5\n",
            "710/710 [==============================] - 671s 944ms/step - loss: 0.2269 - accuracy: 0.9047\n",
            "Score for fold 4: loss of 0.22514857351779938; accuracy of 90.46611785888672%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/5\n",
            "710/710 [==============================] - 726s 1s/step - loss: 0.3073 - accuracy: 0.8719\n",
            "Epoch 2/5\n",
            "710/710 [==============================] - 683s 962ms/step - loss: 0.2608 - accuracy: 0.8932\n",
            "Epoch 3/5\n",
            "710/710 [==============================] - 666s 939ms/step - loss: 0.2460 - accuracy: 0.8987\n",
            "Epoch 4/5\n",
            "710/710 [==============================] - 668s 940ms/step - loss: 0.2361 - accuracy: 0.9013\n",
            "Epoch 5/5\n",
            "710/710 [==============================] - 667s 939ms/step - loss: 0.2267 - accuracy: 0.9043\n",
            "Score for fold 5: loss of 0.2172069102525711; accuracy of 91.3736879825592%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.2301158607006073 - Accuracy: 90.64234495162964%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.22255489230155945 - Accuracy: 90.43087363243103%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.22727316617965698 - Accuracy: 90.52780270576477%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.22514857351779938 - Accuracy: 90.46611785888672%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.2172069102525711 - Accuracy: 91.3736879825592%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 90.68816542625427 (+- 0.3502173846521897)\n",
            "> Loss: 0.22445988059043884\n",
            "------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Model configuration\n",
        "from tensorflow.keras.layers import Dense , Flatten ,Embedding,Input,LSTM, Bidirectional, Dense, Dropout,GRU,SpatialDropout1D, Conv1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.layers import SpatialDropout1D\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "\n",
        "batch_size = 64\n",
        "no_classes = 100\n",
        "no_epochs = 5\n",
        "optimizer = Adam()\n",
        "verbosity = 1\n",
        "num_folds = 5\n",
        "\n",
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "\n",
        "fold_no = 1\n",
        "for train, test in kfold.split(inputs, targets):\n",
        "\n",
        "    # Define the model architecture\n",
        "    embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
        "                                          100,\n",
        "                                          weights=[embedding_matrix],\n",
        "                                          input_length=60,\n",
        "                                          trainable=False)\n",
        "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "    embedding_sequences = embedding_layer(sequence_input)\n",
        "    x = SpatialDropout1D(0.2)(embedding_sequences)\n",
        "    x = Bidirectional(GRU(units=128, return_sequences = True, dropout=0.25, recurrent_dropout=0.25))(x)\n",
        "    x = Bidirectional(GRU(units=128, return_sequences = True, dropout=0.25, recurrent_dropout=0.25))(x)\n",
        "    x = attention()(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dropout(0.25)(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "    model = tf.keras.Model(sequence_input, outputs)\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  # model.summary()\n",
        "\n",
        "  # Generate a print\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  # Fit data to model\n",
        "    history = model.fit(inputs[train], targets[train],\n",
        "              batch_size=batch_size,\n",
        "              epochs=no_epochs,\n",
        "              verbose=verbosity)\n",
        "    # model_path = \"content_english/TwitterHateModels_word2vec/\" + str(fold_no) + \"model\"\n",
        "    \n",
        "    # convert the history.history dict to a pandas DataFrame:     \n",
        "    # hist_df = pd.DataFrame(history.history) \n",
        "\n",
        "    # save to json:  \n",
        "    # hist_json_file = \"content_english/TwitterHateHistory_word2vec/\" + str(fold_no) + 'history.json' \n",
        "    # with open(hist_json_file, mode='w') as f:\n",
        "    #     hist_df.to_json(f)\n",
        "            \n",
        "    #save model\n",
        "    # model.save(model_path)\n",
        "    \n",
        "  # Generate generalization metrics\n",
        "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
        "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "  # Increase fold number\n",
        "    fold_no = fold_no + 1\n",
        "\n",
        "  # == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BI-GRU GLOVE"
      ],
      "metadata": {
        "id": "ueOvG3SI69wK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYUEgJnA6WFP",
        "outputId": "76a49343-e72d-4f3e-9cc6-92bde11bedf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/5\n",
            "710/710 [==============================] - 210s 285ms/step - loss: 0.3306 - accuracy: 0.8601\n",
            "Epoch 2/5\n",
            "710/710 [==============================] - 196s 276ms/step - loss: 0.2792 - accuracy: 0.8832\n",
            "Epoch 3/5\n",
            "710/710 [==============================] - 196s 276ms/step - loss: 0.2614 - accuracy: 0.8914\n",
            "Epoch 4/5\n",
            "710/710 [==============================] - 196s 276ms/step - loss: 0.2506 - accuracy: 0.8948\n",
            "Epoch 5/5\n",
            "710/710 [==============================] - 222s 312ms/step - loss: 0.2411 - accuracy: 0.8994\n",
            "Score for fold 1: loss of 0.2293679565191269; accuracy of 90.55423140525818%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/5\n",
            "710/710 [==============================] - 226s 307ms/step - loss: 0.3065 - accuracy: 0.8726\n",
            "Epoch 2/5\n",
            "710/710 [==============================] - 200s 281ms/step - loss: 0.2630 - accuracy: 0.8905\n",
            "Epoch 3/5\n",
            "710/710 [==============================] - 199s 281ms/step - loss: 0.2490 - accuracy: 0.8958\n",
            "Epoch 4/5\n",
            "710/710 [==============================] - 200s 281ms/step - loss: 0.2388 - accuracy: 0.9002\n",
            "Epoch 5/5\n",
            "710/710 [==============================] - 201s 283ms/step - loss: 0.2301 - accuracy: 0.9026\n",
            "Score for fold 2: loss of 0.23006755113601685; accuracy of 90.32514095306396%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/5\n",
            "710/710 [==============================] - 205s 280ms/step - loss: 0.3066 - accuracy: 0.8715\n",
            "Epoch 2/5\n",
            "710/710 [==============================] - 199s 280ms/step - loss: 0.2627 - accuracy: 0.8902\n",
            "Epoch 3/5\n",
            "710/710 [==============================] - 226s 319ms/step - loss: 0.2485 - accuracy: 0.8966\n",
            "Epoch 4/5\n",
            "710/710 [==============================] - 226s 318ms/step - loss: 0.2388 - accuracy: 0.9007\n",
            "Epoch 5/5\n",
            "710/710 [==============================] - 224s 315ms/step - loss: 0.2302 - accuracy: 0.9031\n",
            "Score for fold 3: loss of 0.2459828406572342; accuracy of 90.2722716331482%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/5\n",
            "710/710 [==============================] - 239s 324ms/step - loss: 0.3071 - accuracy: 0.8710\n",
            "Epoch 2/5\n",
            "710/710 [==============================] - 250s 352ms/step - loss: 0.2646 - accuracy: 0.8907\n",
            "Epoch 3/5\n",
            "710/710 [==============================] - 298s 420ms/step - loss: 0.2485 - accuracy: 0.8960\n",
            "Epoch 4/5\n",
            "710/710 [==============================] - 237s 333ms/step - loss: 0.2392 - accuracy: 0.9000\n",
            "Epoch 5/5\n",
            "710/710 [==============================] - 229s 322ms/step - loss: 0.2313 - accuracy: 0.9028\n",
            "Score for fold 4: loss of 0.22751617431640625; accuracy of 90.4837429523468%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/5\n",
            "710/710 [==============================] - 243s 309ms/step - loss: 0.3053 - accuracy: 0.8718\n",
            "Epoch 2/5\n",
            "710/710 [==============================] - 219s 308ms/step - loss: 0.2637 - accuracy: 0.8905\n",
            "Epoch 3/5\n",
            "710/710 [==============================] - 210s 295ms/step - loss: 0.2526 - accuracy: 0.8944\n",
            "Epoch 4/5\n",
            "710/710 [==============================] - 222s 312ms/step - loss: 0.2399 - accuracy: 0.8986\n",
            "Epoch 5/5\n",
            "710/710 [==============================] - 221s 311ms/step - loss: 0.2332 - accuracy: 0.9016\n",
            "Score for fold 5: loss of 0.2235572636127472; accuracy of 90.84500670433044%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.21968962252140045 - Accuracy: 90.66877961158752%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.22542496025562286 - Accuracy: 90.60710072517395%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.23883076012134552 - Accuracy: 90.2898907661438%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.23056155443191528 - Accuracy: 90.61591625213623%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.23075366020202637 - Accuracy: 90.42206406593323%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 6 - Loss: 0.2293679565191269 - Accuracy: 90.55423140525818%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 7 - Loss: 0.23006755113601685 - Accuracy: 90.32514095306396%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 8 - Loss: 0.2459828406572342 - Accuracy: 90.2722716331482%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 9 - Loss: 0.22751617431640625 - Accuracy: 90.4837429523468%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 10 - Loss: 0.2235572636127472 - Accuracy: 90.84500670433044%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 90.50841450691223 (+- 0.17537856743241786)\n",
            "> Loss: 0.23017523437738419\n",
            "------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Model configuration\n",
        "from tensorflow.keras.layers import Dense , Flatten ,Embedding,Input,LSTM, Bidirectional, Dense, Dropout,GRU,SpatialDropout1D, Conv1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.layers import SpatialDropout1D\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "\n",
        "batch_size = 64\n",
        "no_classes = 100\n",
        "no_epochs = 5\n",
        "optimizer = Adam()\n",
        "verbosity = 1\n",
        "num_folds = 5\n",
        "\n",
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "for train, test in kfold.split(inputs, targets):\n",
        "\n",
        "    # Define the model architecture\n",
        "    embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
        "                                          100,\n",
        "                                          weights=[embedding_matrix],\n",
        "                                          input_length=60,\n",
        "                                          trainable=False)\n",
        "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "    embedding_sequences = embedding_layer(sequence_input)\n",
        "    x = SpatialDropout1D(0.2)(embedding_sequences)\n",
        "    x = Bidirectional(GRU(units=128, dropout=0.25, recurrent_dropout=0.25))(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dropout(0.25)(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "    model = tf.keras.Model(sequence_input, outputs)\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  # model.summary()\n",
        "\n",
        "  # Generate a print\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  # Fit data to model\n",
        "    history = model.fit(inputs[train], targets[train],\n",
        "              batch_size=batch_size,\n",
        "              epochs=no_epochs,\n",
        "              verbose=verbosity)\n",
        "    # model_path = \"content_english/TwitterHateModels_word2vec/\" + str(fold_no) + \"model\"\n",
        "    \n",
        "    # convert the history.history dict to a pandas DataFrame:     \n",
        "    # hist_df = pd.DataFrame(history.history) \n",
        "\n",
        "    # save to json:  \n",
        "    # hist_json_file = \"content_english/TwitterHateHistory_word2vec/\" + str(fold_no) + 'history.json' \n",
        "    # with open(hist_json_file, mode='w') as f:\n",
        "    #     hist_df.to_json(f)\n",
        "            \n",
        "    #save model\n",
        "    # model.save(model_path)\n",
        "    \n",
        "  # Generate generalization metrics\n",
        "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
        "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "  # Increase fold number\n",
        "    fold_no = fold_no + 1\n",
        "\n",
        "  # == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM GLOVE"
      ],
      "metadata": {
        "id": "UBHU_3Hl7B9L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzUUe2W-6WFR",
        "outputId": "e3b92f0b-88dd-4cca-91f6-03d782e6161e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-04-22 19:51:55.978282: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
            "2022-04-22 19:51:55.978394: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
            "2022-04-22 19:51:55.978466: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (mostafa): /proc/driver/nvidia/version does not exist\n",
            "2022-04-22 19:51:55.998998: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/5\n",
            "710/710 [==============================] - 174s 237ms/step - loss: 0.3360 - accuracy: 0.8564\n",
            "Epoch 2/5\n",
            "710/710 [==============================] - 162s 227ms/step - loss: 0.2769 - accuracy: 0.8847\n",
            "Epoch 3/5\n",
            "710/710 [==============================] - 160s 225ms/step - loss: 0.2606 - accuracy: 0.8914\n",
            "Epoch 4/5\n",
            "710/710 [==============================] - 180s 253ms/step - loss: 0.2482 - accuracy: 0.8960\n",
            "Epoch 5/5\n",
            "710/710 [==============================] - 165s 232ms/step - loss: 0.2405 - accuracy: 0.8993\n",
            "Score for fold 1: loss of 0.21968962252140045; accuracy of 90.66877961158752%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/5\n",
            "710/710 [==============================] - 160s 218ms/step - loss: 0.3120 - accuracy: 0.8653\n",
            "Epoch 2/5\n",
            "710/710 [==============================] - 160s 226ms/step - loss: 0.2619 - accuracy: 0.8914\n",
            "Epoch 3/5\n",
            "710/710 [==============================] - 179s 251ms/step - loss: 0.2483 - accuracy: 0.8955\n",
            "Epoch 4/5\n",
            "710/710 [==============================] - 182s 256ms/step - loss: 0.2379 - accuracy: 0.8997\n",
            "Epoch 5/5\n",
            "710/710 [==============================] - 149s 210ms/step - loss: 0.2264 - accuracy: 0.9042\n",
            "Score for fold 2: loss of 0.22542496025562286; accuracy of 90.60710072517395%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/5\n",
            "710/710 [==============================] - 162s 222ms/step - loss: 0.3051 - accuracy: 0.8722\n",
            "Epoch 2/5\n",
            "710/710 [==============================] - 149s 209ms/step - loss: 0.2592 - accuracy: 0.8920\n",
            "Epoch 3/5\n",
            "710/710 [==============================] - 145s 205ms/step - loss: 0.2449 - accuracy: 0.8973\n",
            "Epoch 4/5\n",
            "710/710 [==============================] - 141s 198ms/step - loss: 0.2346 - accuracy: 0.9033\n",
            "Epoch 5/5\n",
            "710/710 [==============================] - 136s 192ms/step - loss: 0.2268 - accuracy: 0.9055\n",
            "Score for fold 3: loss of 0.23883076012134552; accuracy of 90.2898907661438%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/5\n",
            "710/710 [==============================] - 146s 200ms/step - loss: 0.3062 - accuracy: 0.8711\n",
            "Epoch 2/5\n",
            "710/710 [==============================] - 135s 190ms/step - loss: 0.2589 - accuracy: 0.8907\n",
            "Epoch 3/5\n",
            "710/710 [==============================] - 133s 187ms/step - loss: 0.2446 - accuracy: 0.8979\n",
            "Epoch 4/5\n",
            "710/710 [==============================] - 134s 189ms/step - loss: 0.2339 - accuracy: 0.9015\n",
            "Epoch 5/5\n",
            "710/710 [==============================] - 134s 189ms/step - loss: 0.2254 - accuracy: 0.9053\n",
            "Score for fold 4: loss of 0.23056155443191528; accuracy of 90.61591625213623%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/5\n",
            "710/710 [==============================] - 137s 188ms/step - loss: 0.3050 - accuracy: 0.8726\n",
            "Epoch 2/5\n",
            "710/710 [==============================] - 133s 187ms/step - loss: 0.2603 - accuracy: 0.8918\n",
            "Epoch 3/5\n",
            "710/710 [==============================] - 153s 216ms/step - loss: 0.2446 - accuracy: 0.8975\n",
            "Epoch 4/5\n",
            "710/710 [==============================] - 149s 210ms/step - loss: 0.2328 - accuracy: 0.9020\n",
            "Epoch 5/5\n",
            "710/710 [==============================] - 133s 188ms/step - loss: 0.2246 - accuracy: 0.9057\n",
            "Score for fold 5: loss of 0.23075366020202637; accuracy of 90.42206406593323%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.21968962252140045 - Accuracy: 90.66877961158752%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.22542496025562286 - Accuracy: 90.60710072517395%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.23883076012134552 - Accuracy: 90.2898907661438%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.23056155443191528 - Accuracy: 90.61591625213623%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.23075366020202637 - Accuracy: 90.42206406593323%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 90.52075028419495 (+- 0.1424505027224381)\n",
            "> Loss: 0.2290521115064621\n",
            "------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Model configuration\n",
        "from tensorflow.keras.layers import Dense , Flatten ,Embedding,Input,LSTM, Bidirectional, Dense, Dropout,GRU,SpatialDropout1D, Conv1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.layers import SpatialDropout1D\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "\n",
        "batch_size = 64\n",
        "no_classes = 100\n",
        "no_epochs = 5\n",
        "optimizer = Adam()\n",
        "verbosity = 1\n",
        "num_folds = 5\n",
        "\n",
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "for train, test in kfold.split(inputs, targets):\n",
        "\n",
        "    # Define the model architecture\n",
        "    embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
        "                                          100,\n",
        "                                          weights=[embedding_matrix],\n",
        "                                          input_length=60,\n",
        "                                          trainable=False)\n",
        "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "    embedding_sequences = embedding_layer(sequence_input)\n",
        "    x = SpatialDropout1D(0.2)(embedding_sequences)\n",
        "    x = LSTM(units=128, dropout=0.25, recurrent_dropout=0.25)(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dropout(0.25)(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "    model = tf.keras.Model(sequence_input, outputs)\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  # model.summary()\n",
        "\n",
        "  # Generate a print\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  # Fit data to model\n",
        "    history = model.fit(inputs[train], targets[train],\n",
        "              batch_size=batch_size,\n",
        "              epochs=no_epochs,\n",
        "              verbose=verbosity)\n",
        "    # model_path = \"content_english/TwitterHateModels_word2vec/\" + str(fold_no) + \"model\"\n",
        "    \n",
        "    # convert the history.history dict to a pandas DataFrame:     \n",
        "    # hist_df = pd.DataFrame(history.history) \n",
        "\n",
        "    # save to json:  \n",
        "    # hist_json_file = \"content_english/TwitterHateHistory_word2vec/\" + str(fold_no) + 'history.json' \n",
        "    # with open(hist_json_file, mode='w') as f:\n",
        "    #     hist_df.to_json(f)\n",
        "            \n",
        "    #save model\n",
        "    # model.save(model_path)\n",
        "    \n",
        "  # Generate generalization metrics\n",
        "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
        "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "  # Increase fold number\n",
        "    fold_no = fold_no + 1\n",
        "\n",
        "  # == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BANGLA HATE SPEECH DETECTION"
      ],
      "metadata": {
        "id": "c5F9M6x17Fd7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bz6DXKJM6WFT",
        "outputId": "99cd4640-1a2c-4e54-c011-3a622e060c21"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-04-20 16:42:31.048927: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "2022-04-20 16:42:31.049009: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import one_hot,Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense , Flatten ,Embedding,Input,LSTM, Bidirectional, Dense, Dropout,GRU,SpatialDropout1D, Conv1D\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "from tensorflow.keras.initializers import Constant\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import np_utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Voujx1va6WFT"
      },
      "outputs": [],
      "source": [
        "file = '../content/cleaned_data.csv'\n",
        "data = pd.read_csv(file, encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "MHr-uOOv6WFU",
        "outputId": "13eac70f-8813-4a9d-e496-f7f19efe919d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>sentence</th>\n",
              "      <th>hate</th>\n",
              "      <th>category</th>\n",
              "      <th>cleaned_text</th>\n",
              "      <th>sentenceLength</th>\n",
              "      <th>ReviewChars</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>   !!!!!</td>\n",
              "      <td>1</td>\n",
              "      <td>sports</td>\n",
              "      <td>   </td>\n",
              "      <td>4</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>     </td>\n",
              "      <td>1</td>\n",
              "      <td>sports</td>\n",
              "      <td>     </td>\n",
              "      <td>6</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>        ...</td>\n",
              "      <td>1</td>\n",
              "      <td>sports</td>\n",
              "      <td>        ...</td>\n",
              "      <td>20</td>\n",
              "      <td>107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>      </td>\n",
              "      <td>1</td>\n",
              "      <td>sports</td>\n",
              "      <td>      </td>\n",
              "      <td>7</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>        </td>\n",
              "      <td>1</td>\n",
              "      <td>sports</td>\n",
              "      <td>         </td>\n",
              "      <td>10</td>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0                                           sentence  hate  \\\n",
              "0           0                        !!!!!     1   \n",
              "1           1                            1   \n",
              "2           2          ...     1   \n",
              "3           3                           1   \n",
              "4           4                1   \n",
              "\n",
              "  category                                       cleaned_text  sentenceLength  \\\n",
              "0   sports                                            4   \n",
              "1   sports                                      6   \n",
              "2   sports          ...              20   \n",
              "3   sports                                     7   \n",
              "4   sports                          10   \n",
              "\n",
              "   ReviewChars  \n",
              "0           30  \n",
              "1           33  \n",
              "2          107  \n",
              "3           35  \n",
              "4           48  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5AagzK-6WFV",
        "outputId": "c07395a6-e253-4825-fa4f-fa90f9a7fd44"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((24000,), (6000,), (24000,), (6000,))"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['cleaned_text'], data['hate'],random_state = 0,test_size = 0.20)\n",
        "X_train.shape,X_test.shape,y_train.shape,y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgF0EUyl6WFW",
        "outputId": "befbcbad-5ac8-4d44-a5e5-2f3cbcc76d75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total words =  509593\n",
            "Total Sentances =  30000\n"
          ]
        }
      ],
      "source": [
        "corpus=[]\n",
        "words = 0;\n",
        "j = 0\n",
        "for i in data['cleaned_text'].values:\n",
        "    corpus.append(str(i).split(\" \"))\n",
        "    words += len(corpus[j])\n",
        "    j += 1\n",
        "print(\"Total words = \", words)\n",
        "print(\"Total Sentances = \", len(corpus))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MACHINE LEARNING CLASSIFIERS"
      ],
      "metadata": {
        "id": "Rt41tLh57NAr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRVbQpSM6WFW"
      },
      "outputs": [],
      "source": [
        "Tfidf_vect = TfidfVectorizer(max_features=4000)\n",
        "Tfidf_vect.fit(data['cleaned_text'])\n",
        "Train_X_Tfidf = Tfidf_vect.transform(X_train)\n",
        "Test_X_Tfidf = Tfidf_vect.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4FwAmiO6WFX",
        "outputId": "3de59e78-4515-4fe4-fcb0-dc6cc875d1da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Naive Bayes Training Accuracy Score ->  74.75\n",
            "Naive Bayes Test Accuracy Score ->  73.33333333333333\n"
          ]
        }
      ],
      "source": [
        "model = MultinomialNB()\n",
        "model.fit(Train_X_Tfidf,y_train)\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "predictions_train_NB = model.predict(Train_X_Tfidf)\n",
        "predictions_NB = model.predict(Test_X_Tfidf)\n",
        "print(\"Naive Bayes Training Accuracy Score -> \",accuracy_score(predictions_train_NB, y_train)*100)\n",
        "print(\"Naive Bayes Test Accuracy Score -> \",accuracy_score(predictions_NB, y_test)*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faNHT9ml6WFZ",
        "outputId": "65d54663-2f9b-4a18-9ba2-70798870cf3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVM Train Accuracy Score ->  76.27916666666667\n",
            "SVM Test Accuracy Score ->  75.38333333333334\n"
          ]
        }
      ],
      "source": [
        "from sklearn import svm\n",
        "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
        "SVM.fit(Train_X_Tfidf,y_train)\n",
        "pred_y_train = SVM.predict(Train_X_Tfidf)\n",
        "pred_y = SVM.predict(Test_X_Tfidf)\n",
        "# Use accuracy_score function to get the accuracy\n",
        "print(\"SVM Train Accuracy Score -> \",accuracy_score(pred_y_train, y_train)*100)\n",
        "print(\"SVM Test Accuracy Score -> \",accuracy_score(pred_y, y_test)*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wr2BRQgu6WFZ",
        "outputId": "248c8d21-a3a4-46e2-975e-d669a71c03aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Accuracy Score ->  94.55\n",
            "Test Accuracy Score ->  74.1\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "RF=RandomForestClassifier(n_estimators=100)\n",
        "RF.fit(Train_X_Tfidf,y_train)\n",
        "pred_y_train = RF.predict(Train_X_Tfidf)\n",
        "pred_y = RF.predict(Test_X_Tfidf)\n",
        "# Use accuracy_score function to get the accuracy\n",
        "print(\"Train Accuracy Score -> \",accuracy_score(pred_y_train, y_train)*100)\n",
        "print(\"Test Accuracy Score -> \",accuracy_score(pred_y, y_test)*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "qCZFYqBr6WFZ",
        "outputId": "3718458c-1eea-4424-9d0d-8369a1b5ecbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Accuracy Score ->  94.55\n",
            "Test Accuracy Score ->  68.53333333333333\n"
          ]
        }
      ],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "clf = DecisionTreeClassifier()\n",
        "\n",
        "clf = clf.fit(Train_X_Tfidf,y_train)\n",
        "\n",
        "pred_y_train = clf.predict(Train_X_Tfidf)\n",
        "pred_y = clf.predict(Test_X_Tfidf)\n",
        "# Use accuracy_score function to get the accuracy\n",
        "print(\"Train Accuracy Score -> \",accuracy_score(pred_y_train, y_train)*100)\n",
        "print(\"Test Accuracy Score -> \",accuracy_score(pred_y, y_test)*100)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WORD2VEC AND 5-FOLD CROSS VALIDATION"
      ],
      "metadata": {
        "id": "_oIJXeRk7RG3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyiNN7d76WFa",
        "outputId": "22c4ba19-8090-40bf-aba0-eb80080899f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of vocabulary :  47752\n",
            "All sentances with same length  (30000, 60)\n"
          ]
        }
      ],
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "\n",
        "# tokenize every words so that evey words maps to numaric value\n",
        "tok = Tokenizer()\n",
        "tok.fit_on_texts(data['cleaned_text'].values.astype('U'))\n",
        "encd_rev = tok.texts_to_sequences(data['cleaned_text'].values.astype('U'))\n",
        "# tok.word_index\n",
        "\n",
        "# make all the input sentance same length with add padding\n",
        "max_rev_len = 60 # max lenght of a sentance\n",
        "vocab_size = len(tok.word_index) + 1  # total no of words\n",
        "embed_dim = 100 # embedding dimension \n",
        "pad_rev= pad_sequences(encd_rev, maxlen=60, padding='post')\n",
        "\n",
        "print(\"Size of vocabulary : \", vocab_size)\n",
        "print(\"All sentances with same length \", pad_rev.shape)\n",
        "\n",
        "# lebel encode the output label to categorical\n",
        "Y = data['hate']\n",
        "# encode class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "encoded_Y = encoder.transform(Y)\n",
        "\n",
        "\n",
        "# test train split\n",
        "X_train,X_test,y_train,y_test=train_test_split(pad_rev,encoded_Y,test_size=0.2, random_state = 0)\n",
        "\n",
        "# make a dictionary. word as key and feature vector as value\n",
        "embedding_index={}\n",
        "f = open('../content/ban_hate_w2v.txt',encoding='utf-8')\n",
        "for line in f:\n",
        "    values=line.split()\n",
        "    word=values[0]\n",
        "    coefs=np.asarray(values[1:])\n",
        "    embedding_index[word]=coefs\n",
        "f.close()\n",
        "\n",
        "# create a embeddings matrix with 200 dimenstion\n",
        "EMBEDDING_DIM=100\n",
        "embedding_matrix=np.zeros((vocab_size,EMBEDDING_DIM))\n",
        "for word, i in tok.word_index.items():\n",
        "    if i>vocab_size:\n",
        "        continue\n",
        "    embedding_vector=embedding_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i]=embedding_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FKG7hdC6WFb",
        "outputId": "e20428d3-3f15-4e76-9d50-e9a2399fdde9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((24000, 60), (6000, 60), (24000,), (6000,))"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "961sLF9w6WFd",
        "outputId": "44bd6080-b94a-498b-e2ea-418d1861be77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(24000, 1) (6000, 1)\n"
          ]
        }
      ],
      "source": [
        "y_train = y_train.reshape(-1, 1)\n",
        "y_test = y_test.reshape(-1, 1)\n",
        "print(y_train.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVV0d8Mk6WFd",
        "outputId": "78b1ebf8-e3db-4d92-ac43-d374d7cb2aeb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((30000, 60), (30000, 1))"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define per-fold score containers\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "# Merge inputs and targets\n",
        "inputs = np.concatenate((X_train, X_test), axis=0)\n",
        "targets = np.concatenate((y_train, y_test), axis=0)\n",
        "inputs.shape, targets.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HATE DETECT W2V"
      ],
      "metadata": {
        "id": "I-yxqaP07eO8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "a7NPib-P6WFe",
        "outputId": "d6a14ce7-f725-41be-c528-ca62c3516d53"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-04-17 09:09:42.507043: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 19100800 exceeds 10% of free system memory.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "375/375 [==============================] - 373s 962ms/step - loss: 0.4292 - accuracy: 0.8075\n",
            "Epoch 2/10\n",
            "375/375 [==============================] - 362s 963ms/step - loss: 0.3768 - accuracy: 0.8356\n",
            "Epoch 3/10\n",
            "375/375 [==============================] - 363s 969ms/step - loss: 0.3549 - accuracy: 0.8498\n",
            "Epoch 4/10\n",
            "375/375 [==============================] - 396s 1s/step - loss: 0.3392 - accuracy: 0.8543\n",
            "Epoch 5/10\n",
            "375/375 [==============================] - 314s 837ms/step - loss: 0.3311 - accuracy: 0.8589\n",
            "Epoch 6/10\n",
            "375/375 [==============================] - 314s 837ms/step - loss: 0.3218 - accuracy: 0.8617\n",
            "Epoch 7/10\n",
            "375/375 [==============================] - 339s 904ms/step - loss: 0.3119 - accuracy: 0.8678\n",
            "Epoch 8/10\n",
            "375/375 [==============================] - 411s 1s/step - loss: 0.3082 - accuracy: 0.8695\n",
            "Epoch 9/10\n",
            "375/375 [==============================] - 521s 1s/step - loss: 0.2995 - accuracy: 0.8726\n",
            "Epoch 10/10\n",
            "375/375 [==============================] - 485s 1s/step - loss: 0.2924 - accuracy: 0.8755\n",
            "Score for fold 1: loss of 0.3255404233932495; accuracy of 86.15000247955322%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "375/375 [==============================] - 587s 1s/step - loss: 0.4135 - accuracy: 0.8181\n",
            "Epoch 2/10\n",
            "375/375 [==============================] - 285s 759ms/step - loss: 0.3571 - accuracy: 0.8444\n",
            "Epoch 3/10\n",
            "375/375 [==============================] - 285s 759ms/step - loss: 0.3372 - accuracy: 0.8566\n",
            "Epoch 4/10\n",
            "375/375 [==============================] - 286s 763ms/step - loss: 0.3253 - accuracy: 0.8618\n",
            "Epoch 5/10\n",
            "375/375 [==============================] - 288s 768ms/step - loss: 0.3147 - accuracy: 0.8658\n",
            "Epoch 6/10\n",
            "375/375 [==============================] - 317s 846ms/step - loss: 0.3073 - accuracy: 0.8673\n",
            "Epoch 7/10\n",
            "375/375 [==============================] - 317s 846ms/step - loss: 0.3015 - accuracy: 0.8726\n",
            "Epoch 8/10\n",
            "375/375 [==============================] - 374s 998ms/step - loss: 0.2954 - accuracy: 0.8746\n",
            "Epoch 9/10\n",
            "375/375 [==============================] - 289s 770ms/step - loss: 0.2906 - accuracy: 0.8767\n",
            "Epoch 10/10\n",
            "375/375 [==============================] - 287s 766ms/step - loss: 0.2818 - accuracy: 0.8789\n",
            "Score for fold 2: loss of 0.3157474100589752; accuracy of 87.33333349227905%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "375/375 [==============================] - 323s 834ms/step - loss: 0.4105 - accuracy: 0.8203\n",
            "Epoch 2/10\n",
            "375/375 [==============================] - 302s 804ms/step - loss: 0.3542 - accuracy: 0.8496\n",
            "Epoch 3/10\n",
            "375/375 [==============================] - 301s 802ms/step - loss: 0.3355 - accuracy: 0.8586\n",
            "Epoch 4/10\n",
            "375/375 [==============================] - 302s 806ms/step - loss: 0.3228 - accuracy: 0.8636\n",
            "Epoch 5/10\n",
            "375/375 [==============================] - 289s 771ms/step - loss: 0.3128 - accuracy: 0.8691\n",
            "Epoch 6/10\n",
            "375/375 [==============================] - 310s 826ms/step - loss: 0.3067 - accuracy: 0.8704\n",
            "Epoch 7/10\n",
            "375/375 [==============================] - 319s 852ms/step - loss: 0.2983 - accuracy: 0.8757\n",
            "Epoch 8/10\n",
            "375/375 [==============================] - 319s 851ms/step - loss: 0.2901 - accuracy: 0.8790\n",
            "Epoch 9/10\n",
            "375/375 [==============================] - 321s 858ms/step - loss: 0.2860 - accuracy: 0.8809\n",
            "Epoch 10/10\n",
            "375/375 [==============================] - 322s 860ms/step - loss: 0.2808 - accuracy: 0.8832\n",
            "Score for fold 3: loss of 0.317362517118454; accuracy of 86.65000200271606%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/10\n",
            "375/375 [==============================] - 346s 862ms/step - loss: 0.4089 - accuracy: 0.8218\n",
            "Epoch 2/10\n",
            "375/375 [==============================] - 323s 859ms/step - loss: 0.3553 - accuracy: 0.8477\n",
            "Epoch 3/10\n",
            "375/375 [==============================] - 322s 858ms/step - loss: 0.3343 - accuracy: 0.8567\n",
            "Epoch 4/10\n",
            "375/375 [==============================] - 322s 858ms/step - loss: 0.3251 - accuracy: 0.8610\n",
            "Epoch 5/10\n",
            "375/375 [==============================] - 322s 859ms/step - loss: 0.3140 - accuracy: 0.8675\n",
            "Epoch 6/10\n",
            "375/375 [==============================] - 322s 859ms/step - loss: 0.3056 - accuracy: 0.8705\n",
            "Epoch 7/10\n",
            "375/375 [==============================] - 323s 862ms/step - loss: 0.2979 - accuracy: 0.8749\n",
            "Epoch 8/10\n",
            "375/375 [==============================] - 322s 859ms/step - loss: 0.2931 - accuracy: 0.8766\n",
            "Epoch 9/10\n",
            "375/375 [==============================] - 331s 882ms/step - loss: 0.2895 - accuracy: 0.8794\n",
            "Epoch 10/10\n",
            "375/375 [==============================] - 349s 931ms/step - loss: 0.2810 - accuracy: 0.8818\n",
            "Score for fold 4: loss of 0.31800708174705505; accuracy of 87.00000047683716%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/10\n",
            "375/375 [==============================] - 338s 868ms/step - loss: 0.4106 - accuracy: 0.8201\n",
            "Epoch 2/10\n",
            "375/375 [==============================] - 319s 850ms/step - loss: 0.3537 - accuracy: 0.8467\n",
            "Epoch 3/10\n",
            "375/375 [==============================] - 325s 867ms/step - loss: 0.3369 - accuracy: 0.8551\n",
            "Epoch 4/10\n",
            "375/375 [==============================] - 321s 855ms/step - loss: 0.3230 - accuracy: 0.8610\n",
            "Epoch 5/10\n",
            "375/375 [==============================] - 307s 816ms/step - loss: 0.3123 - accuracy: 0.8665\n",
            "Epoch 6/10\n",
            "375/375 [==============================] - 286s 763ms/step - loss: 0.3032 - accuracy: 0.8692\n",
            "Epoch 7/10\n",
            "375/375 [==============================] - 288s 767ms/step - loss: 0.2973 - accuracy: 0.8706\n",
            "Epoch 8/10\n",
            "375/375 [==============================] - 286s 764ms/step - loss: 0.2890 - accuracy: 0.8747\n",
            "Epoch 9/10\n",
            "375/375 [==============================] - 288s 768ms/step - loss: 0.2855 - accuracy: 0.8756\n",
            "Epoch 10/10\n",
            "375/375 [==============================] - 337s 898ms/step - loss: 0.2782 - accuracy: 0.8790\n",
            "Score for fold 5: loss of 0.33390888571739197; accuracy of 86.43333315849304%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.3255404233932495 - Accuracy: 86.15000247955322%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.3157474100589752 - Accuracy: 87.33333349227905%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.317362517118454 - Accuracy: 86.65000200271606%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.31800708174705505 - Accuracy: 87.00000047683716%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.33390888571739197 - Accuracy: 86.43333315849304%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 86.71333432197571 (+- 0.41625253258603695)\n",
            "> Loss: 0.32211326360702514\n",
            "------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Model configuration\n",
        "from tensorflow.keras.layers import Dense , Flatten ,Embedding,Input,LSTM, Bidirectional, Dense, Dropout,GRU,SpatialDropout1D, Conv1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.layers import SpatialDropout1D\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "\n",
        "batch_size = 64\n",
        "no_classes = 100\n",
        "no_epochs = 10\n",
        "optimizer = Adam()\n",
        "verbosity = 1\n",
        "num_folds = 5\n",
        "\n",
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "for train, test in kfold.split(inputs, targets):\n",
        "\n",
        "    # Define the model architecture\n",
        "    embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
        "                                          100,\n",
        "                                          weights=[embedding_matrix],\n",
        "                                          input_length=60,\n",
        "                                          trainable=False)\n",
        "    sequence_input = Input(shape=(60,), dtype='int32')\n",
        "    embedding_sequences = embedding_layer(sequence_input)\n",
        "    x = SpatialDropout1D(0.2)(embedding_sequences)\n",
        "    x = Bidirectional(GRU(units=128, return_sequences = True, dropout=0.25, recurrent_dropout=0.25))(x)\n",
        "    x = Bidirectional(GRU(units=128, return_sequences = True, dropout=0.25, recurrent_dropout=0.25))(x)\n",
        "    x = attention()(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dropout(0.25)(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "    model = tf.keras.Model(sequence_input, outputs)\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  # model.summary()\n",
        "\n",
        "  # Generate a print\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  # Fit data to model\n",
        "    history = model.fit(inputs[train], targets[train],\n",
        "              batch_size=batch_size,\n",
        "              epochs=no_epochs,\n",
        "              verbose=verbosity)\n",
        "    model_path = \"..content/w2v_models/\" + str(fold_no) + \"model\"\n",
        "    \n",
        "#     convert the history.history dict to a pandas DataFrame:     \n",
        "    hist_df = pd.DataFrame(history.history) \n",
        "\n",
        "#     save to json:  \n",
        "    hist_json_file = \"..content/w2v_history\" + str(fold_no) + 'history.json' \n",
        "    with open(hist_json_file, mode='w') as f:\n",
        "        hist_df.to_json(f)\n",
        "            \n",
        "#     save model\n",
        "    model.save(model_path)\n",
        "    \n",
        "  # Generate generalization metrics\n",
        "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
        "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "  # Increase fold number\n",
        "    fold_no = fold_no + 1\n",
        "\n",
        "  # == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BI-GRU W2V"
      ],
      "metadata": {
        "id": "lbuwTESs7hxi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "1t2R8fC-6WFg",
        "outputId": "7b79bbda-dade-4fa5-8c64-81a83c68c0ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/5\n",
            "375/375 [==============================] - 172s 384ms/step - loss: 0.4514 - accuracy: 0.7949\n",
            "Epoch 2/5\n",
            "375/375 [==============================] - 138s 367ms/step - loss: 0.3984 - accuracy: 0.8227\n",
            "Epoch 3/5\n",
            "375/375 [==============================] - 150s 400ms/step - loss: 0.3762 - accuracy: 0.8340\n",
            "Epoch 4/5\n",
            "375/375 [==============================] - 127s 337ms/step - loss: 0.3614 - accuracy: 0.8412\n",
            "Epoch 5/5\n",
            "375/375 [==============================] - 128s 341ms/step - loss: 0.3495 - accuracy: 0.8456\n",
            "Score for fold 1: loss of 0.36566096544265747; accuracy of 85.25000214576721%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/5\n",
            "375/375 [==============================] - 463s 309ms/step - loss: 0.4318 - accuracy: 0.8075\n",
            "Epoch 2/5\n",
            "375/375 [==============================] - 100s 267ms/step - loss: 0.3822 - accuracy: 0.8331\n",
            "Epoch 3/5\n",
            "375/375 [==============================] - 100s 265ms/step - loss: 0.3629 - accuracy: 0.8413\n",
            "Epoch 4/5\n",
            "375/375 [==============================] - 100s 265ms/step - loss: 0.3512 - accuracy: 0.8467\n",
            "Epoch 5/5\n",
            "375/375 [==============================] - 100s 266ms/step - loss: 0.3425 - accuracy: 0.8481\n",
            "Score for fold 2: loss of 0.3272828459739685; accuracy of 86.33333444595337%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/5\n",
            "375/375 [==============================] - 116s 265ms/step - loss: 0.4299 - accuracy: 0.8077\n",
            "Epoch 2/5\n",
            "375/375 [==============================] - 94s 251ms/step - loss: 0.3802 - accuracy: 0.8327\n",
            "Epoch 3/5\n",
            "375/375 [==============================] - 95s 254ms/step - loss: 0.3569 - accuracy: 0.8450\n",
            "Epoch 4/5\n",
            "375/375 [==============================] - 95s 254ms/step - loss: 0.3496 - accuracy: 0.8487\n",
            "Epoch 5/5\n",
            "375/375 [==============================] - 95s 254ms/step - loss: 0.3396 - accuracy: 0.8540\n",
            "Score for fold 3: loss of 0.328748494386673; accuracy of 86.15000247955322%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/5\n",
            "375/375 [==============================] - 114s 250ms/step - loss: 0.4212 - accuracy: 0.8131\n",
            "Epoch 2/5\n",
            "375/375 [==============================] - 94s 252ms/step - loss: 0.3718 - accuracy: 0.8358\n",
            "Epoch 3/5\n",
            "375/375 [==============================] - 94s 250ms/step - loss: 0.3588 - accuracy: 0.8417\n",
            "Epoch 4/5\n",
            "375/375 [==============================] - 94s 250ms/step - loss: 0.3453 - accuracy: 0.8497\n",
            "Epoch 5/5\n",
            "375/375 [==============================] - 94s 250ms/step - loss: 0.3348 - accuracy: 0.8555\n",
            "Score for fold 4: loss of 0.3546135127544403; accuracy of 85.14999747276306%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/5\n",
            "375/375 [==============================] - 150s 251ms/step - loss: 0.4278 - accuracy: 0.8080\n",
            "Epoch 2/5\n",
            "375/375 [==============================] - 94s 250ms/step - loss: 0.3772 - accuracy: 0.8363\n",
            "Epoch 3/5\n",
            "375/375 [==============================] - 94s 251ms/step - loss: 0.3616 - accuracy: 0.8408\n",
            "Epoch 4/5\n",
            "375/375 [==============================] - 93s 247ms/step - loss: 0.3477 - accuracy: 0.8470\n",
            "Epoch 5/5\n",
            "375/375 [==============================] - 103s 273ms/step - loss: 0.3374 - accuracy: 0.8516\n",
            "Score for fold 5: loss of 0.33963847160339355; accuracy of 85.21666526794434%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.311845064163208 - Accuracy: 88.18333148956299%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.3438962399959564 - Accuracy: 87.99999952316284%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.3144559860229492 - Accuracy: 88.06666731834412%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.32712557911872864 - Accuracy: 87.4666690826416%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.33121421933174133 - Accuracy: 87.56666779518127%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 6 - Loss: 0.3089353144168854 - Accuracy: 87.56666779518127%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 7 - Loss: 0.3088591694831848 - Accuracy: 87.73333430290222%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 8 - Loss: 0.31690821051597595 - Accuracy: 86.2833321094513%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 9 - Loss: 0.3066258430480957 - Accuracy: 87.65000104904175%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 10 - Loss: 0.32030919194221497 - Accuracy: 87.05000281333923%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 11 - Loss: 0.3099775016307831 - Accuracy: 87.65000104904175%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 12 - Loss: 0.3076343536376953 - Accuracy: 87.03333139419556%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 13 - Loss: 0.33590734004974365 - Accuracy: 85.72726249694824%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 14 - Loss: 0.3103717267513275 - Accuracy: 87.34638690948486%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 15 - Loss: 0.3111148178577423 - Accuracy: 87.43554949760437%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 16 - Loss: 0.3178783059120178 - Accuracy: 86.50416135787964%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 17 - Loss: 0.31216880679130554 - Accuracy: 87.14332580566406%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 18 - Loss: 0.3370093107223511 - Accuracy: 85.92779040336609%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 19 - Loss: 0.33912110328674316 - Accuracy: 85.6732964515686%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 20 - Loss: 0.34115079045295715 - Accuracy: 85.52971482276917%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 21 - Loss: 0.35374337434768677 - Accuracy: 84.46693420410156%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 22 - Loss: 0.34680038690567017 - Accuracy: 85.87194085121155%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 23 - Loss: 0.36566096544265747 - Accuracy: 85.25000214576721%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 24 - Loss: 0.3272828459739685 - Accuracy: 86.33333444595337%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 25 - Loss: 0.328748494386673 - Accuracy: 86.15000247955322%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 26 - Loss: 0.3546135127544403 - Accuracy: 85.14999747276306%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 27 - Loss: 0.33963847160339355 - Accuracy: 85.21666526794434%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 86.665791493875 (+- 1.037641776292198)\n",
            "> Loss: 0.32699988616837394\n",
            "------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Model configuration\n",
        "from tensorflow.keras.layers import Dense , Flatten ,Embedding,Input,LSTM, Bidirectional, Dense, Dropout,GRU,SpatialDropout1D, Conv1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.layers import SpatialDropout1D\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "\n",
        "batch_size = 64\n",
        "no_classes = 100\n",
        "no_epochs = 5\n",
        "optimizer = Adam()\n",
        "verbosity = 1\n",
        "num_folds = 5\n",
        "\n",
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "for train, test in kfold.split(inputs, targets):\n",
        "\n",
        "    # Define the model architecture\n",
        "    embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
        "                                          100,\n",
        "                                          weights=[embedding_matrix],\n",
        "                                          input_length=60,\n",
        "                                          trainable=False)\n",
        "    sequence_input = Input(shape=(60,), dtype='int32')\n",
        "    embedding_sequences = embedding_layer(sequence_input)\n",
        "    x = SpatialDropout1D(0.2)(embedding_sequences)\n",
        "    x = Bidirectional(GRU(units=128, dropout=0.25, recurrent_dropout=0.25))(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dropout(0.25)(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "    model = tf.keras.Model(sequence_input, outputs)\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  # model.summary()\n",
        "\n",
        "  # Generate a print\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  # Fit data to model\n",
        "    history = model.fit(inputs[train], targets[train],\n",
        "              batch_size=batch_size,\n",
        "              epochs=no_epochs,\n",
        "              verbose=verbosity)\n",
        "#     model_path = \"..content/w2v_models/\" + str(fold_no) + \"model\"\n",
        "    \n",
        "#     convert the history.history dict to a pandas DataFrame:     \n",
        "    hist_df = pd.DataFrame(history.history) \n",
        "\n",
        "#     save to json:  \n",
        "#     hist_json_file = \"..content/w2v_history\" + str(fold_no) + 'history.json' \n",
        "#     with open(hist_json_file, mode='w') as f:\n",
        "#         hist_df.to_json(f)\n",
        "            \n",
        "#     save model\n",
        "#     model.save(model_path)\n",
        "    \n",
        "  # Generate generalization metrics\n",
        "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
        "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "  # Increase fold number\n",
        "    fold_no = fold_no + 1\n",
        "\n",
        "  # == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM W2V"
      ],
      "metadata": {
        "id": "JlJZYnQD7luI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0Z3sILAP6WFj",
        "outputId": "ac0866c5-c255-466c-cb86-05e579dcce9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/5\n",
            "375/375 [==============================] - 71s 172ms/step - loss: 0.5104 - accuracy: 0.7573\n",
            "Epoch 2/5\n",
            "375/375 [==============================] - 67s 178ms/step - loss: 0.4363 - accuracy: 0.8090\n",
            "Epoch 3/5\n",
            "375/375 [==============================] - 68s 180ms/step - loss: 0.4113 - accuracy: 0.8203\n",
            "Epoch 4/5\n",
            "375/375 [==============================] - 66s 177ms/step - loss: 0.3941 - accuracy: 0.8248\n",
            "Epoch 5/5\n",
            "375/375 [==============================] - 67s 178ms/step - loss: 0.3793 - accuracy: 0.8331\n",
            "Score for fold 1: loss of 0.35536471009254456; accuracy of 84.21666622161865%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/5\n",
            "375/375 [==============================] - 70s 178ms/step - loss: 0.6197 - accuracy: 0.6793\n",
            "Epoch 2/5\n",
            "375/375 [==============================] - 69s 184ms/step - loss: 0.6337 - accuracy: 0.6669\n",
            "Epoch 3/5\n",
            "375/375 [==============================] - 68s 181ms/step - loss: 0.5727 - accuracy: 0.6805\n",
            "Epoch 4/5\n",
            "375/375 [==============================] - 67s 178ms/step - loss: 0.6019 - accuracy: 0.6706\n",
            "Epoch 5/5\n",
            "375/375 [==============================] - 68s 181ms/step - loss: 0.5708 - accuracy: 0.6885\n",
            "Score for fold 2: loss of 0.5168890953063965; accuracy of 72.2166657447815%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/5\n",
            "375/375 [==============================] - 72s 182ms/step - loss: 0.5773 - accuracy: 0.6913\n",
            "Epoch 2/5\n",
            "375/375 [==============================] - 67s 180ms/step - loss: 0.6081 - accuracy: 0.6869\n",
            "Epoch 3/5\n",
            "375/375 [==============================] - 68s 182ms/step - loss: 0.5764 - accuracy: 0.7013\n",
            "Epoch 4/5\n",
            "375/375 [==============================] - 67s 180ms/step - loss: 0.4816 - accuracy: 0.7762\n",
            "Epoch 5/5\n",
            "375/375 [==============================] - 67s 179ms/step - loss: 0.4311 - accuracy: 0.8067\n",
            "Score for fold 3: loss of 0.3861267864704132; accuracy of 83.35000276565552%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/5\n",
            "375/375 [==============================] - 71s 176ms/step - loss: 0.6192 - accuracy: 0.6877\n",
            "Epoch 2/5\n",
            "375/375 [==============================] - 68s 181ms/step - loss: 0.5548 - accuracy: 0.6967\n",
            "Epoch 3/5\n",
            "375/375 [==============================] - 67s 178ms/step - loss: 0.6154 - accuracy: 0.6696\n",
            "Epoch 4/5\n",
            "375/375 [==============================] - 66s 176ms/step - loss: 0.6143 - accuracy: 0.6668\n",
            "Epoch 5/5\n",
            "375/375 [==============================] - 70s 187ms/step - loss: 0.5718 - accuracy: 0.6846\n",
            "Score for fold 4: loss of 0.46687328815460205; accuracy of 80.11666536331177%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/5\n",
            "375/375 [==============================] - 75s 182ms/step - loss: 0.6221 - accuracy: 0.6685\n",
            "Epoch 2/5\n",
            "375/375 [==============================] - 68s 180ms/step - loss: 0.6057 - accuracy: 0.6762\n",
            "Epoch 3/5\n",
            "375/375 [==============================] - 68s 183ms/step - loss: 0.4722 - accuracy: 0.7870\n",
            "Epoch 4/5\n",
            "375/375 [==============================] - 68s 180ms/step - loss: 0.4333 - accuracy: 0.8085\n",
            "Epoch 5/5\n",
            "375/375 [==============================] - 67s 177ms/step - loss: 0.4069 - accuracy: 0.8248\n",
            "Score for fold 5: loss of 0.386237233877182; accuracy of 83.24999809265137%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.311845064163208 - Accuracy: 88.18333148956299%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.3438962399959564 - Accuracy: 87.99999952316284%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.3144559860229492 - Accuracy: 88.06666731834412%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.32712557911872864 - Accuracy: 87.4666690826416%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.33121421933174133 - Accuracy: 87.56666779518127%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 6 - Loss: 0.3089353144168854 - Accuracy: 87.56666779518127%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 7 - Loss: 0.3088591694831848 - Accuracy: 87.73333430290222%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 8 - Loss: 0.31690821051597595 - Accuracy: 86.2833321094513%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 9 - Loss: 0.3066258430480957 - Accuracy: 87.65000104904175%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 10 - Loss: 0.32030919194221497 - Accuracy: 87.05000281333923%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 11 - Loss: 0.3099775016307831 - Accuracy: 87.65000104904175%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 12 - Loss: 0.3076343536376953 - Accuracy: 87.03333139419556%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 13 - Loss: 0.33590734004974365 - Accuracy: 85.72726249694824%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 14 - Loss: 0.3103717267513275 - Accuracy: 87.34638690948486%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 15 - Loss: 0.3111148178577423 - Accuracy: 87.43554949760437%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 16 - Loss: 0.3178783059120178 - Accuracy: 86.50416135787964%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 17 - Loss: 0.31216880679130554 - Accuracy: 87.14332580566406%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 18 - Loss: 0.3370093107223511 - Accuracy: 85.92779040336609%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 19 - Loss: 0.33912110328674316 - Accuracy: 85.6732964515686%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 20 - Loss: 0.34115079045295715 - Accuracy: 85.52971482276917%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 21 - Loss: 0.35374337434768677 - Accuracy: 84.46693420410156%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 22 - Loss: 0.34680038690567017 - Accuracy: 85.87194085121155%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 23 - Loss: 0.36566096544265747 - Accuracy: 85.25000214576721%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 24 - Loss: 0.3272828459739685 - Accuracy: 86.33333444595337%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 25 - Loss: 0.328748494386673 - Accuracy: 86.15000247955322%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 26 - Loss: 0.3546135127544403 - Accuracy: 85.14999747276306%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 27 - Loss: 0.33963847160339355 - Accuracy: 85.21666526794434%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 28 - Loss: 0.35536471009254456 - Accuracy: 84.21666622161865%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 29 - Loss: 0.5168890953063965 - Accuracy: 72.2166657447815%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 30 - Loss: 0.3861267864704132 - Accuracy: 83.35000276565552%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 31 - Loss: 0.46687328815460205 - Accuracy: 80.11666536331177%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 32 - Loss: 0.386237233877182 - Accuracy: 83.24999809265137%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 85.72269901633263 (+- 2.962990280461816)\n",
            "> Loss: 0.3418902512639761\n",
            "------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Model configuration\n",
        "from tensorflow.keras.layers import Dense , Flatten ,Embedding,Input,LSTM, Bidirectional, Dense, Dropout,GRU,SpatialDropout1D, Conv1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.layers import SpatialDropout1D\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "\n",
        "batch_size = 64\n",
        "no_classes = 100\n",
        "no_epochs = 5\n",
        "optimizer = Adam()\n",
        "verbosity = 1\n",
        "num_folds = 5\n",
        "\n",
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "for train, test in kfold.split(inputs, targets):\n",
        "\n",
        "    # Define the model architecture\n",
        "    embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
        "                                          100,\n",
        "                                          weights=[embedding_matrix],\n",
        "                                          input_length=60,\n",
        "                                          trainable=False)\n",
        "    sequence_input = Input(shape=(60,), dtype='int32')\n",
        "    embedding_sequences = embedding_layer(sequence_input)\n",
        "    x = SpatialDropout1D(0.2)(embedding_sequences)\n",
        "    x = LSTM(units=128, dropout=0.25, recurrent_dropout=0.25)(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dropout(0.25)(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "    model = tf.keras.Model(sequence_input, outputs)\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  # model.summary()\n",
        "\n",
        "  # Generate a print\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  # Fit data to model\n",
        "    history = model.fit(inputs[train], targets[train],\n",
        "              batch_size=batch_size,\n",
        "              epochs=no_epochs,\n",
        "              verbose=verbosity)\n",
        "#     model_path = \"..content/w2v_models/\" + str(fold_no) + \"model\"\n",
        "    \n",
        "#     convert the history.history dict to a pandas DataFrame:     \n",
        "    hist_df = pd.DataFrame(history.history) \n",
        "\n",
        "#     save to json:  \n",
        "#     hist_json_file = \"..content/w2v_history\" + str(fold_no) + 'history.json' \n",
        "#     with open(hist_json_file, mode='w') as f:\n",
        "#         hist_df.to_json(f)\n",
        "            \n",
        "#     save model\n",
        "#     model.save(model_path)\n",
        "    \n",
        "  # Generate generalization metrics\n",
        "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
        "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "  # Increase fold number\n",
        "    fold_no = fold_no + 1\n",
        "\n",
        "  # == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GLOVE AND 5-FOLD CROSS VALIDATION"
      ],
      "metadata": {
        "id": "J801OMmI7pkc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrQmbpWe6WFl",
        "outputId": "612e21e1-b569-445c-a1d9-0c6012c0dbef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of vocabulary :  47752\n",
            "All sentances with same length  (30000, 60)\n"
          ]
        }
      ],
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "\n",
        "# tokenize every words so that evey words maps to numaric value\n",
        "tok = Tokenizer()\n",
        "tok.fit_on_texts(data['cleaned_text'].values.astype('U'))\n",
        "encd_rev = tok.texts_to_sequences(data['cleaned_text'].values.astype('U'))\n",
        "# tok.word_index\n",
        "\n",
        "# make all the input sentance same length with add padding\n",
        "max_rev_len = 60 # max lenght of a sentance\n",
        "vocab_size = len(tok.word_index) + 1  # total no of words\n",
        "embed_dim = 100 # embedding dimension \n",
        "pad_rev= pad_sequences(encd_rev, maxlen=60, padding='post')\n",
        "\n",
        "print(\"Size of vocabulary : \", vocab_size)\n",
        "print(\"All sentances with same length \", pad_rev.shape)\n",
        "\n",
        "# lebel encode the output label to categorical\n",
        "Y = data['hate']\n",
        "# encode class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "encoded_Y = encoder.transform(Y)\n",
        "\n",
        "\n",
        "# test train split\n",
        "X_train,X_test,y_train,y_test=train_test_split(pad_rev,encoded_Y,test_size=0.2, random_state = 0)\n",
        "\n",
        "# make a dictionary. word as key and feature vector as value\n",
        "embedding_index={}\n",
        "f = open('../content/bn_glove.39M.100d.txt',encoding='utf-8')\n",
        "for line in f:\n",
        "    values=line.split()\n",
        "    word=values[0]\n",
        "    coefs=np.asarray(values[1:])\n",
        "    embedding_index[word]=coefs\n",
        "f.close()\n",
        "\n",
        "# create a embeddings matrix with 200 dimenstion\n",
        "EMBEDDING_DIM=100\n",
        "embedding_matrix=np.zeros((vocab_size,EMBEDDING_DIM))\n",
        "for word, i in tok.word_index.items():\n",
        "    if i>vocab_size:\n",
        "        continue\n",
        "    embedding_vector=embedding_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i]=embedding_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HATE DETECT GLOVE"
      ],
      "metadata": {
        "id": "9Zi4ynEr7uW8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "zcI5eyoj6WFm",
        "outputId": "4fc0231f-5463-464c-85b5-a67bfb9a7bd9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-04-17 14:27:34.568859: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
            "2022-04-17 14:27:34.568936: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
            "2022-04-17 14:27:34.568996: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (mostafa): /proc/driver/nvidia/version does not exist\n",
            "2022-04-17 14:27:34.594014: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "375/375 [==============================] - 397s 1s/step - loss: 0.5390 - accuracy: 0.7303\n",
            "Epoch 2/10\n",
            "375/375 [==============================] - 389s 1s/step - loss: 0.4774 - accuracy: 0.7737\n",
            "Epoch 3/10\n",
            "375/375 [==============================] - 292s 779ms/step - loss: 0.4516 - accuracy: 0.7876\n",
            "Epoch 4/10\n",
            "375/375 [==============================] - 284s 756ms/step - loss: 0.4353 - accuracy: 0.7989\n",
            "Epoch 5/10\n",
            "375/375 [==============================] - 285s 759ms/step - loss: 0.4191 - accuracy: 0.8048\n",
            "Epoch 6/10\n",
            "375/375 [==============================] - 284s 757ms/step - loss: 0.4051 - accuracy: 0.8144\n",
            "Epoch 7/10\n",
            "375/375 [==============================] - 284s 758ms/step - loss: 0.3944 - accuracy: 0.8177\n",
            "Epoch 8/10\n",
            "375/375 [==============================] - 285s 760ms/step - loss: 0.3866 - accuracy: 0.8239\n",
            "Epoch 9/10\n",
            "375/375 [==============================] - 282s 751ms/step - loss: 0.3730 - accuracy: 0.8316\n",
            "Epoch 10/10\n",
            "375/375 [==============================] - 282s 752ms/step - loss: 0.3682 - accuracy: 0.8339\n",
            "Score for fold 1: loss of 0.4174869656562805; accuracy of 81.19999766349792%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "375/375 [==============================] - 295s 761ms/step - loss: 0.5180 - accuracy: 0.7470\n",
            "Epoch 2/10\n",
            "375/375 [==============================] - 282s 752ms/step - loss: 0.4532 - accuracy: 0.7878\n",
            "Epoch 3/10\n",
            "375/375 [==============================] - 282s 751ms/step - loss: 0.4285 - accuracy: 0.8006\n",
            "Epoch 4/10\n",
            "375/375 [==============================] - 297s 793ms/step - loss: 0.4083 - accuracy: 0.8123\n",
            "Epoch 5/10\n",
            "375/375 [==============================] - 321s 856ms/step - loss: 0.3974 - accuracy: 0.8171\n",
            "Epoch 6/10\n",
            "375/375 [==============================] - 322s 859ms/step - loss: 0.3841 - accuracy: 0.8268\n",
            "Epoch 7/10\n",
            "375/375 [==============================] - 321s 855ms/step - loss: 0.3723 - accuracy: 0.8310\n",
            "Epoch 8/10\n",
            "375/375 [==============================] - 319s 851ms/step - loss: 0.3636 - accuracy: 0.8357\n",
            "Epoch 9/10\n",
            "375/375 [==============================] - 319s 849ms/step - loss: 0.3551 - accuracy: 0.8410\n",
            "Epoch 10/10\n",
            "375/375 [==============================] - 324s 864ms/step - loss: 0.3507 - accuracy: 0.8409\n",
            "Score for fold 2: loss of 0.39772823452949524; accuracy of 82.28333592414856%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "375/375 [==============================] - 497s 1s/step - loss: 0.5109 - accuracy: 0.7518\n",
            "Epoch 2/10\n",
            "375/375 [==============================] - 357s 953ms/step - loss: 0.4520 - accuracy: 0.7881\n",
            "Epoch 3/10\n",
            "375/375 [==============================] - 392s 1s/step - loss: 0.4241 - accuracy: 0.8052\n",
            "Epoch 4/10\n",
            "375/375 [==============================] - 357s 951ms/step - loss: 0.4075 - accuracy: 0.8130\n",
            "Epoch 5/10\n",
            "375/375 [==============================] - 287s 766ms/step - loss: 0.3959 - accuracy: 0.8195\n",
            "Epoch 6/10\n",
            "375/375 [==============================] - 291s 775ms/step - loss: 0.3826 - accuracy: 0.8259\n",
            "Epoch 7/10\n",
            "375/375 [==============================] - 289s 771ms/step - loss: 0.3700 - accuracy: 0.8299\n",
            "Epoch 8/10\n",
            "375/375 [==============================] - 288s 767ms/step - loss: 0.3603 - accuracy: 0.8363\n",
            "Epoch 9/10\n",
            "375/375 [==============================] - 292s 778ms/step - loss: 0.3503 - accuracy: 0.8407\n",
            "Epoch 10/10\n",
            "375/375 [==============================] - 290s 772ms/step - loss: 0.3434 - accuracy: 0.8468\n",
            "Score for fold 3: loss of 0.4215695261955261; accuracy of 81.68333172798157%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/10\n",
            "375/375 [==============================] - 495s 769ms/step - loss: 0.5153 - accuracy: 0.7489\n",
            "Epoch 2/10\n",
            "375/375 [==============================] - 289s 771ms/step - loss: 0.4553 - accuracy: 0.7875\n",
            "Epoch 3/10\n",
            "375/375 [==============================] - 290s 772ms/step - loss: 0.4297 - accuracy: 0.8000\n",
            "Epoch 4/10\n",
            "375/375 [==============================] - 286s 764ms/step - loss: 0.4128 - accuracy: 0.8093\n",
            "Epoch 5/10\n",
            "375/375 [==============================] - 324s 864ms/step - loss: 0.3982 - accuracy: 0.8177\n",
            "Epoch 6/10\n",
            "375/375 [==============================] - 390s 1s/step - loss: 0.3876 - accuracy: 0.8238\n",
            "Epoch 7/10\n",
            "375/375 [==============================] - 336s 897ms/step - loss: 0.3756 - accuracy: 0.8288\n",
            "Epoch 8/10\n",
            "375/375 [==============================] - 294s 784ms/step - loss: 0.3684 - accuracy: 0.8340\n",
            "Epoch 9/10\n",
            "375/375 [==============================] - 294s 783ms/step - loss: 0.3601 - accuracy: 0.8374\n",
            "Epoch 10/10\n",
            "375/375 [==============================] - 302s 805ms/step - loss: 0.3538 - accuracy: 0.8416\n",
            "Score for fold 4: loss of 0.40456706285476685; accuracy of 81.65000081062317%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/10\n",
            "375/375 [==============================] - 351s 896ms/step - loss: 0.5210 - accuracy: 0.7461\n",
            "Epoch 2/10\n",
            "375/375 [==============================] - 335s 894ms/step - loss: 0.4588 - accuracy: 0.7845\n",
            "Epoch 3/10\n",
            "375/375 [==============================] - 338s 901ms/step - loss: 0.4346 - accuracy: 0.7954\n",
            "Epoch 4/10\n",
            "375/375 [==============================] - 338s 901ms/step - loss: 0.4173 - accuracy: 0.8057\n",
            "Epoch 5/10\n",
            "375/375 [==============================] - 342s 913ms/step - loss: 0.4106 - accuracy: 0.8099\n",
            "Epoch 6/10\n",
            "375/375 [==============================] - 335s 892ms/step - loss: 0.3969 - accuracy: 0.8183\n",
            "Epoch 7/10\n",
            "375/375 [==============================] - 336s 896ms/step - loss: 0.3849 - accuracy: 0.8241\n",
            "Epoch 8/10\n",
            "375/375 [==============================] - 337s 898ms/step - loss: 0.3798 - accuracy: 0.8273\n",
            "Epoch 9/10\n",
            "375/375 [==============================] - 345s 920ms/step - loss: 0.3702 - accuracy: 0.8312\n",
            "Epoch 10/10\n",
            "375/375 [==============================] - 422s 1s/step - loss: 0.3589 - accuracy: 0.8373\n"
          ]
        }
      ],
      "source": [
        "# Model configuration\n",
        "from tensorflow.keras.layers import Dense , Flatten ,Embedding,Input,LSTM, Bidirectional, Dense, Dropout,GRU,SpatialDropout1D, Conv1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.layers import SpatialDropout1D\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "\n",
        "batch_size = 64\n",
        "no_classes = 100\n",
        "no_epochs = 10\n",
        "optimizer = Adam()\n",
        "verbosity = 1\n",
        "num_folds = 5\n",
        "\n",
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "for train, test in kfold.split(inputs, targets):\n",
        "\n",
        "    # Define the model architecture\n",
        "    embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
        "                                          100,\n",
        "                                          weights=[embedding_matrix],\n",
        "                                          input_length=60,\n",
        "                                          trainable=False)\n",
        "    sequence_input = Input(shape=(60,), dtype='int32')\n",
        "    embedding_sequences = embedding_layer(sequence_input)\n",
        "    x = SpatialDropout1D(0.2)(embedding_sequences)\n",
        "    x = Bidirectional(GRU(units=128, return_sequences = True, dropout=0.25, recurrent_dropout=0.25))(x)\n",
        "    x = Bidirectional(GRU(units=128, return_sequences = True, dropout=0.25, recurrent_dropout=0.25))(x)\n",
        "    x = attention()(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dropout(0.25)(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "    model = tf.keras.Model(sequence_input, outputs)\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  # model.summary()\n",
        "\n",
        "  # Generate a print\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  # Fit data to model\n",
        "    history = model.fit(inputs[train], targets[train],\n",
        "              batch_size=batch_size,\n",
        "              epochs=no_epochs,\n",
        "              verbose=verbosity)\n",
        "    # model_path = \"content_english/TwitterHateModels_word2vec/\" + str(fold_no) + \"model\"\n",
        "    \n",
        "    # convert the history.history dict to a pandas DataFrame:     \n",
        "    # hist_df = pd.DataFrame(history.history) \n",
        "\n",
        "    # save to json:  \n",
        "    # hist_json_file = \"content_english/TwitterHateHistory_word2vec/\" + str(fold_no) + 'history.json' \n",
        "    # with open(hist_json_file, mode='w') as f:\n",
        "    #     hist_df.to_json(f)\n",
        "            \n",
        "    #save model\n",
        "    # model.save(model_path)\n",
        "    \n",
        "  # Generate generalization metrics\n",
        "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
        "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "  # Increase fold number\n",
        "    fold_no = fold_no + 1\n",
        "\n",
        "  # == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BI-GRU GLOVE"
      ],
      "metadata": {
        "id": "2ihlgL3M7yIF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "SW25Tubh6WFp",
        "outputId": "3d4822c2-56bd-4dce-9738-ea88895e1612"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "375/375 [==============================] - 115s 284ms/step - loss: 0.5299 - accuracy: 0.7329\n",
            "Epoch 2/10\n",
            "375/375 [==============================] - 105s 279ms/step - loss: 0.4838 - accuracy: 0.7668\n",
            "Epoch 3/10\n",
            "375/375 [==============================] - 107s 284ms/step - loss: 0.4622 - accuracy: 0.7823\n",
            "Epoch 4/10\n",
            "375/375 [==============================] - 106s 282ms/step - loss: 0.4503 - accuracy: 0.7879\n",
            "Epoch 5/10\n",
            "375/375 [==============================] - 106s 282ms/step - loss: 0.4319 - accuracy: 0.7963\n",
            "Epoch 6/10\n",
            "375/375 [==============================] - 107s 284ms/step - loss: 0.4232 - accuracy: 0.8044\n",
            "Epoch 7/10\n",
            "375/375 [==============================] - 107s 285ms/step - loss: 0.4115 - accuracy: 0.8125\n",
            "Epoch 8/10\n",
            "375/375 [==============================] - 107s 285ms/step - loss: 0.4049 - accuracy: 0.8140\n",
            "Epoch 9/10\n",
            "375/375 [==============================] - 109s 291ms/step - loss: 0.3950 - accuracy: 0.8186\n",
            "Epoch 10/10\n",
            "375/375 [==============================] - 107s 285ms/step - loss: 0.3895 - accuracy: 0.8206\n",
            "Score for fold 1: loss of 0.4115995764732361; accuracy of 81.76666498184204%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "375/375 [==============================] - 117s 288ms/step - loss: 0.5073 - accuracy: 0.7530\n",
            "Epoch 2/10\n",
            "375/375 [==============================] - 109s 290ms/step - loss: 0.4569 - accuracy: 0.7867\n",
            "Epoch 3/10\n",
            "375/375 [==============================] - 109s 290ms/step - loss: 0.4356 - accuracy: 0.7966\n",
            "Epoch 4/10\n",
            "375/375 [==============================] - 109s 290ms/step - loss: 0.4204 - accuracy: 0.8069\n",
            "Epoch 5/10\n",
            "375/375 [==============================] - 109s 290ms/step - loss: 0.4064 - accuracy: 0.8130\n",
            "Epoch 6/10\n",
            "375/375 [==============================] - 109s 290ms/step - loss: 0.3942 - accuracy: 0.8186\n",
            "Epoch 7/10\n",
            "375/375 [==============================] - 109s 290ms/step - loss: 0.3822 - accuracy: 0.8255\n",
            "Epoch 8/10\n",
            "375/375 [==============================] - 109s 290ms/step - loss: 0.3754 - accuracy: 0.8300\n",
            "Epoch 9/10\n",
            "375/375 [==============================] - 109s 290ms/step - loss: 0.3660 - accuracy: 0.8338\n",
            "Epoch 10/10\n",
            "375/375 [==============================] - 108s 287ms/step - loss: 0.3587 - accuracy: 0.8373\n",
            "Score for fold 2: loss of 0.43474316596984863; accuracy of 80.31666874885559%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "375/375 [==============================] - 119s 285ms/step - loss: 0.5164 - accuracy: 0.7454\n",
            "Epoch 2/10\n",
            "375/375 [==============================] - 106s 282ms/step - loss: 0.4571 - accuracy: 0.7860\n",
            "Epoch 3/10\n",
            "375/375 [==============================] - 106s 284ms/step - loss: 0.4357 - accuracy: 0.7975\n",
            "Epoch 4/10\n",
            "375/375 [==============================] - 108s 289ms/step - loss: 0.4166 - accuracy: 0.8099\n",
            "Epoch 5/10\n",
            "375/375 [==============================] - 108s 287ms/step - loss: 0.4053 - accuracy: 0.8139\n",
            "Epoch 6/10\n",
            "375/375 [==============================] - 109s 290ms/step - loss: 0.3965 - accuracy: 0.8157\n",
            "Epoch 7/10\n",
            "375/375 [==============================] - 107s 285ms/step - loss: 0.3865 - accuracy: 0.8238\n",
            "Epoch 8/10\n",
            "375/375 [==============================] - 107s 285ms/step - loss: 0.3713 - accuracy: 0.8298\n",
            "Epoch 9/10\n",
            "375/375 [==============================] - 108s 287ms/step - loss: 0.3676 - accuracy: 0.8335\n",
            "Epoch 10/10\n",
            "375/375 [==============================] - 107s 284ms/step - loss: 0.3608 - accuracy: 0.8349\n",
            "Score for fold 3: loss of 0.40986374020576477; accuracy of 81.93333148956299%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/10\n",
            "375/375 [==============================] - 127s 287ms/step - loss: 0.5106 - accuracy: 0.7488\n",
            "Epoch 2/10\n",
            "375/375 [==============================] - 107s 285ms/step - loss: 0.4577 - accuracy: 0.7851\n",
            "Epoch 3/10\n",
            "375/375 [==============================] - 108s 288ms/step - loss: 0.4334 - accuracy: 0.7970\n",
            "Epoch 4/10\n",
            "375/375 [==============================] - 105s 279ms/step - loss: 0.4168 - accuracy: 0.8061\n",
            "Epoch 5/10\n",
            "375/375 [==============================] - 109s 290ms/step - loss: 0.4076 - accuracy: 0.8116\n",
            "Epoch 6/10\n",
            "375/375 [==============================] - 107s 287ms/step - loss: 0.3926 - accuracy: 0.8187\n",
            "Epoch 7/10\n",
            "375/375 [==============================] - 108s 287ms/step - loss: 0.3854 - accuracy: 0.8232\n",
            "Epoch 8/10\n",
            "375/375 [==============================] - 108s 288ms/step - loss: 0.3745 - accuracy: 0.8275\n",
            "Epoch 9/10\n",
            "375/375 [==============================] - 108s 288ms/step - loss: 0.3651 - accuracy: 0.8328\n",
            "Epoch 10/10\n",
            "375/375 [==============================] - 108s 287ms/step - loss: 0.3616 - accuracy: 0.8340\n",
            "Score for fold 4: loss of 0.44043344259262085; accuracy of 79.75000143051147%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/10\n",
            "375/375 [==============================] - 115s 286ms/step - loss: 0.5058 - accuracy: 0.7519\n",
            "Epoch 2/10\n",
            "375/375 [==============================] - 108s 287ms/step - loss: 0.4591 - accuracy: 0.7830\n",
            "Epoch 3/10\n",
            "375/375 [==============================] - 107s 286ms/step - loss: 0.4355 - accuracy: 0.7968\n",
            "Epoch 4/10\n",
            "375/375 [==============================] - 108s 287ms/step - loss: 0.4200 - accuracy: 0.8038\n",
            "Epoch 5/10\n",
            "375/375 [==============================] - 108s 287ms/step - loss: 0.4076 - accuracy: 0.8098\n",
            "Epoch 6/10\n",
            "375/375 [==============================] - 107s 284ms/step - loss: 0.3947 - accuracy: 0.8160\n",
            "Epoch 7/10\n",
            "375/375 [==============================] - 108s 287ms/step - loss: 0.3857 - accuracy: 0.8227\n",
            "Epoch 8/10\n",
            "375/375 [==============================] - 106s 283ms/step - loss: 0.3789 - accuracy: 0.8259\n",
            "Epoch 9/10\n",
            "375/375 [==============================] - 107s 284ms/step - loss: 0.3701 - accuracy: 0.8310\n",
            "Epoch 10/10\n",
            "375/375 [==============================] - 108s 287ms/step - loss: 0.3621 - accuracy: 0.8339\n",
            "Score for fold 5: loss of 0.4260956645011902; accuracy of 81.25%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.311845064163208 - Accuracy: 88.18333148956299%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.3438962399959564 - Accuracy: 87.99999952316284%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.3144559860229492 - Accuracy: 88.06666731834412%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.32712557911872864 - Accuracy: 87.4666690826416%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.33121421933174133 - Accuracy: 87.56666779518127%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 6 - Loss: 0.3089353144168854 - Accuracy: 87.56666779518127%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 7 - Loss: 0.3088591694831848 - Accuracy: 87.73333430290222%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 8 - Loss: 0.31690821051597595 - Accuracy: 86.2833321094513%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 9 - Loss: 0.3066258430480957 - Accuracy: 87.65000104904175%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 10 - Loss: 0.32030919194221497 - Accuracy: 87.05000281333923%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 11 - Loss: 0.3099775016307831 - Accuracy: 87.65000104904175%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 12 - Loss: 0.3076343536376953 - Accuracy: 87.03333139419556%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 13 - Loss: 0.33590734004974365 - Accuracy: 85.72726249694824%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 14 - Loss: 0.3103717267513275 - Accuracy: 87.34638690948486%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 15 - Loss: 0.3111148178577423 - Accuracy: 87.43554949760437%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 16 - Loss: 0.3178783059120178 - Accuracy: 86.50416135787964%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 17 - Loss: 0.31216880679130554 - Accuracy: 87.14332580566406%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 18 - Loss: 0.3370093107223511 - Accuracy: 85.92779040336609%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 19 - Loss: 0.33912110328674316 - Accuracy: 85.6732964515686%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 20 - Loss: 0.34115079045295715 - Accuracy: 85.52971482276917%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 21 - Loss: 0.35374337434768677 - Accuracy: 84.46693420410156%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 22 - Loss: 0.34680038690567017 - Accuracy: 85.87194085121155%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 23 - Loss: 0.36566096544265747 - Accuracy: 85.25000214576721%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 24 - Loss: 0.3272828459739685 - Accuracy: 86.33333444595337%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 25 - Loss: 0.328748494386673 - Accuracy: 86.15000247955322%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 26 - Loss: 0.3546135127544403 - Accuracy: 85.14999747276306%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 27 - Loss: 0.33963847160339355 - Accuracy: 85.21666526794434%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 28 - Loss: 0.35536471009254456 - Accuracy: 84.21666622161865%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 29 - Loss: 0.5168890953063965 - Accuracy: 72.2166657447815%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 30 - Loss: 0.3861267864704132 - Accuracy: 83.35000276565552%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 31 - Loss: 0.46687328815460205 - Accuracy: 80.11666536331177%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 32 - Loss: 0.386237233877182 - Accuracy: 83.24999809265137%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 33 - Loss: 0.4115995764732361 - Accuracy: 81.76666498184204%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 34 - Loss: 0.43474316596984863 - Accuracy: 80.31666874885559%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 35 - Loss: 0.40986374020576477 - Accuracy: 81.93333148956299%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 36 - Loss: 0.44043344259262085 - Accuracy: 79.75000143051147%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 37 - Loss: 0.4260956645011902 - Accuracy: 81.25%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 85.08494689657881 (+- 3.208105718950085)\n",
            "> Loss: 0.3530600981132404\n",
            "------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Model configuration\n",
        "from tensorflow.keras.layers import Dense , Flatten ,Embedding,Input,LSTM, Bidirectional, Dense, Dropout,GRU,SpatialDropout1D, Conv1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.layers import SpatialDropout1D\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "\n",
        "batch_size = 64\n",
        "no_classes = 100\n",
        "no_epochs = 10\n",
        "optimizer = Adam()\n",
        "verbosity = 1\n",
        "num_folds = 5\n",
        "\n",
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "for train, test in kfold.split(inputs, targets):\n",
        "\n",
        "    # Define the model architecture\n",
        "    embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
        "                                          100,\n",
        "                                          weights=[embedding_matrix],\n",
        "                                          input_length=60,\n",
        "                                          trainable=False)\n",
        "    sequence_input = Input(shape=(60,), dtype='int32')\n",
        "    embedding_sequences = embedding_layer(sequence_input)\n",
        "    x = SpatialDropout1D(0.2)(embedding_sequences)\n",
        "    x = Bidirectional(GRU(units=128, dropout=0.25, recurrent_dropout=0.25))(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dropout(0.25)(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "    model = tf.keras.Model(sequence_input, outputs)\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  # model.summary()\n",
        "\n",
        "  # Generate a print\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  # Fit data to model\n",
        "    history = model.fit(inputs[train], targets[train],\n",
        "              batch_size=batch_size,\n",
        "              epochs=no_epochs,\n",
        "              verbose=verbosity)\n",
        "    # model_path = \"content_english/TwitterHateModels_word2vec/\" + str(fold_no) + \"model\"\n",
        "    \n",
        "    # convert the history.history dict to a pandas DataFrame:     \n",
        "    # hist_df = pd.DataFrame(history.history) \n",
        "\n",
        "    # save to json:  \n",
        "    # hist_json_file = \"content_english/TwitterHateHistory_word2vec/\" + str(fold_no) + 'history.json' \n",
        "    # with open(hist_json_file, mode='w') as f:\n",
        "    #     hist_df.to_json(f)\n",
        "            \n",
        "    #save model\n",
        "    # model.save(model_path)\n",
        "    \n",
        "  # Generate generalization metrics\n",
        "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
        "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "  # Increase fold number\n",
        "    fold_no = fold_no + 1\n",
        "\n",
        "  # == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM GLOVE"
      ],
      "metadata": {
        "id": "-xfN_N1R70-C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "drpf9nks6WFs",
        "outputId": "aac5106d-dd11-4544-8f75-ebbf8ffbc21b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/5\n",
            "375/375 [==============================] - 102s 256ms/step - loss: 0.5716 - accuracy: 0.7113\n",
            "Epoch 2/5\n",
            "375/375 [==============================] - 90s 239ms/step - loss: 0.5289 - accuracy: 0.7412\n",
            "Epoch 3/5\n",
            "375/375 [==============================] - 91s 241ms/step - loss: 0.5139 - accuracy: 0.7513\n",
            "Epoch 4/5\n",
            "375/375 [==============================] - 90s 239ms/step - loss: 0.4953 - accuracy: 0.7638\n",
            "Epoch 5/5\n",
            "375/375 [==============================] - 88s 236ms/step - loss: 0.4830 - accuracy: 0.7711\n",
            "Score for fold 1: loss of 0.46865135431289673; accuracy of 77.47945189476013%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/5\n",
            "375/375 [==============================] - 94s 239ms/step - loss: 0.5713 - accuracy: 0.7091\n",
            "Epoch 2/5\n",
            "375/375 [==============================] - 89s 238ms/step - loss: 0.5197 - accuracy: 0.7469\n",
            "Epoch 3/5\n",
            "375/375 [==============================] - 90s 240ms/step - loss: 0.4977 - accuracy: 0.7634\n",
            "Epoch 4/5\n",
            "375/375 [==============================] - 90s 239ms/step - loss: 0.4778 - accuracy: 0.7754\n",
            "Epoch 5/5\n",
            "375/375 [==============================] - 91s 242ms/step - loss: 0.4640 - accuracy: 0.7830\n",
            "Score for fold 2: loss of 0.4540518820285797; accuracy of 78.9027750492096%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/5\n",
            "375/375 [==============================] - 94s 241ms/step - loss: 0.5607 - accuracy: 0.7197\n",
            "Epoch 2/5\n",
            "375/375 [==============================] - 89s 238ms/step - loss: 0.5147 - accuracy: 0.7504\n",
            "Epoch 3/5\n",
            "375/375 [==============================] - 90s 239ms/step - loss: 0.4875 - accuracy: 0.7669\n",
            "Epoch 4/5\n",
            "375/375 [==============================] - 87s 232ms/step - loss: 0.4695 - accuracy: 0.7801\n",
            "Epoch 5/5\n",
            "375/375 [==============================] - 89s 239ms/step - loss: 0.4511 - accuracy: 0.7901\n",
            "Score for fold 3: loss of 0.44624224305152893; accuracy of 79.19142842292786%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/5\n",
            "375/375 [==============================] - 101s 257ms/step - loss: 0.5618 - accuracy: 0.7165\n",
            "Epoch 2/5\n",
            "375/375 [==============================] - 91s 242ms/step - loss: 0.5114 - accuracy: 0.7521\n",
            "Epoch 3/5\n",
            "375/375 [==============================] - 87s 233ms/step - loss: 0.4895 - accuracy: 0.7663\n",
            "Epoch 4/5\n",
            "375/375 [==============================] - 89s 237ms/step - loss: 0.4759 - accuracy: 0.7727\n",
            "Epoch 5/5\n",
            "375/375 [==============================] - 90s 239ms/step - loss: 0.4583 - accuracy: 0.7868\n",
            "Score for fold 4: loss of 0.4669942259788513; accuracy of 77.35555768013%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/5\n",
            "375/375 [==============================] - 94s 239ms/step - loss: 0.5901 - accuracy: 0.6912\n",
            "Epoch 2/5\n",
            "375/375 [==============================] - 89s 237ms/step - loss: 0.5457 - accuracy: 0.7117\n",
            "Epoch 3/5\n",
            "375/375 [==============================] - 90s 240ms/step - loss: 0.5156 - accuracy: 0.7429\n",
            "Epoch 4/5\n",
            "375/375 [==============================] - 90s 239ms/step - loss: 0.4943 - accuracy: 0.7622\n",
            "Epoch 5/5\n",
            "375/375 [==============================] - 90s 238ms/step - loss: 0.4764 - accuracy: 0.7719\n",
            "Score for fold 5: loss of 0.45519253611564636; accuracy of 78.45555543899536%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.311845064163208 - Accuracy: 88.18333148956299%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.3438962399959564 - Accuracy: 87.99999952316284%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.3144559860229492 - Accuracy: 88.06666731834412%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.32712557911872864 - Accuracy: 87.4666690826416%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.33121421933174133 - Accuracy: 87.56666779518127%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 6 - Loss: 0.3089353144168854 - Accuracy: 87.56666779518127%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 7 - Loss: 0.3088591694831848 - Accuracy: 87.73333430290222%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 8 - Loss: 0.31690821051597595 - Accuracy: 86.2833321094513%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 9 - Loss: 0.3066258430480957 - Accuracy: 87.65000104904175%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 10 - Loss: 0.32030919194221497 - Accuracy: 87.05000281333923%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 11 - Loss: 0.3099775016307831 - Accuracy: 87.65000104904175%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 12 - Loss: 0.3076343536376953 - Accuracy: 87.03333139419556%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 13 - Loss: 0.33590734004974365 - Accuracy: 85.72726249694824%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 14 - Loss: 0.3103717267513275 - Accuracy: 87.34638690948486%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 15 - Loss: 0.3111148178577423 - Accuracy: 87.43554949760437%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 16 - Loss: 0.3178783059120178 - Accuracy: 86.50416135787964%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 17 - Loss: 0.31216880679130554 - Accuracy: 87.14332580566406%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 18 - Loss: 0.3370093107223511 - Accuracy: 85.92779040336609%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 19 - Loss: 0.33912110328674316 - Accuracy: 85.6732964515686%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 20 - Loss: 0.34115079045295715 - Accuracy: 85.52971482276917%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 21 - Loss: 0.35374337434768677 - Accuracy: 84.46693420410156%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 22 - Loss: 0.34680038690567017 - Accuracy: 85.87194085121155%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 23 - Loss: 0.36566096544265747 - Accuracy: 85.25000214576721%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 24 - Loss: 0.3272828459739685 - Accuracy: 86.33333444595337%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 25 - Loss: 0.328748494386673 - Accuracy: 86.15000247955322%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 26 - Loss: 0.3546135127544403 - Accuracy: 85.14999747276306%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 27 - Loss: 0.33963847160339355 - Accuracy: 85.21666526794434%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 28 - Loss: 0.35536471009254456 - Accuracy: 84.21666622161865%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 29 - Loss: 0.5168890953063965 - Accuracy: 72.2166657447815%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 30 - Loss: 0.3861267864704132 - Accuracy: 83.35000276565552%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 31 - Loss: 0.46687328815460205 - Accuracy: 80.11666536331177%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 32 - Loss: 0.386237233877182 - Accuracy: 83.24999809265137%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 33 - Loss: 0.4115995764732361 - Accuracy: 81.76666498184204%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 34 - Loss: 0.43474316596984863 - Accuracy: 80.31666874885559%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 35 - Loss: 0.40986374020576477 - Accuracy: 81.93333148956299%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 36 - Loss: 0.44043344259262085 - Accuracy: 79.75000143051147%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 37 - Loss: 0.4260956645011902 - Accuracy: 81.25%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 38 - Loss: 0.46865135431289673 - Accuracy: 77.47945189476013%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 39 - Loss: 0.4540518820285797 - Accuracy: 78.9027750492096%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 40 - Loss: 0.44624224305152893 - Accuracy: 79.19142842292786%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 41 - Loss: 0.4669942259788513 - Accuracy: 77.35555768013%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 42 - Loss: 0.45519253611564636 - Accuracy: 78.45555543899536%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 84.27447151570092 (+- 3.740709006419514)\n",
            "> Loss: 0.3655799017066047\n",
            "------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Model configuration\n",
        "from tensorflow.keras.layers import Dense , Flatten ,Embedding,Input,LSTM, Bidirectional, Dense, Dropout,GRU,SpatialDropout1D, Conv1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.layers import SpatialDropout1D\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "\n",
        "batch_size = 64\n",
        "no_classes = 100\n",
        "no_epochs = 5\n",
        "optimizer = Adam()\n",
        "verbosity = 1\n",
        "num_folds = 5\n",
        "\n",
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "for train, test in kfold.split(inputs, targets):\n",
        "\n",
        "    # Define the model architecture\n",
        "    embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
        "                                          100,\n",
        "                                          weights=[embedding_matrix],\n",
        "                                          input_length=60,\n",
        "                                          trainable=False)\n",
        "    sequence_input = Input(shape=(60,), dtype='int32')\n",
        "    embedding_sequences = embedding_layer(sequence_input)\n",
        "    x = SpatialDropout1D(0.2)(embedding_sequences)\n",
        "    x = LSTM(units=128, return_sequences = True, dropout=0.25, recurrent_dropout=0.25)(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dropout(0.25)(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "    model = tf.keras.Model(sequence_input, outputs)\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  # model.summary()\n",
        "\n",
        "  # Generate a print\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  # Fit data to model\n",
        "    history = model.fit(inputs[train], targets[train],\n",
        "              batch_size=batch_size,\n",
        "              epochs=no_epochs,\n",
        "              verbose=verbosity)\n",
        "    # model_path = \"content_english/TwitterHateModels_word2vec/\" + str(fold_no) + \"model\"\n",
        "    \n",
        "    # convert the history.history dict to a pandas DataFrame:     \n",
        "    # hist_df = pd.DataFrame(history.history) \n",
        "\n",
        "    # save to json:  \n",
        "    # hist_json_file = \"content_english/TwitterHateHistory_word2vec/\" + str(fold_no) + 'history.json' \n",
        "    # with open(hist_json_file, mode='w') as f:\n",
        "    #     hist_df.to_json(f)\n",
        "            \n",
        "    #save model\n",
        "    # model.save(model_path)\n",
        "    \n",
        "  # Generate generalization metrics\n",
        "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
        "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "  # Increase fold number\n",
        "    fold_no = fold_no + 1\n",
        "\n",
        "  # == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FASTTEXT AND 5-FOLD CROSS VALIDATION"
      ],
      "metadata": {
        "id": "WVOYEJ7Q732P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KaMrrGL6WFt"
      },
      "outputs": [],
      "source": [
        "data.to_csv('../content/train_ft.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnbUZsFL6WFu",
        "outputId": "6565b5fe-4402-45cd-eb75-15fdb0e884ff"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Read 0M words\n",
            "Number of words:  13150\n",
            "Number of labels: 0\n",
            "Progress: 100.0% words/sec/thread:   39534 lr:  0.000000 avg.loss:  2.323040 ETA:   0h 0m 0s% words/sec/thread:   39534 lr: -0.000004 avg.loss:  2.323040 ETA:   0h 0m 0s\n"
          ]
        }
      ],
      "source": [
        "import fasttext\n",
        "model = fasttext.train_unsupervised('../content/train_ft.csv', model='cbow')\n",
        "model.save_model(\"fasttext_bn_100.vec\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3gjGldk86WFu",
        "outputId": "c0f838fc-2e0a-4fc5-a0f1-277d293cc5e9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['</s>',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'and',\n",
              " 'TikTok',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'others\",',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '?',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " ',',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " ',',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '<br',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'others\",',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " ',0,\"Meme,',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " ...]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_qs5WNK6WFv",
        "outputId": "e223d7c2-afb9-405f-8e50-e78a366d310c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding Matrix Shape: (47752, 100)\n"
          ]
        }
      ],
      "source": [
        "embedding_matrix = np.zeros((vocab_size, 100))\n",
        "\n",
        "for word, token in tok.word_index.items():\n",
        "    if model.__contains__(word):\n",
        "        embedding_matrix[token] = model.__getitem__(word)\n",
        "\n",
        "print(\"Embedding Matrix Shape:\", embedding_matrix.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "N5a4cCYz6WFw",
        "outputId": "c34f36c5-7a76-49ee-c90d-40b6bf3e4d69"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.01350805,  0.40760002, -1.38908362, ..., -0.52245951,\n",
              "        -0.88665086, -0.63685602],\n",
              "       [-0.22113167,  0.45417839, -0.27309507, ...,  0.05569401,\n",
              "        -0.26027063, -1.6534363 ],\n",
              "       ...,\n",
              "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "         0.        ,  0.        ]])"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedding_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HATE DETECT FT"
      ],
      "metadata": {
        "id": "U5H-iaaQ8AVa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "zUkJ9Ob96WFx",
        "outputId": "ffa02b17-dd5c-41e8-af81-d519c25233df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "375/375 [==============================] - 331s 856ms/step - loss: 0.4031 - accuracy: 0.8219\n",
            "Epoch 2/10\n",
            "375/375 [==============================] - 322s 860ms/step - loss: 0.3502 - accuracy: 0.8485\n",
            "Epoch 3/10\n",
            "375/375 [==============================] - 346s 923ms/step - loss: 0.3357 - accuracy: 0.8606\n",
            "Epoch 4/10\n",
            "375/375 [==============================] - 330s 879ms/step - loss: 0.3242 - accuracy: 0.8658\n",
            "Epoch 5/10\n",
            "375/375 [==============================] - 318s 848ms/step - loss: 0.3128 - accuracy: 0.8690\n",
            "Epoch 6/10\n",
            "375/375 [==============================] - 355s 947ms/step - loss: 0.3054 - accuracy: 0.8731\n",
            "Epoch 7/10\n",
            "375/375 [==============================] - 324s 865ms/step - loss: 0.2982 - accuracy: 0.8771\n",
            "Epoch 8/10\n",
            "375/375 [==============================] - 395s 1s/step - loss: 0.2903 - accuracy: 0.8810\n",
            "Epoch 9/10\n",
            "375/375 [==============================] - 287s 766ms/step - loss: 0.2809 - accuracy: 0.8845\n",
            "Epoch 10/10\n",
            "375/375 [==============================] - 289s 770ms/step - loss: 0.2749 - accuracy: 0.8880\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '..content/fasttext_history/1history.json'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m#     save to json:  \u001b[39;00m\n\u001b[1;32m     59\u001b[0m     hist_json_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..content/fasttext_history/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(fold_no) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhistory.json\u001b[39m\u001b[38;5;124m'\u001b[39m \n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhist_json_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     61\u001b[0m         hist_df\u001b[38;5;241m.\u001b[39mto_json(f)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m#     save model\u001b[39;00m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '..content/fasttext_history/1history.json'"
          ]
        }
      ],
      "source": [
        "# Model configuration\n",
        "from tensorflow.keras.layers import Dense , Flatten ,Embedding,Input,LSTM, Bidirectional, Dense, Dropout,GRU,SpatialDropout1D, Conv1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.layers import SpatialDropout1D\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "\n",
        "batch_size = 64\n",
        "no_classes = 100\n",
        "no_epochs = 10\n",
        "optimizer = Adam()\n",
        "verbosity = 1\n",
        "num_folds = 5\n",
        "\n",
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FS9Y3KPG6WFy",
        "outputId": "a3b5d273-d643-4f49-eb3b-0e249d0867dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "375/375 [==============================] - 648s 2s/step - loss: 0.3900 - accuracy: 0.8332\n",
            "Epoch 2/10\n",
            "375/375 [==============================] - 487s 1s/step - loss: 0.3324 - accuracy: 0.8601\n",
            "Epoch 3/10\n",
            "375/375 [==============================] - 291s 775ms/step - loss: 0.3169 - accuracy: 0.8692\n",
            "Epoch 4/10\n",
            "375/375 [==============================] - 321s 856ms/step - loss: 0.3033 - accuracy: 0.8746\n",
            "Epoch 5/10\n",
            "375/375 [==============================] - 358s 955ms/step - loss: 0.2921 - accuracy: 0.8787\n",
            "Epoch 6/10\n",
            "375/375 [==============================] - 324s 865ms/step - loss: 0.2820 - accuracy: 0.8823\n",
            "Epoch 7/10\n",
            "375/375 [==============================] - 350s 932ms/step - loss: 0.2745 - accuracy: 0.8886\n",
            "Epoch 8/10\n",
            "375/375 [==============================] - 310s 828ms/step - loss: 0.2663 - accuracy: 0.8895\n",
            "Epoch 9/10\n",
            "375/375 [==============================] - 314s 838ms/step - loss: 0.2530 - accuracy: 0.8944\n",
            "Epoch 10/10\n",
            "375/375 [==============================] - 288s 768ms/step - loss: 0.2434 - accuracy: 0.9003\n",
            "INFO:tensorflow:Assets written to: ../content/fasttext_models/2model/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ../content/fasttext_models/2model/assets\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f2384656430> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f23824ecbe0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f235a721250> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f2353ac85e0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 2: loss of 0.3438962399959564; accuracy of 87.99999952316284%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "375/375 [==============================] - 301s 773ms/step - loss: 0.3856 - accuracy: 0.8317\n",
            "Epoch 2/10\n",
            "375/375 [==============================] - 290s 774ms/step - loss: 0.3316 - accuracy: 0.8603\n",
            "Epoch 3/10\n",
            "375/375 [==============================] - 290s 774ms/step - loss: 0.3116 - accuracy: 0.8689\n",
            "Epoch 4/10\n",
            "375/375 [==============================] - 293s 781ms/step - loss: 0.3018 - accuracy: 0.8732\n",
            "Epoch 5/10\n",
            "375/375 [==============================] - 296s 788ms/step - loss: 0.2910 - accuracy: 0.8808\n",
            "Epoch 6/10\n",
            "375/375 [==============================] - 311s 830ms/step - loss: 0.2810 - accuracy: 0.8834\n",
            "Epoch 7/10\n",
            "375/375 [==============================] - 326s 870ms/step - loss: 0.2695 - accuracy: 0.8875\n",
            "Epoch 8/10\n",
            "375/375 [==============================] - 330s 881ms/step - loss: 0.2614 - accuracy: 0.8907\n",
            "Epoch 9/10\n",
            "375/375 [==============================] - 326s 871ms/step - loss: 0.2532 - accuracy: 0.8963\n",
            "Epoch 10/10\n",
            "375/375 [==============================] - 326s 867ms/step - loss: 0.2453 - accuracy: 0.8981\n",
            "INFO:tensorflow:Assets written to: ../content/fasttext_models/3model/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ../content/fasttext_models/3model/assets\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f23826b5430> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f235a6a3730> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f2382556f70> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f2382556580> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 3: loss of 0.3144559860229492; accuracy of 88.06666731834412%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/10\n",
            "375/375 [==============================] - 340s 874ms/step - loss: 0.3832 - accuracy: 0.8331\n",
            "Epoch 2/10\n",
            "375/375 [==============================] - 327s 873ms/step - loss: 0.3284 - accuracy: 0.8631\n",
            "Epoch 3/10\n",
            "375/375 [==============================] - 327s 873ms/step - loss: 0.3105 - accuracy: 0.8729\n",
            "Epoch 4/10\n",
            "375/375 [==============================] - 323s 859ms/step - loss: 0.2994 - accuracy: 0.8791\n",
            "Epoch 5/10\n",
            "375/375 [==============================] - 320s 853ms/step - loss: 0.2872 - accuracy: 0.8820\n",
            "Epoch 6/10\n",
            "375/375 [==============================] - 354s 945ms/step - loss: 0.2779 - accuracy: 0.8857\n",
            "Epoch 7/10\n",
            "375/375 [==============================] - 289s 771ms/step - loss: 0.2690 - accuracy: 0.8892\n",
            "Epoch 8/10\n",
            "375/375 [==============================] - 288s 769ms/step - loss: 0.2598 - accuracy: 0.8923\n",
            "Epoch 9/10\n",
            "375/375 [==============================] - 289s 769ms/step - loss: 0.2503 - accuracy: 0.8970\n",
            "Epoch 10/10\n",
            "375/375 [==============================] - 289s 771ms/step - loss: 0.2439 - accuracy: 0.8997\n",
            "INFO:tensorflow:Assets written to: ../content/fasttext_models/4model/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ../content/fasttext_models/4model/assets\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f2382c27070> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f23824d1970> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f2338d15c70> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f2384848700> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 4: loss of 0.32712557911872864; accuracy of 87.4666690826416%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/10\n",
            "375/375 [==============================] - 306s 788ms/step - loss: 0.3923 - accuracy: 0.8305\n",
            "Epoch 2/10\n",
            "375/375 [==============================] - 320s 853ms/step - loss: 0.3314 - accuracy: 0.8611\n",
            "Epoch 3/10\n",
            "375/375 [==============================] - 322s 859ms/step - loss: 0.3140 - accuracy: 0.8690\n",
            "Epoch 4/10\n",
            "375/375 [==============================] - 322s 859ms/step - loss: 0.3038 - accuracy: 0.8756\n",
            "Epoch 5/10\n",
            "375/375 [==============================] - 323s 862ms/step - loss: 0.2933 - accuracy: 0.8793\n",
            "Epoch 6/10\n",
            "375/375 [==============================] - 325s 868ms/step - loss: 0.2822 - accuracy: 0.8853\n",
            "Epoch 7/10\n",
            "375/375 [==============================] - 331s 882ms/step - loss: 0.2748 - accuracy: 0.8872\n",
            "Epoch 8/10\n",
            "375/375 [==============================] - 322s 860ms/step - loss: 0.2662 - accuracy: 0.8917\n",
            "Epoch 9/10\n",
            "375/375 [==============================] - 322s 859ms/step - loss: 0.2577 - accuracy: 0.8933\n",
            "Epoch 10/10\n",
            "375/375 [==============================] - 323s 863ms/step - loss: 0.2469 - accuracy: 0.9000\n",
            "INFO:tensorflow:Assets written to: ../content/fasttext_models/5model/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ../content/fasttext_models/5model/assets\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f23581d0400> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f2358042460> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f235a64e640> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f2381846a30> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 0.33121421933174133; accuracy of 87.56666779518127%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 6 ...\n",
            "Epoch 1/10\n",
            "375/375 [==============================] - 333s 860ms/step - loss: 0.3873 - accuracy: 0.8347\n",
            "Epoch 2/10\n",
            "375/375 [==============================] - 322s 859ms/step - loss: 0.3357 - accuracy: 0.8593\n",
            "Epoch 3/10\n",
            "375/375 [==============================] - 324s 864ms/step - loss: 0.3168 - accuracy: 0.8691\n",
            "Epoch 4/10\n",
            "375/375 [==============================] - 322s 859ms/step - loss: 0.3031 - accuracy: 0.8753\n",
            "Epoch 5/10\n",
            "375/375 [==============================] - 324s 863ms/step - loss: 0.2930 - accuracy: 0.8786\n",
            "Epoch 6/10\n",
            "375/375 [==============================] - 322s 860ms/step - loss: 0.2833 - accuracy: 0.8839\n",
            "Epoch 7/10\n",
            "375/375 [==============================] - 326s 870ms/step - loss: 0.2754 - accuracy: 0.8864\n",
            "Epoch 8/10\n",
            "375/375 [==============================] - 323s 861ms/step - loss: 0.2674 - accuracy: 0.8898\n",
            "Epoch 9/10\n",
            "375/375 [==============================] - 324s 863ms/step - loss: 0.2563 - accuracy: 0.8940\n",
            "Epoch 10/10\n",
            "375/375 [==============================] - 324s 861ms/step - loss: 0.2503 - accuracy: 0.8966\n",
            "INFO:tensorflow:Assets written to: ../content/fasttext_models/6model/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ../content/fasttext_models/6model/assets\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f238152e9a0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f238296f970> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f235ab02d00> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f235ab35c70> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 6: loss of 0.3089353144168854; accuracy of 87.56666779518127%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.311845064163208 - Accuracy: 88.18333148956299%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.3438962399959564 - Accuracy: 87.99999952316284%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.3144559860229492 - Accuracy: 88.06666731834412%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.32712557911872864 - Accuracy: 87.4666690826416%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.33121421933174133 - Accuracy: 87.56666779518127%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 6 - Loss: 0.3089353144168854 - Accuracy: 87.56666779518127%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 87.80833383401234 (+- 0.28214503308607086)\n",
            "> Loss: 0.3229120671749115\n",
            "------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Model configuration\n",
        "from tensorflow.keras.layers import Dense , Flatten ,Embedding,Input,LSTM, Bidirectional, Dense, Dropout,GRU,SpatialDropout1D, Conv1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.layers import SpatialDropout1D\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "\n",
        "batch_size = 64\n",
        "no_classes = 100\n",
        "no_epochs = 10\n",
        "optimizer = Adam()\n",
        "verbosity = 1\n",
        "num_folds = 5\n",
        "\n",
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "for train, test in kfold.split(inputs, targets):\n",
        "\n",
        "    # Define the model architecture\n",
        "    embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
        "                                          100,\n",
        "                                          weights=[embedding_matrix],\n",
        "                                          input_length=60,\n",
        "                                          trainable=False)\n",
        "    sequence_input = Input(shape=(60,), dtype='int32')\n",
        "    embedding_sequences = embedding_layer(sequence_input)\n",
        "    x = SpatialDropout1D(0.2)(embedding_sequences)\n",
        "    x = Bidirectional(GRU(units=128, return_sequences = True, dropout=0.25, recurrent_dropout=0.25))(x)\n",
        "    x = Bidirectional(GRU(units=128, return_sequences = True, dropout=0.25, recurrent_dropout=0.25))(x)\n",
        "    x = attention()(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dropout(0.25)(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "    model = tf.keras.Model(sequence_input, outputs)\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  # model.summary()\n",
        "\n",
        "  # Generate a print\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  # Fit data to model\n",
        "    history = model.fit(inputs[train], targets[train],\n",
        "              batch_size=batch_size,\n",
        "              epochs=no_epochs,\n",
        "              verbose=verbosity)\n",
        "    model_path = \"../content/fasttext_models/\" + str(fold_no) + \"model\"\n",
        "    \n",
        "#     convert the history.history dict to a pandas DataFrame:     \n",
        "    hist_df = pd.DataFrame(history.history) \n",
        "\n",
        "#     save to json:  \n",
        "    hist_json_file = \"../content/fasttext_history/\" + str(fold_no) + 'history.json' \n",
        "    with open(hist_json_file, mode='w') as f:\n",
        "        hist_df.to_json(f)\n",
        "            \n",
        "#     save model\n",
        "    model.save(model_path)\n",
        "    \n",
        "  # Generate generalization metrics\n",
        "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
        "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "  # Increase fold number\n",
        "    fold_no = fold_no + 1\n",
        "\n",
        "  # == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BI-GRU ATT FT"
      ],
      "metadata": {
        "id": "rk_5VjiE8Ec4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "GfpYKr6G6WF0",
        "outputId": "93bc5830-6f5e-40b8-8e82-37c772ec2e7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/5\n",
            "375/375 [==============================] - 171s 439ms/step - loss: 0.4046 - accuracy: 0.8205\n",
            "Epoch 2/5\n",
            "375/375 [==============================] - 147s 392ms/step - loss: 0.3557 - accuracy: 0.8492\n",
            "Epoch 3/5\n",
            "375/375 [==============================] - 131s 349ms/step - loss: 0.3388 - accuracy: 0.8594\n",
            "Epoch 4/5\n",
            "375/375 [==============================] - 147s 392ms/step - loss: 0.3241 - accuracy: 0.8660\n",
            "Epoch 5/5\n",
            "375/375 [==============================] - 125s 334ms/step - loss: 0.3171 - accuracy: 0.8669\n",
            "Score for fold 1: loss of 0.31690821051597595; accuracy of 86.2833321094513%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/5\n",
            "375/375 [==============================] - 126s 317ms/step - loss: 0.3890 - accuracy: 0.8317\n",
            "Epoch 2/5\n",
            "375/375 [==============================] - 119s 316ms/step - loss: 0.3423 - accuracy: 0.8554\n",
            "Epoch 3/5\n",
            "375/375 [==============================] - 119s 317ms/step - loss: 0.3217 - accuracy: 0.8644\n",
            "Epoch 4/5\n",
            "375/375 [==============================] - 132s 352ms/step - loss: 0.3114 - accuracy: 0.8683\n",
            "Epoch 5/5\n",
            "375/375 [==============================] - 135s 360ms/step - loss: 0.3034 - accuracy: 0.8748\n",
            "Score for fold 2: loss of 0.3066258430480957; accuracy of 87.65000104904175%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/5\n",
            "375/375 [==============================] - 138s 335ms/step - loss: 0.3792 - accuracy: 0.8391\n",
            "Epoch 2/5\n",
            "375/375 [==============================] - 120s 319ms/step - loss: 0.3349 - accuracy: 0.8599\n",
            "Epoch 3/5\n",
            "375/375 [==============================] - 131s 350ms/step - loss: 0.3158 - accuracy: 0.8709\n",
            "Epoch 4/5\n",
            "375/375 [==============================] - 124s 331ms/step - loss: 0.3037 - accuracy: 0.8724\n",
            "Epoch 5/5\n",
            "375/375 [==============================] - 112s 299ms/step - loss: 0.2963 - accuracy: 0.8751\n",
            "Score for fold 3: loss of 0.32030919194221497; accuracy of 87.05000281333923%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/5\n",
            "375/375 [==============================] - 105s 265ms/step - loss: 0.3839 - accuracy: 0.8352\n",
            "Epoch 2/5\n",
            "375/375 [==============================] - 99s 265ms/step - loss: 0.3394 - accuracy: 0.8580\n",
            "Epoch 3/5\n",
            "375/375 [==============================] - 99s 264ms/step - loss: 0.3211 - accuracy: 0.8657\n",
            "Epoch 4/5\n",
            "375/375 [==============================] - 100s 265ms/step - loss: 0.3086 - accuracy: 0.8714\n",
            "Epoch 5/5\n",
            "375/375 [==============================] - 98s 262ms/step - loss: 0.2988 - accuracy: 0.8757\n",
            "Score for fold 4: loss of 0.3099775016307831; accuracy of 87.65000104904175%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/5\n",
            "375/375 [==============================] - 104s 262ms/step - loss: 0.3846 - accuracy: 0.8342\n",
            "Epoch 2/5\n",
            "375/375 [==============================] - 99s 263ms/step - loss: 0.3411 - accuracy: 0.8584\n",
            "Epoch 3/5\n",
            "375/375 [==============================] - 98s 262ms/step - loss: 0.3202 - accuracy: 0.8679\n",
            "Epoch 4/5\n",
            "375/375 [==============================] - 99s 263ms/step - loss: 0.3089 - accuracy: 0.8742\n",
            "Epoch 5/5\n",
            "375/375 [==============================] - 99s 263ms/step - loss: 0.3015 - accuracy: 0.8734\n",
            "Score for fold 5: loss of 0.3076343536376953; accuracy of 87.03333139419556%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.311845064163208 - Accuracy: 88.18333148956299%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.3438962399959564 - Accuracy: 87.99999952316284%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.3144559860229492 - Accuracy: 88.06666731834412%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.32712557911872864 - Accuracy: 87.4666690826416%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.33121421933174133 - Accuracy: 87.56666779518127%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 6 - Loss: 0.3089353144168854 - Accuracy: 87.56666779518127%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 7 - Loss: 0.3088591694831848 - Accuracy: 87.73333430290222%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 8 - Loss: 0.31690821051597595 - Accuracy: 86.2833321094513%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 9 - Loss: 0.3066258430480957 - Accuracy: 87.65000104904175%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 10 - Loss: 0.32030919194221497 - Accuracy: 87.05000281333923%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 11 - Loss: 0.3099775016307831 - Accuracy: 87.65000104904175%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 12 - Loss: 0.3076343536376953 - Accuracy: 87.03333139419556%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 87.52083381017049 (+- 0.5035580240873564)\n",
            "> Loss: 0.31731555610895157\n",
            "------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Model configuration\n",
        "from tensorflow.keras.layers import Dense , Flatten ,Embedding,Input,LSTM, Bidirectional, Dense, Dropout,GRU,SpatialDropout1D, Conv1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.layers import SpatialDropout1D\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "\n",
        "batch_size = 64\n",
        "no_classes = 100\n",
        "no_epochs = 5\n",
        "optimizer = Adam()\n",
        "verbosity = 1\n",
        "num_folds = 5\n",
        "\n",
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "for train, test in kfold.split(inputs, targets):\n",
        "\n",
        "    # Define the model architecture\n",
        "    embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
        "                                          100,\n",
        "                                          weights=[embedding_matrix],\n",
        "                                          input_length=60,\n",
        "                                          trainable=False)\n",
        "    sequence_input = Input(shape=(60,), dtype='int32')\n",
        "    embedding_sequences = embedding_layer(sequence_input)\n",
        "    x = SpatialDropout1D(0.2)(embedding_sequences)\n",
        "    x = Bidirectional(GRU(units=128, return_sequences = True, dropout=0.25, recurrent_dropout=0.25))(x)\n",
        "    x = attention()(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dropout(0.25)(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "    model = tf.keras.Model(sequence_input, outputs)\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  # model.summary()\n",
        "\n",
        "  # Generate a print\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  # Fit data to model\n",
        "    history = model.fit(inputs[train], targets[train],\n",
        "              batch_size=batch_size,\n",
        "              epochs=no_epochs,\n",
        "              verbose=verbosity)\n",
        "#     model_path = \"../content/fasttext_models/\" + str(fold_no) + \"model\"\n",
        "    \n",
        "#     convert the history.history dict to a pandas DataFrame:     \n",
        "#     hist_df = pd.DataFrame(history.history) \n",
        "\n",
        "#     save to json:  \n",
        "#     hist_json_file = \"../content/fasttext_history/\" + str(fold_no) + 'history.json' \n",
        "#     with open(hist_json_file, mode='w') as f:\n",
        "#         hist_df.to_json(f)\n",
        "            \n",
        "#     save model\n",
        "#     model.save(model_path)\n",
        "    \n",
        "  # Generate generalization metrics\n",
        "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
        "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "  # Increase fold number\n",
        "    fold_no = fold_no + 1\n",
        "\n",
        "  # == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BI-GRU FT"
      ],
      "metadata": {
        "id": "vY-17Vgm8Kgc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sxXEATMn6WF1",
        "outputId": "4f5437c2-6a52-461d-df0a-df7623494e04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/5\n",
            "375/375 [==============================] - 123s 313ms/step - loss: 0.4304 - accuracy: 0.8093\n",
            "Epoch 2/5\n",
            "375/375 [==============================] - 125s 334ms/step - loss: 0.3631 - accuracy: 0.8463\n",
            "Epoch 3/5\n",
            "375/375 [==============================] - 143s 381ms/step - loss: 0.3513 - accuracy: 0.8520\n",
            "Epoch 4/5\n",
            "375/375 [==============================] - 155s 412ms/step - loss: 0.3379 - accuracy: 0.8577\n",
            "Epoch 5/5\n",
            "375/375 [==============================] - 173s 461ms/step - loss: 0.3333 - accuracy: 0.8607\n",
            "Score for fold 1: loss of 0.33590734004974365; accuracy of 85.72726249694824%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/5\n",
            "375/375 [==============================] - 227s 582ms/step - loss: 0.4255 - accuracy: 0.8087\n",
            "Epoch 2/5\n",
            "375/375 [==============================] - 224s 598ms/step - loss: 0.3574 - accuracy: 0.8469\n",
            "Epoch 3/5\n",
            "375/375 [==============================] - 123s 328ms/step - loss: 0.3397 - accuracy: 0.8576\n",
            "Epoch 4/5\n",
            "375/375 [==============================] - 125s 333ms/step - loss: 0.3277 - accuracy: 0.8636\n",
            "Epoch 5/5\n",
            "375/375 [==============================] - 124s 331ms/step - loss: 0.3188 - accuracy: 0.8666\n",
            "Score for fold 2: loss of 0.3103717267513275; accuracy of 87.34638690948486%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/5\n",
            "375/375 [==============================] - 133s 333ms/step - loss: 0.4068 - accuracy: 0.8217\n",
            "Epoch 2/5\n",
            "375/375 [==============================] - 125s 333ms/step - loss: 0.3474 - accuracy: 0.8517\n",
            "Epoch 3/5\n",
            "375/375 [==============================] - 127s 337ms/step - loss: 0.3330 - accuracy: 0.8584\n",
            "Epoch 4/5\n",
            "375/375 [==============================] - 136s 362ms/step - loss: 0.3231 - accuracy: 0.8643\n",
            "Epoch 5/5\n",
            "375/375 [==============================] - 133s 356ms/step - loss: 0.3152 - accuracy: 0.8700\n",
            "Score for fold 3: loss of 0.3111148178577423; accuracy of 87.43554949760437%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/5\n",
            "375/375 [==============================] - 141s 358ms/step - loss: 0.4354 - accuracy: 0.8026\n",
            "Epoch 2/5\n",
            "375/375 [==============================] - 137s 364ms/step - loss: 0.3646 - accuracy: 0.8438\n",
            "Epoch 3/5\n",
            "375/375 [==============================] - 155s 413ms/step - loss: 0.3496 - accuracy: 0.8516\n",
            "Epoch 4/5\n",
            "375/375 [==============================] - 171s 457ms/step - loss: 0.3361 - accuracy: 0.8566\n",
            "Epoch 5/5\n",
            "375/375 [==============================] - 188s 502ms/step - loss: 0.3281 - accuracy: 0.8623\n",
            "Score for fold 4: loss of 0.3178783059120178; accuracy of 86.50416135787964%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/5\n",
            "375/375 [==============================] - 131s 321ms/step - loss: 0.4209 - accuracy: 0.8166\n",
            "Epoch 2/5\n",
            "375/375 [==============================] - 120s 320ms/step - loss: 0.3503 - accuracy: 0.8526\n",
            "Epoch 3/5\n",
            "375/375 [==============================] - 135s 360ms/step - loss: 0.3378 - accuracy: 0.8603\n",
            "Epoch 4/5\n",
            "375/375 [==============================] - 170s 452ms/step - loss: 0.3266 - accuracy: 0.8655\n",
            "Epoch 5/5\n",
            "375/375 [==============================] - 185s 494ms/step - loss: 0.3213 - accuracy: 0.8659\n",
            "Score for fold 5: loss of 0.31216880679130554; accuracy of 87.14332580566406%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.311845064163208 - Accuracy: 88.18333148956299%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.3438962399959564 - Accuracy: 87.99999952316284%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.3144559860229492 - Accuracy: 88.06666731834412%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.32712557911872864 - Accuracy: 87.4666690826416%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.33121421933174133 - Accuracy: 87.56666779518127%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 6 - Loss: 0.3089353144168854 - Accuracy: 87.56666779518127%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 7 - Loss: 0.3088591694831848 - Accuracy: 87.73333430290222%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 8 - Loss: 0.31690821051597595 - Accuracy: 86.2833321094513%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 9 - Loss: 0.3066258430480957 - Accuracy: 87.65000104904175%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 10 - Loss: 0.32030919194221497 - Accuracy: 87.05000281333923%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 11 - Loss: 0.3099775016307831 - Accuracy: 87.65000104904175%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 12 - Loss: 0.3076343536376953 - Accuracy: 87.03333139419556%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 13 - Loss: 0.33590734004974365 - Accuracy: 85.72726249694824%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 14 - Loss: 0.3103717267513275 - Accuracy: 87.34638690948486%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 15 - Loss: 0.3111148178577423 - Accuracy: 87.43554949760437%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 16 - Loss: 0.3178783059120178 - Accuracy: 86.50416135787964%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 17 - Loss: 0.31216880679130554 - Accuracy: 87.14332580566406%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 87.31804069350747 (+- 0.6312667350890057)\n",
            "> Loss: 0.3173663335687974\n",
            "------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Model configuration\n",
        "from tensorflow.keras.layers import Dense , Flatten ,Embedding,Input,LSTM, Bidirectional, Dense, Dropout,GRU,SpatialDropout1D, Conv1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.layers import SpatialDropout1D\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "\n",
        "batch_size = 64\n",
        "no_classes = 100\n",
        "no_epochs = 5\n",
        "optimizer = Adam()\n",
        "verbosity = 1\n",
        "num_folds = 5\n",
        "\n",
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "for train, test in kfold.split(inputs, targets):\n",
        "\n",
        "    # Define the model architecture\n",
        "    embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
        "                                          100,\n",
        "                                          weights=[embedding_matrix],\n",
        "                                          input_length=60,\n",
        "                                          trainable=False)\n",
        "    sequence_input = Input(shape=(60,), dtype='int32')\n",
        "    embedding_sequences = embedding_layer(sequence_input)\n",
        "    x = SpatialDropout1D(0.2)(embedding_sequences)\n",
        "    x = Bidirectional(GRU(units=128, return_sequences = True, dropout=0.25, recurrent_dropout=0.25))(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dropout(0.25)(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "    model = tf.keras.Model(sequence_input, outputs)\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  # model.summary()\n",
        "\n",
        "  # Generate a print\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  # Fit data to model\n",
        "    history = model.fit(inputs[train], targets[train],\n",
        "              batch_size=batch_size,\n",
        "              epochs=no_epochs,\n",
        "              verbose=verbosity)\n",
        "#     model_path = \"../content/fasttext_models/\" + str(fold_no) + \"model\"\n",
        "    \n",
        "#     convert the history.history dict to a pandas DataFrame:     \n",
        "#     hist_df = pd.DataFrame(history.history) \n",
        "\n",
        "#     save to json:  \n",
        "#     hist_json_file = \"../content/fasttext_history/\" + str(fold_no) + 'history.json' \n",
        "#     with open(hist_json_file, mode='w') as f:\n",
        "#         hist_df.to_json(f)\n",
        "            \n",
        "#     save model\n",
        "#     model.save(model_path)\n",
        "    \n",
        "  # Generate generalization metrics\n",
        "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
        "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "  # Increase fold number\n",
        "    fold_no = fold_no + 1\n",
        "\n",
        "  # == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM FT"
      ],
      "metadata": {
        "id": "zra3D1vf8Ngj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "QEYK34Hp6WF2",
        "outputId": "990576ab-15a9-43cf-96e3-da106b235e6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/5\n",
            "375/375 [==============================] - 111s 267ms/step - loss: 0.4439 - accuracy: 0.8023\n",
            "Epoch 2/5\n",
            "375/375 [==============================] - 113s 300ms/step - loss: 0.3936 - accuracy: 0.8276\n",
            "Epoch 3/5\n",
            "375/375 [==============================] - 98s 262ms/step - loss: 0.3714 - accuracy: 0.8404\n",
            "Epoch 4/5\n",
            "375/375 [==============================] - 81s 215ms/step - loss: 0.3602 - accuracy: 0.8468\n",
            "Epoch 5/5\n",
            "375/375 [==============================] - 80s 213ms/step - loss: 0.3497 - accuracy: 0.8513\n",
            "Score for fold 1: loss of 0.3370093107223511; accuracy of 85.92779040336609%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/5\n",
            "375/375 [==============================] - 89s 227ms/step - loss: 0.4445 - accuracy: 0.7967\n",
            "Epoch 2/5\n",
            "375/375 [==============================] - 80s 213ms/step - loss: 0.3822 - accuracy: 0.8376\n",
            "Epoch 3/5\n",
            "375/375 [==============================] - 79s 211ms/step - loss: 0.3685 - accuracy: 0.8424\n",
            "Epoch 4/5\n",
            "375/375 [==============================] - 79s 212ms/step - loss: 0.3589 - accuracy: 0.8470\n",
            "Epoch 5/5\n",
            "375/375 [==============================] - 79s 211ms/step - loss: 0.3529 - accuracy: 0.8520\n",
            "Score for fold 2: loss of 0.33912110328674316; accuracy of 85.6732964515686%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/5\n",
            "375/375 [==============================] - 83s 210ms/step - loss: 0.4687 - accuracy: 0.7818\n",
            "Epoch 2/5\n",
            "375/375 [==============================] - 80s 212ms/step - loss: 0.3954 - accuracy: 0.8302\n",
            "Epoch 3/5\n",
            "375/375 [==============================] - 79s 210ms/step - loss: 0.3838 - accuracy: 0.8397\n",
            "Epoch 4/5\n",
            "375/375 [==============================] - 80s 212ms/step - loss: 0.3692 - accuracy: 0.8447\n",
            "Epoch 5/5\n",
            "375/375 [==============================] - 79s 210ms/step - loss: 0.3601 - accuracy: 0.8496\n",
            "Score for fold 3: loss of 0.34115079045295715; accuracy of 85.52971482276917%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/5\n",
            "375/375 [==============================] - 83s 213ms/step - loss: 0.4571 - accuracy: 0.7970\n",
            "Epoch 2/5\n",
            "375/375 [==============================] - 79s 211ms/step - loss: 0.3980 - accuracy: 0.8293\n",
            "Epoch 3/5\n",
            "375/375 [==============================] - 83s 220ms/step - loss: 0.3777 - accuracy: 0.8385\n",
            "Epoch 4/5\n",
            "375/375 [==============================] - 86s 230ms/step - loss: 0.3742 - accuracy: 0.8338\n",
            "Epoch 5/5\n",
            "375/375 [==============================] - 95s 253ms/step - loss: 0.3582 - accuracy: 0.8466\n",
            "Score for fold 4: loss of 0.35374337434768677; accuracy of 84.46693420410156%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/5\n",
            "375/375 [==============================] - 86s 222ms/step - loss: 0.4298 - accuracy: 0.8080\n",
            "Epoch 2/5\n",
            "375/375 [==============================] - 82s 219ms/step - loss: 0.3720 - accuracy: 0.8407\n",
            "Epoch 3/5\n",
            "375/375 [==============================] - 93s 248ms/step - loss: 0.3551 - accuracy: 0.8497\n",
            "Epoch 4/5\n",
            "375/375 [==============================] - 109s 291ms/step - loss: 0.3476 - accuracy: 0.8528\n",
            "Epoch 5/5\n",
            "375/375 [==============================] - 110s 293ms/step - loss: 0.3378 - accuracy: 0.8593\n",
            "Score for fold 5: loss of 0.34680038690567017; accuracy of 85.87194085121155%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.311845064163208 - Accuracy: 88.18333148956299%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.3438962399959564 - Accuracy: 87.99999952316284%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.3144559860229492 - Accuracy: 88.06666731834412%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.32712557911872864 - Accuracy: 87.4666690826416%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.33121421933174133 - Accuracy: 87.56666779518127%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 6 - Loss: 0.3089353144168854 - Accuracy: 87.56666779518127%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 7 - Loss: 0.3088591694831848 - Accuracy: 87.73333430290222%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 8 - Loss: 0.31690821051597595 - Accuracy: 86.2833321094513%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 9 - Loss: 0.3066258430480957 - Accuracy: 87.65000104904175%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 10 - Loss: 0.32030919194221497 - Accuracy: 87.05000281333923%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 11 - Loss: 0.3099775016307831 - Accuracy: 87.65000104904175%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 12 - Loss: 0.3076343536376953 - Accuracy: 87.03333139419556%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 13 - Loss: 0.33590734004974365 - Accuracy: 85.72726249694824%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 14 - Loss: 0.3103717267513275 - Accuracy: 87.34638690948486%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 15 - Loss: 0.3111148178577423 - Accuracy: 87.43554949760437%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 16 - Loss: 0.3178783059120178 - Accuracy: 86.50416135787964%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 17 - Loss: 0.31216880679130554 - Accuracy: 87.14332580566406%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 18 - Loss: 0.3370093107223511 - Accuracy: 85.92779040336609%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 19 - Loss: 0.33912110328674316 - Accuracy: 85.6732964515686%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 20 - Loss: 0.34115079045295715 - Accuracy: 85.52971482276917%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 21 - Loss: 0.35374337434768677 - Accuracy: 84.46693420410156%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 22 - Loss: 0.34680038690567017 - Accuracy: 85.87194085121155%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 86.90347129648381 (+- 0.9781585517891779)\n",
            "> Loss: 0.3233205743811347\n",
            "------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Model configuration\n",
        "from tensorflow.keras.layers import Dense , Flatten ,Embedding,Input,LSTM, Bidirectional, Dense, Dropout,GRU,SpatialDropout1D, Conv1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.layers import SpatialDropout1D\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "\n",
        "batch_size = 64\n",
        "no_classes = 100\n",
        "no_epochs = 5\n",
        "optimizer = Adam()\n",
        "verbosity = 1\n",
        "num_folds = 5\n",
        "\n",
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "for train, test in kfold.split(inputs, targets):\n",
        "\n",
        "    # Define the model architecture\n",
        "    embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
        "                                          100,\n",
        "                                          weights=[embedding_matrix],\n",
        "                                          input_length=60,\n",
        "                                          trainable=False)\n",
        "    sequence_input = Input(shape=(60,), dtype='int32')\n",
        "    embedding_sequences = embedding_layer(sequence_input)\n",
        "    x = SpatialDropout1D(0.2)(embedding_sequences)\n",
        "    x = LSTM(units=128, return_sequences = True, dropout=0.25, recurrent_dropout=0.25)(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dropout(0.25)(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "    model = tf.keras.Model(sequence_input, outputs)\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  # model.summary()\n",
        "\n",
        "  # Generate a print\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  # Fit data to model\n",
        "    history = model.fit(inputs[train], targets[train],\n",
        "              batch_size=batch_size,\n",
        "              epochs=no_epochs,\n",
        "              verbose=verbosity)\n",
        "#     model_path = \"../content/fasttext_models/\" + str(fold_no) + \"model\"\n",
        "    \n",
        "#     convert the history.history dict to a pandas DataFrame:     \n",
        "#     hist_df = pd.DataFrame(history.history) \n",
        "\n",
        "#     save to json:  \n",
        "#     hist_json_file = \"../content/fasttext_history/\" + str(fold_no) + 'history.json' \n",
        "#     with open(hist_json_file, mode='w') as f:\n",
        "#         hist_df.to_json(f)\n",
        "            \n",
        "#     save model\n",
        "#     model.save(model_path)\n",
        "    \n",
        "  # Generate generalization metrics\n",
        "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
        "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "  # Increase fold number\n",
        "    fold_no = fold_no + 1\n",
        "\n",
        "  # == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "7t8QZNxpcipD",
        "J_siWNqaks8x",
        "YlBi4xvvuJax",
        "-uV8RrDeuVQc",
        "uk99_ApDWDwq",
        "fSpmZrJuLITR",
        "tX3IZPLCLRdl",
        "83gbUbRaLUyG",
        "SPmUYq_d6c3M",
        "Q1pP3Sap6i4T",
        "S6oqabeq6mTN",
        "3Xmma9IhLcyF",
        "RVKoqK9W6q9_",
        "fRhthajK6twf",
        "tHLtYPg66zJ9",
        "Hn0pAUZnLiKY",
        "wvD-bko464_B",
        "ueOvG3SI69wK",
        "UBHU_3Hl7B9L",
        "c5F9M6x17Fd7",
        "Rt41tLh57NAr",
        "_oIJXeRk7RG3",
        "I-yxqaP07eO8",
        "lbuwTESs7hxi",
        "JlJZYnQD7luI",
        "J801OMmI7pkc",
        "9Zi4ynEr7uW8",
        "2ihlgL3M7yIF",
        "-xfN_N1R70-C",
        "WVOYEJ7Q732P",
        "U5H-iaaQ8AVa",
        "rk_5VjiE8Ec4",
        "vY-17Vgm8Kgc",
        "zra3D1vf8Ngj"
      ],
      "name": "English and Bangla Hate Speech 5-fold.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}