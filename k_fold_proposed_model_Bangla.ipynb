{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/golammostafa13/hate_speech_detection/blob/main/k_fold_proposed_model_Bangla.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ed5CzfiRaGhX",
        "outputId": "8f612877-3853-469c-b191-8971befc55d3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-04-03 05:11:38.331199: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "2022-04-03 05:11:38.331276: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYU0vOgidjq8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import one_hot,Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense , Flatten ,Embedding,Input,LSTM, Bidirectional, Dense, Dropout,GRU,SpatialDropout1D, Conv1D\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "from tensorflow.keras.initializers import Constant\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import np_utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "860u8tr9dv3p"
      },
      "outputs": [],
      "source": [
        "file = 'content/cleaned_data.csv'\n",
        "data = pd.read_csv(file, encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTndhpASg3FA",
        "outputId": "67de4da9-a764-4aae-d130-f853dcb138aa"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>sentence</th>\n",
              "      <th>hate</th>\n",
              "      <th>category</th>\n",
              "      <th>cleaned_text</th>\n",
              "      <th>sentenceLength</th>\n",
              "      <th>ReviewChars</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>যত্তসব পাপন শালার ফাজলামী!!!!!</td>\n",
              "      <td>1</td>\n",
              "      <td>sports</td>\n",
              "      <td>যত্তসব পাপন শালার ফাজলামী</td>\n",
              "      <td>4</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>পাপন শালা রে রিমান্ডে নেওয়া দরকার</td>\n",
              "      <td>1</td>\n",
              "      <td>sports</td>\n",
              "      <td>পাপন শালা রে রিমান্ডে নেওয়া দরকার</td>\n",
              "      <td>6</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>জিল্লুর রহমান স্যারের ছেলে এতো বড় জারজ হবে এটা...</td>\n",
              "      <td>1</td>\n",
              "      <td>sports</td>\n",
              "      <td>জিল্লুর রহমান স্যারের ছেলে এতো বড় জারজ হবে এটা...</td>\n",
              "      <td>20</td>\n",
              "      <td>107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>শালা লুচ্চা দেখতে পাঠার মত দেখা যায়</td>\n",
              "      <td>1</td>\n",
              "      <td>sports</td>\n",
              "      <td>শালা লুচ্চা দেখতে পাঠার মত দেখা যায়</td>\n",
              "      <td>7</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>তুই তো শালা গাজা খাইছচ।তুর মার হেডায় খেলবে সাকিব</td>\n",
              "      <td>1</td>\n",
              "      <td>sports</td>\n",
              "      <td>তুই তো শালা গাজা খাইছচ তুর মার হেডায় খেলবে সাকিব</td>\n",
              "      <td>10</td>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0                                           sentence  hate  \\\n",
              "0           0                     যত্তসব পাপন শালার ফাজলামী!!!!!     1   \n",
              "1           1                  পাপন শালা রে রিমান্ডে নেওয়া দরকার     1   \n",
              "2           2  জিল্লুর রহমান স্যারের ছেলে এতো বড় জারজ হবে এটা...     1   \n",
              "3           3                শালা লুচ্চা দেখতে পাঠার মত দেখা যায়     1   \n",
              "4           4   তুই তো শালা গাজা খাইছচ।তুর মার হেডায় খেলবে সাকিব     1   \n",
              "\n",
              "  category                                       cleaned_text  sentenceLength  \\\n",
              "0   sports                     যত্তসব পাপন শালার ফাজলামী                    4   \n",
              "1   sports                  পাপন শালা রে রিমান্ডে নেওয়া দরকার               6   \n",
              "2   sports  জিল্লুর রহমান স্যারের ছেলে এতো বড় জারজ হবে এটা...              20   \n",
              "3   sports                শালা লুচ্চা দেখতে পাঠার মত দেখা যায়               7   \n",
              "4   sports   তুই তো শালা গাজা খাইছচ তুর মার হেডায় খেলবে সাকিব              10   \n",
              "\n",
              "   ReviewChars  \n",
              "0           30  \n",
              "1           33  \n",
              "2          107  \n",
              "3           35  \n",
              "4           48  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYBM2y0Edyut",
        "outputId": "34532aaf-457b-4f41-ac4e-06c175dc54ec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((24000,), (6000,), (24000,), (6000,))"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['cleaned_text'], data['hate'],random_state = 0,test_size = 0.20)\n",
        "X_train.shape,X_test.shape,y_train.shape,y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DT0rb9sd3CI",
        "outputId": "c37fbf6c-8fcf-443c-bdf2-eca84e173947"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total words =  509593\n",
            "Total Sentances =  30000\n"
          ]
        }
      ],
      "source": [
        "corpus=[]\n",
        "words = 0;\n",
        "j = 0\n",
        "for i in data['cleaned_text'].values:\n",
        "    corpus.append(str(i).split(\" \"))\n",
        "    words += len(corpus[j])\n",
        "    j += 1\n",
        "print(\"Total words = \", words)\n",
        "print(\"Total Sentances = \", len(corpus))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADCgZl2nd-JT",
        "outputId": "6ee2680f-e8a6-41cb-d82c-27ea536ed437"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mostafa/env/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of vocabulary :  47752\n",
            "All sentances with same length  (30000, 60)\n"
          ]
        }
      ],
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "\n",
        "# tokenize every words so that evey words maps to numaric value\n",
        "tok = Tokenizer()\n",
        "tok.fit_on_texts(data['cleaned_text'].values.astype('U'))\n",
        "encd_rev = tok.texts_to_sequences(data['cleaned_text'].values.astype('U'))\n",
        "# tok.word_index\n",
        "\n",
        "# make all the input sentance same length with add padding\n",
        "max_rev_len = 60 # max lenght of a sentance\n",
        "vocab_size = len(tok.word_index) + 1  # total no of words\n",
        "embed_dim = 100 # embedding dimension \n",
        "pad_rev= pad_sequences(encd_rev, maxlen=60, padding='post')\n",
        "\n",
        "print(\"Size of vocabulary : \", vocab_size)\n",
        "print(\"All sentances with same length \", pad_rev.shape)\n",
        "\n",
        "# lebel encode the output label to categorical\n",
        "Y = data['hate']\n",
        "# encode class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "encoded_Y = encoder.transform(Y)\n",
        "# # convert integers to dummy variables (i.e. one hot encoded)\n",
        "# dummy_y = np_utils.to_categorical(encoded_Y)\n",
        "\n",
        "# print(encoder.classes_)\n",
        "# print(dummy_y)\n",
        "\n",
        "# test train split\n",
        "X_train,X_test,y_train,y_test=train_test_split(pad_rev,encoded_Y,test_size=0.2, random_state = 0)\n",
        "\n",
        "\n",
        "# make a dictionary. word as key and feature vector as value\n",
        "embedding_index={}\n",
        "f = open('content/ban_hate_w2v.txt',encoding='utf-8')\n",
        "for line in f:\n",
        "    values=line.split()\n",
        "    word=values[0]\n",
        "    coefs=np.asarray(values[1:])\n",
        "    embedding_index[word]=coefs\n",
        "f.close()\n",
        "\n",
        "# create a embeddings matrix with 200 dimenstion\n",
        "EMBEDDING_DIM=100\n",
        "embedding_matrix=np.zeros((vocab_size,EMBEDDING_DIM))\n",
        "for word, i in tok.word_index.items():\n",
        "    if i>vocab_size:\n",
        "        continue\n",
        "    embedding_vector=embedding_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i]=embedding_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkVEI5a5eF2A",
        "outputId": "5909a0df-3d77-4fd1-e137-d305bc04bc10"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((24000, 60), (6000, 60), (24000,), (6000,))"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLaYV02SeMsl",
        "outputId": "d750f1ec-f915-428a-9952-6a04f2aef7ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(24000, 1) (6000, 1)\n"
          ]
        }
      ],
      "source": [
        "y_train = y_train.reshape(-1, 1)\n",
        "y_test = y_test.reshape(-1, 1)\n",
        "print(y_train.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFM56fBPaau6"
      },
      "outputs": [],
      "source": [
        "# Model configuration\n",
        "batch_size = 64\n",
        "no_classes = 100\n",
        "no_epochs = 5\n",
        "optimizer = Adam()\n",
        "verbosity = 1\n",
        "num_folds = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viL7TP0zaq-r"
      },
      "outputs": [],
      "source": [
        "# Define per-fold score containers\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "# Merge inputs and targets\n",
        "inputs = np.concatenate((X_train, X_test), axis=0)\n",
        "targets = np.concatenate((y_train, y_test), axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNsFo_HpgQQc",
        "outputId": "1ff24a7b-4f03-4fc3-f056-eea7b42e102e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((30000, 60), (30000, 1))"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs.shape, targets.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iliHOAdogXKD"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Layer\n",
        "from keras import backend as K\n",
        "class attention(Layer):\n",
        "    def __init__(self,**kwargs):\n",
        "        super(attention,self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\")\n",
        "        self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),initializer=\"zeros\")        \n",
        "        super(attention, self).build(input_shape)\n",
        "\n",
        "    def call(self,x):\n",
        "        et=K.squeeze(K.tanh(K.dot(x,self.W)+self.b),axis=-1)\n",
        "        at=K.softmax(et)\n",
        "        at=K.expand_dims(at,axis=-1)\n",
        "        output=x*at\n",
        "        return K.sum(output,axis=1)\n",
        "\n",
        "    def compute_output_shape(self,input_shape):\n",
        "        return (input_shape[0],input_shape[-1])\n",
        "\n",
        "    def get_config(self):\n",
        "        return super(attention,self).get_config()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# proposedModel(word2vec)"
      ],
      "metadata": {
        "id": "VkSkynY4g8hF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "8BD4LwjXO_AM",
        "outputId": "8213738d-cd07-42ce-da0e-5090ba84cc25"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-03-26 14:52:02.740884: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
            "2022-03-26 14:52:02.740996: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
            "2022-03-26 14:52:02.741095: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (mostafa): /proc/driver/nvidia/version does not exist\n",
            "2022-03-26 14:52:02.851077: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/5\n",
            "422/422 [==============================] - 498s 1s/step - loss: 0.4040 - accuracy: 0.8219\n",
            "Epoch 2/5\n",
            "422/422 [==============================] - 485s 1s/step - loss: 0.3512 - accuracy: 0.8483\n",
            "Epoch 3/5\n",
            "422/422 [==============================] - 534s 1s/step - loss: 0.3289 - accuracy: 0.8616\n",
            "Epoch 4/5\n",
            "422/422 [==============================] - 539s 1s/step - loss: 0.3145 - accuracy: 0.8684\n",
            "Epoch 5/5\n",
            "422/422 [==============================] - 498s 1s/step - loss: 0.3016 - accuracy: 0.8713\n",
            "Score for fold 1: loss of 0.32966700196266174; accuracy of 86.43333315849304%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/5\n",
            "422/422 [==============================] - 577s 1s/step - loss: 0.4083 - accuracy: 0.8167\n",
            "Epoch 2/5\n",
            "422/422 [==============================] - 626s 1s/step - loss: 0.3552 - accuracy: 0.8457\n",
            "Epoch 3/5\n",
            "422/422 [==============================] - 686s 2s/step - loss: 0.3322 - accuracy: 0.8589\n",
            "Epoch 4/5\n",
            "422/422 [==============================] - 355s 840ms/step - loss: 0.3171 - accuracy: 0.8638\n",
            "Epoch 5/5\n",
            "422/422 [==============================] - 355s 841ms/step - loss: 0.3030 - accuracy: 0.8716\n",
            "Score for fold 2: loss of 0.31441137194633484; accuracy of 87.03333139419556%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/5\n",
            "422/422 [==============================] - 371s 846ms/step - loss: 0.4093 - accuracy: 0.8156\n",
            "Epoch 2/5\n",
            "422/422 [==============================] - 357s 847ms/step - loss: 0.3550 - accuracy: 0.8476\n",
            "Epoch 3/5\n",
            "422/422 [==============================] - 358s 849ms/step - loss: 0.3319 - accuracy: 0.8590\n",
            "Epoch 4/5\n",
            "422/422 [==============================] - 351s 832ms/step - loss: 0.3182 - accuracy: 0.8644\n",
            "Epoch 5/5\n",
            "422/422 [==============================] - 358s 849ms/step - loss: 0.3033 - accuracy: 0.8717\n",
            "Score for fold 3: loss of 0.32594355940818787; accuracy of 86.10000014305115%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/5\n",
            "422/422 [==============================] - 417s 950ms/step - loss: 0.4123 - accuracy: 0.8166\n",
            "Epoch 2/5\n",
            "422/422 [==============================] - 409s 969ms/step - loss: 0.3567 - accuracy: 0.8481\n",
            "Epoch 3/5\n",
            "422/422 [==============================] - 403s 954ms/step - loss: 0.3313 - accuracy: 0.8600\n",
            "Epoch 4/5\n",
            "422/422 [==============================] - 400s 947ms/step - loss: 0.3200 - accuracy: 0.8656\n",
            "Epoch 5/5\n",
            "422/422 [==============================] - 399s 946ms/step - loss: 0.3044 - accuracy: 0.8713\n",
            "Score for fold 4: loss of 0.3160596787929535; accuracy of 87.23333477973938%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/5\n",
            "422/422 [==============================] - 417s 953ms/step - loss: 0.4108 - accuracy: 0.8181\n",
            "Epoch 2/5\n",
            "422/422 [==============================] - 403s 954ms/step - loss: 0.3556 - accuracy: 0.8472\n",
            "Epoch 3/5\n",
            "422/422 [==============================] - 380s 899ms/step - loss: 0.3320 - accuracy: 0.8596\n",
            "Epoch 4/5\n",
            "422/422 [==============================] - 356s 845ms/step - loss: 0.3151 - accuracy: 0.8675\n",
            "Epoch 5/5\n",
            "422/422 [==============================] - 348s 825ms/step - loss: 0.3049 - accuracy: 0.8722\n",
            "Score for fold 5: loss of 0.3092878758907318; accuracy of 87.00000047683716%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 6 ...\n",
            "Epoch 1/5\n",
            "422/422 [==============================] - 393s 899ms/step - loss: 0.4105 - accuracy: 0.8160\n",
            "Epoch 2/5\n",
            "422/422 [==============================] - 545s 1s/step - loss: 0.3539 - accuracy: 0.8488\n",
            "Epoch 3/5\n",
            "422/422 [==============================] - 352s 834ms/step - loss: 0.3330 - accuracy: 0.8598\n",
            "Epoch 4/5\n",
            "422/422 [==============================] - 346s 821ms/step - loss: 0.3171 - accuracy: 0.8658\n",
            "Epoch 5/5\n",
            "422/422 [==============================] - 349s 826ms/step - loss: 0.3042 - accuracy: 0.8717\n",
            "Score for fold 6: loss of 0.3041330575942993; accuracy of 87.90000081062317%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 7 ...\n",
            "Epoch 1/5\n",
            "422/422 [==============================] - 405s 823ms/step - loss: 0.4060 - accuracy: 0.8198\n",
            "Epoch 2/5\n",
            "422/422 [==============================] - 475s 1s/step - loss: 0.3520 - accuracy: 0.8505\n",
            "Epoch 3/5\n",
            "422/422 [==============================] - 491s 1s/step - loss: 0.3320 - accuracy: 0.8596\n",
            "Epoch 4/5\n",
            "422/422 [==============================] - 436s 1s/step - loss: 0.3154 - accuracy: 0.8672\n",
            "Epoch 5/5\n",
            "422/422 [==============================] - 350s 828ms/step - loss: 0.3042 - accuracy: 0.8733\n",
            "Score for fold 7: loss of 0.3201405704021454; accuracy of 86.13333106040955%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 8 ...\n",
            "Epoch 1/5\n",
            "422/422 [==============================] - 369s 842ms/step - loss: 0.4098 - accuracy: 0.8185\n",
            "Epoch 2/5\n",
            "422/422 [==============================] - 356s 843ms/step - loss: 0.3552 - accuracy: 0.8491\n",
            "Epoch 3/5\n",
            "422/422 [==============================] - 355s 840ms/step - loss: 0.3344 - accuracy: 0.8602\n",
            "Epoch 4/5\n",
            "422/422 [==============================] - 399s 946ms/step - loss: 0.3145 - accuracy: 0.8664\n",
            "Epoch 5/5\n",
            "422/422 [==============================] - 401s 950ms/step - loss: 0.3002 - accuracy: 0.8750\n",
            "Score for fold 8: loss of 0.3437519967556; accuracy of 86.53333187103271%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 9 ...\n",
            "Epoch 1/5\n",
            "422/422 [==============================] - 415s 945ms/step - loss: 0.4086 - accuracy: 0.8188\n",
            "Epoch 2/5\n",
            "422/422 [==============================] - 399s 946ms/step - loss: 0.3506 - accuracy: 0.8474\n",
            "Epoch 3/5\n",
            "422/422 [==============================] - 399s 946ms/step - loss: 0.3311 - accuracy: 0.8616\n",
            "Epoch 4/5\n",
            "422/422 [==============================] - 401s 949ms/step - loss: 0.3147 - accuracy: 0.8656\n",
            "Epoch 5/5\n",
            "422/422 [==============================] - 401s 951ms/step - loss: 0.3018 - accuracy: 0.8717\n",
            "Score for fold 9: loss of 0.3353439271450043; accuracy of 86.63333058357239%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 10 ...\n",
            "Epoch 1/5\n",
            "422/422 [==============================] - 415s 947ms/step - loss: 0.4034 - accuracy: 0.8208\n",
            "Epoch 2/5\n",
            "422/422 [==============================] - 399s 946ms/step - loss: 0.3529 - accuracy: 0.8484\n",
            "Epoch 3/5\n",
            "422/422 [==============================] - 400s 948ms/step - loss: 0.3312 - accuracy: 0.8585\n",
            "Epoch 4/5\n",
            "422/422 [==============================] - 397s 942ms/step - loss: 0.3185 - accuracy: 0.8656\n",
            "Epoch 5/5\n",
            "422/422 [==============================] - 397s 942ms/step - loss: 0.3022 - accuracy: 0.8727\n",
            "Score for fold 10: loss of 0.3409653604030609; accuracy of 86.00000143051147%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.32966700196266174 - Accuracy: 86.43333315849304%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.31441137194633484 - Accuracy: 87.03333139419556%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.32594355940818787 - Accuracy: 86.10000014305115%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.3160596787929535 - Accuracy: 87.23333477973938%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.3092878758907318 - Accuracy: 87.00000047683716%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 6 - Loss: 0.3041330575942993 - Accuracy: 87.90000081062317%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 7 - Loss: 0.3201405704021454 - Accuracy: 86.13333106040955%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 8 - Loss: 0.3437519967556 - Accuracy: 86.53333187103271%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 9 - Loss: 0.3353439271450043 - Accuracy: 86.63333058357239%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 10 - Loss: 0.3409653604030609 - Accuracy: 86.00000143051147%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 86.69999957084656 (+- 0.5656857635307517)\n",
            "> Loss: 0.32397044003009795\n",
            "------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "for train, test in kfold.split(inputs, targets):\n",
        "\n",
        "  # Define the model architecture\n",
        "  model=Sequential()\n",
        "  embedding_layer=Embedding(vocab_size,\n",
        "                        EMBEDDING_DIM,\n",
        "                        embeddings_initializer=Constant(embedding_matrix),\n",
        "                        input_length=60,\n",
        "                        trainable=False)  \n",
        "  model.add(embedding_layer)\n",
        "  model.add(Bidirectional(GRU(units=128, return_sequences = True, dropout=0.25, recurrent_dropout=0.25)))\n",
        "  model.add(Bidirectional(GRU(units=128, return_sequences = True, dropout=0.25, recurrent_dropout=0.25)))\n",
        "  model.add(attention())\n",
        "  model.add(Dense(256, activation='relu'))\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Dense(256, activation='relu'))\n",
        "  model.add(Dense(1,activation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "  # model.summary()\n",
        "\n",
        "  # Generate a print\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  # Fit data to model\n",
        "  history = model.fit(inputs[train], targets[train],\n",
        "              batch_size=batch_size,\n",
        "              epochs=no_epochs,\n",
        "              verbose=verbosity)\n",
        "\n",
        "  # Generate generalization metrics\n",
        "  scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
        "  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "  acc_per_fold.append(scores[1] * 100)\n",
        "  loss_per_fold.append(scores[0])\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1\n",
        "\n",
        "  # == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGGfy5Y2g3FV",
        "outputId": "6eca4d70-dacf-4174-faf3-2ea7096296b2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mostafa/env/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of vocabulary :  47752\n",
            "All sentances with same length  (30000, 60)\n"
          ]
        }
      ],
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "\n",
        "# tokenize every words so that evey words maps to numaric value\n",
        "tok = Tokenizer()\n",
        "tok.fit_on_texts(data['cleaned_text'].values.astype('U'))\n",
        "encd_rev = tok.texts_to_sequences(data['cleaned_text'].values.astype('U'))\n",
        "# tok.word_index\n",
        "\n",
        "# make all the input sentance same length with add padding\n",
        "max_rev_len = 60 # max lenght of a sentance\n",
        "vocab_size = len(tok.word_index) + 1  # total no of words\n",
        "embed_dim = 100 # embedding dimension \n",
        "pad_rev= pad_sequences(encd_rev, maxlen=60, padding='post')\n",
        "\n",
        "print(\"Size of vocabulary : \", vocab_size)\n",
        "print(\"All sentances with same length \", pad_rev.shape)\n",
        "\n",
        "# lebel encode the output label to categorical\n",
        "Y = data['hate']\n",
        "# encode class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "encoded_Y = encoder.transform(Y)\n",
        "# # convert integers to dummy variables (i.e. one hot encoded)\n",
        "# dummy_y = np_utils.to_categorical(encoded_Y)\n",
        "\n",
        "# print(encoder.classes_)\n",
        "# print(dummy_y)\n",
        "\n",
        "# test train split\n",
        "X_train,X_test,y_train,y_test=train_test_split(pad_rev,encoded_Y,test_size=0.2, random_state = 0)\n",
        "\n",
        "\n",
        "# make a dictionary. word as key and feature vector as value\n",
        "embedding_index={}\n",
        "f = open('content/bn_glove.39M.100d.txt',encoding='utf-8')\n",
        "for line in f:\n",
        "    values=line.split()\n",
        "    word=values[0]\n",
        "    coefs=np.asarray(values[1:])\n",
        "    embedding_index[word]=coefs\n",
        "f.close()\n",
        "\n",
        "# create a embeddings matrix with 200 dimenstion\n",
        "EMBEDDING_DIM=100\n",
        "embedding_matrix=np.zeros((vocab_size,EMBEDDING_DIM))\n",
        "for word, i in tok.word_index.items():\n",
        "    if i>vocab_size:\n",
        "        continue\n",
        "    embedding_vector=embedding_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i]=embedding_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3J7Xum6g3FW",
        "outputId": "67064f28-dc34-40ba-d9b5-bc9d11ce34dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(24000, 1) (6000, 1)\n"
          ]
        }
      ],
      "source": [
        "y_train = y_train.reshape(-1, 1)\n",
        "y_test = y_test.reshape(-1, 1)\n",
        "print(y_train.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRRih9Wtg3FY"
      },
      "outputs": [],
      "source": [
        "# Model configuration\n",
        "batch_size = 64\n",
        "no_classes = 100\n",
        "no_epochs = 5\n",
        "optimizer = Adam()\n",
        "verbosity = 1\n",
        "num_folds = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-PsMEZKg3FY"
      },
      "outputs": [],
      "source": [
        "# Define per-fold score containers\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "# Merge inputs and targets\n",
        "inputs = np.concatenate((X_train, X_test), axis=0)\n",
        "targets = np.concatenate((y_train, y_test), axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# proposedModel(glove)"
      ],
      "metadata": {
        "id": "f5PAJllchh8I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGLIQGgYg3FZ",
        "outputId": "32921957-f0d8-4626-e062-5cfbdd234079"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/5\n",
            "422/422 [==============================] - 407s 924ms/step - loss: 0.4060 - accuracy: 0.8199\n",
            "Epoch 2/5\n",
            "422/422 [==============================] - 382s 906ms/step - loss: 0.3538 - accuracy: 0.8487\n",
            "Epoch 3/5\n",
            "422/422 [==============================] - 389s 920ms/step - loss: 0.3314 - accuracy: 0.8594\n",
            "Epoch 4/5\n",
            "422/422 [==============================] - 383s 909ms/step - loss: 0.3146 - accuracy: 0.8672\n",
            "Epoch 5/5\n",
            "422/422 [==============================] - 384s 911ms/step - loss: 0.3030 - accuracy: 0.8718\n",
            "INFO:tensorflow:Assets written to: content/models_word2vec/1model/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: content/models_word2vec/1model/assets\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f8fb010dfa0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f8fb0436af0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f8fc814dfd0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f8fc814d0d0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 1: loss of 0.32331371307373047; accuracy of 86.16666793823242%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/5\n",
            "422/422 [==============================] - 405s 920ms/step - loss: 0.4096 - accuracy: 0.8148\n",
            "Epoch 2/5\n",
            "422/422 [==============================] - 388s 920ms/step - loss: 0.3561 - accuracy: 0.8473\n",
            "Epoch 3/5\n",
            "422/422 [==============================] - 386s 915ms/step - loss: 0.3321 - accuracy: 0.8583\n",
            "Epoch 4/5\n",
            "422/422 [==============================] - 386s 915ms/step - loss: 0.3158 - accuracy: 0.8679\n",
            "Epoch 5/5\n",
            "422/422 [==============================] - 387s 917ms/step - loss: 0.3048 - accuracy: 0.8686\n",
            "INFO:tensorflow:Assets written to: content/models_word2vec/2model/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: content/models_word2vec/2model/assets\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f8f5d8d7550> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f8f5d8d78e0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f8fb3e67550> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f8fbafb6970> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 2: loss of 0.30470460653305054; accuracy of 87.8666639328003%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/5\n",
            "422/422 [==============================] - 401s 917ms/step - loss: 0.4045 - accuracy: 0.8175\n",
            "Epoch 2/5\n",
            "422/422 [==============================] - 386s 914ms/step - loss: 0.3561 - accuracy: 0.8483\n",
            "Epoch 3/5\n",
            "422/422 [==============================] - 388s 920ms/step - loss: 0.3305 - accuracy: 0.8583\n",
            "Epoch 4/5\n",
            "422/422 [==============================] - 386s 914ms/step - loss: 0.3157 - accuracy: 0.8673\n",
            "Epoch 5/5\n",
            "422/422 [==============================] - 388s 921ms/step - loss: 0.3051 - accuracy: 0.8694\n",
            "INFO:tensorflow:Assets written to: content/models_word2vec/3model/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: content/models_word2vec/3model/assets\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f8fb3620cd0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f8fbaf32220> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f8fbbf16760> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f8f5ee54b80> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 3: loss of 0.33505740761756897; accuracy of 86.19999885559082%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/5\n",
            "422/422 [==============================] - 403s 917ms/step - loss: 0.4033 - accuracy: 0.8195\n",
            "Epoch 2/5\n",
            "422/422 [==============================] - 386s 915ms/step - loss: 0.3516 - accuracy: 0.8481\n",
            "Epoch 3/5\n",
            "422/422 [==============================] - 389s 922ms/step - loss: 0.3306 - accuracy: 0.8608\n",
            "Epoch 4/5\n",
            "422/422 [==============================] - 387s 916ms/step - loss: 0.3143 - accuracy: 0.8656\n",
            "Epoch 5/5\n",
            "422/422 [==============================] - 389s 921ms/step - loss: 0.3025 - accuracy: 0.8706\n",
            "INFO:tensorflow:Assets written to: content/models_word2vec/4model/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: content/models_word2vec/4model/assets\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f8fbbcb5430> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f8fbbcb5f10> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f8f5dea3190> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f8f5dea3c10> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 4: loss of 0.33538293838500977; accuracy of 85.29999852180481%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/5\n",
            "422/422 [==============================] - 401s 907ms/step - loss: 0.4126 - accuracy: 0.8154\n",
            "Epoch 2/5\n",
            "422/422 [==============================] - 387s 915ms/step - loss: 0.3568 - accuracy: 0.8473\n",
            "Epoch 3/5\n",
            "422/422 [==============================] - 386s 914ms/step - loss: 0.3339 - accuracy: 0.8594\n",
            "Epoch 4/5\n",
            "422/422 [==============================] - 388s 920ms/step - loss: 0.3183 - accuracy: 0.8670\n",
            "Epoch 5/5\n",
            "422/422 [==============================] - 387s 917ms/step - loss: 0.3037 - accuracy: 0.8716\n",
            "INFO:tensorflow:Assets written to: content/models_word2vec/5model/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: content/models_word2vec/5model/assets\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f8fb826a940> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f8fbac4a0d0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f8f5fae50a0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f8fb0e47790> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 0.3097512125968933; accuracy of 87.5333309173584%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 6 ...\n",
            "Epoch 1/5\n",
            "422/422 [==============================] - 400s 910ms/step - loss: 0.4073 - accuracy: 0.8203\n",
            "Epoch 2/5\n",
            "422/422 [==============================] - 386s 914ms/step - loss: 0.3551 - accuracy: 0.8480\n",
            "Epoch 3/5\n",
            "422/422 [==============================] - 386s 915ms/step - loss: 0.3306 - accuracy: 0.8614\n",
            "Epoch 4/5\n",
            "422/422 [==============================] - 386s 914ms/step - loss: 0.3163 - accuracy: 0.8675\n",
            "Epoch 5/5\n",
            "422/422 [==============================] - 387s 917ms/step - loss: 0.3037 - accuracy: 0.8718\n",
            "INFO:tensorflow:Assets written to: content/models_word2vec/6model/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: content/models_word2vec/6model/assets\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f8f5d748a30> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f8f5d748cd0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f8fbb43a490> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f8fbb43abe0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 6: loss of 0.32734572887420654; accuracy of 86.43333315849304%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 7 ...\n",
            "Epoch 1/5\n",
            "422/422 [==============================] - 400s 913ms/step - loss: 0.4108 - accuracy: 0.8154\n",
            "Epoch 2/5\n",
            "422/422 [==============================] - 386s 915ms/step - loss: 0.3546 - accuracy: 0.8474\n",
            "Epoch 3/5\n",
            "422/422 [==============================] - 388s 918ms/step - loss: 0.3346 - accuracy: 0.8585\n",
            "Epoch 4/5\n",
            "422/422 [==============================] - 387s 917ms/step - loss: 0.3185 - accuracy: 0.8671\n",
            "Epoch 5/5\n",
            "422/422 [==============================] - 387s 916ms/step - loss: 0.3004 - accuracy: 0.8763\n",
            "INFO:tensorflow:Assets written to: content/models_word2vec/7model/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: content/models_word2vec/7model/assets\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f8fb966b370> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f8fb97a87c0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f8fb0679d00> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f8fb32eb700> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 7: loss of 0.31838682293891907; accuracy of 87.1999979019165%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 8 ...\n",
            "Epoch 1/5\n",
            "422/422 [==============================] - 407s 928ms/step - loss: 0.4094 - accuracy: 0.8145\n",
            "Epoch 2/5\n",
            "422/422 [==============================] - 390s 924ms/step - loss: 0.3548 - accuracy: 0.8469\n",
            "Epoch 3/5\n",
            "422/422 [==============================] - 391s 925ms/step - loss: 0.3340 - accuracy: 0.8576\n",
            "Epoch 4/5\n",
            "422/422 [==============================] - 389s 922ms/step - loss: 0.3183 - accuracy: 0.8658\n",
            "Epoch 5/5\n",
            "422/422 [==============================] - 389s 921ms/step - loss: 0.3026 - accuracy: 0.8732\n",
            "INFO:tensorflow:Assets written to: content/models_word2vec/8model/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: content/models_word2vec/8model/assets\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f8f5f173250> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f8fbb499490> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f8f5df00250> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f8fc0285130> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 8: loss of 0.3152604103088379; accuracy of 87.1999979019165%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 9 ...\n",
            "Epoch 1/5\n",
            "422/422 [==============================] - 400s 909ms/step - loss: 0.4076 - accuracy: 0.8180\n",
            "Epoch 2/5\n",
            "422/422 [==============================] - 386s 915ms/step - loss: 0.3547 - accuracy: 0.8484\n",
            "Epoch 3/5\n",
            "422/422 [==============================] - 385s 913ms/step - loss: 0.3347 - accuracy: 0.8566\n",
            "Epoch 4/5\n",
            "422/422 [==============================] - 383s 908ms/step - loss: 0.3165 - accuracy: 0.8672\n",
            "Epoch 5/5\n",
            "422/422 [==============================] - 375s 889ms/step - loss: 0.3037 - accuracy: 0.8700\n",
            "INFO:tensorflow:Assets written to: content/models_word2vec/9model/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: content/models_word2vec/9model/assets\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f8fb02f47c0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f8fb02f4790> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f8fbb081b80> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f8fbbf7ed90> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 9: loss of 0.3276544511318207; accuracy of 86.46666407585144%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 10 ...\n",
            "Epoch 1/5\n",
            "422/422 [==============================] - 442s 1s/step - loss: 0.4073 - accuracy: 0.8199\n",
            "Epoch 2/5\n",
            "422/422 [==============================] - 418s 991ms/step - loss: 0.3531 - accuracy: 0.8515\n",
            "Epoch 3/5\n",
            "422/422 [==============================] - 475s 1s/step - loss: 0.3291 - accuracy: 0.8605\n",
            "Epoch 4/5\n",
            "422/422 [==============================] - 421s 997ms/step - loss: 0.3157 - accuracy: 0.8669\n",
            "Epoch 5/5\n",
            "422/422 [==============================] - 372s 883ms/step - loss: 0.3018 - accuracy: 0.8733\n",
            "INFO:tensorflow:Assets written to: content/models_word2vec/10model/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: content/models_word2vec/10model/assets\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f8f5c15ad00> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f8fb2d9f8b0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f8fc0608d60> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f8f61bbe400> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 10: loss of 0.3311815857887268; accuracy of 86.19999885559082%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.3173454701900482 - Accuracy: 86.56666874885559%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.32625967264175415 - Accuracy: 86.56666874885559%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.32167190313339233 - Accuracy: 86.56666874885559%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.32052621245384216 - Accuracy: 86.83333396911621%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.3238447308540344 - Accuracy: 86.76666617393494%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 6 - Loss: 0.319640576839447 - Accuracy: 86.79999709129333%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 7 - Loss: 0.3226678967475891 - Accuracy: 86.2333357334137%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 8 - Loss: 0.3233823776245117 - Accuracy: 86.2333357334137%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 9 - Loss: 0.3447829484939575 - Accuracy: 86.13333106040955%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 10 - Loss: 0.3056170344352722 - Accuracy: 86.63333058357239%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 11 - Loss: 0.33110311627388 - Accuracy: 86.00000143051147%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 12 - Loss: 0.32338619232177734 - Accuracy: 87.1999979019165%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 13 - Loss: 0.3306342661380768 - Accuracy: 86.83333396911621%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 14 - Loss: 0.3305692672729492 - Accuracy: 86.56666874885559%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 15 - Loss: 0.29719942808151245 - Accuracy: 87.33333349227905%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 16 - Loss: 0.3485431671142578 - Accuracy: 84.60000157356262%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 17 - Loss: 0.3160696029663086 - Accuracy: 87.1999979019165%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 18 - Loss: 0.33856335282325745 - Accuracy: 86.79999709129333%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 19 - Loss: 0.34366917610168457 - Accuracy: 85.56666374206543%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 20 - Loss: 0.34393367171287537 - Accuracy: 86.2999975681305%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 21 - Loss: 0.32331371307373047 - Accuracy: 86.16666793823242%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 22 - Loss: 0.30470460653305054 - Accuracy: 87.8666639328003%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 23 - Loss: 0.33505740761756897 - Accuracy: 86.19999885559082%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 24 - Loss: 0.33538293838500977 - Accuracy: 85.29999852180481%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 25 - Loss: 0.3097512125968933 - Accuracy: 87.5333309173584%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 26 - Loss: 0.32734572887420654 - Accuracy: 86.43333315849304%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 27 - Loss: 0.31838682293891907 - Accuracy: 87.1999979019165%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 28 - Loss: 0.3152604103088379 - Accuracy: 87.1999979019165%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 29 - Loss: 0.3276544511318207 - Accuracy: 86.46666407585144%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 30 - Loss: 0.3311815857887268 - Accuracy: 86.19999885559082%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 86.54333273569743 (+- 0.6538852466523671)\n",
            "> Loss: 0.3252482980489731\n",
            "------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "for train, test in kfold.split(inputs, targets):\n",
        "\n",
        "  # Define the model architecture\n",
        "    model=Sequential()\n",
        "    embedding_layer=Embedding(vocab_size,\n",
        "                        EMBEDDING_DIM,\n",
        "                        embeddings_initializer=Constant(embedding_matrix),\n",
        "                        input_length=60,\n",
        "                        trainable=False)  \n",
        "    model.add(embedding_layer)\n",
        "    model.add(Bidirectional(GRU(units=128, return_sequences = True, dropout=0.25, recurrent_dropout=0.25)))\n",
        "    model.add(Bidirectional(GRU(units=128, return_sequences = True, dropout=0.25, recurrent_dropout=0.25)))\n",
        "    model.add(attention())\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dense(1,activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "  # model.summary()\n",
        "\n",
        "  # Generate a print\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  # Fit data to model\n",
        "    history = model.fit(inputs[train], targets[train],\n",
        "              batch_size=batch_size,\n",
        "              epochs=no_epochs,\n",
        "              verbose=verbosity)\n",
        "    model_path = \"content/models_word2vec/\" + str(fold_no) + \"model\"\n",
        "    \n",
        "    # convert the history.history dict to a pandas DataFrame:     \n",
        "    hist_df = pd.DataFrame(history.history) \n",
        "\n",
        "    # save to json:  \n",
        "    hist_json_file = \"content/History_word2vec/\" + str(fold_no) + 'history.json' \n",
        "    with open(hist_json_file, mode='w') as f:\n",
        "        hist_df.to_json(f)\n",
        "            \n",
        "    #save model\n",
        "    model.save(model_path)\n",
        "    \n",
        "  # Generate generalization metrics\n",
        "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
        "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "  # Increase fold number\n",
        "    fold_no = fold_no + 1\n",
        "\n",
        "  # == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bi-GRU(word2vec)"
      ],
      "metadata": {
        "id": "gum115jUhrrO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYE3uv_rg3Fb",
        "outputId": "81902542-c5c1-48e8-81c5-8a9dbc922fbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/5\n",
            "422/422 [==============================] - 164s 368ms/step - loss: 0.4565 - accuracy: 0.7954\n",
            "Epoch 2/5\n",
            "422/422 [==============================] - 163s 387ms/step - loss: 0.3836 - accuracy: 0.8337\n",
            "Epoch 3/5\n",
            "422/422 [==============================] - 158s 375ms/step - loss: 0.3649 - accuracy: 0.8437\n",
            "Epoch 4/5\n",
            "422/422 [==============================] - 161s 381ms/step - loss: 0.3505 - accuracy: 0.8498\n",
            "Epoch 5/5\n",
            "422/422 [==============================] - 151s 358ms/step - loss: 0.3376 - accuracy: 0.8558\n",
            "Score for fold 1: loss of 0.3423159718513489; accuracy of 86.12943291664124%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/5\n",
            "422/422 [==============================] - 153s 342ms/step - loss: 0.4493 - accuracy: 0.7981\n",
            "Epoch 2/5\n",
            "422/422 [==============================] - 144s 341ms/step - loss: 0.3824 - accuracy: 0.8329\n",
            "Epoch 3/5\n",
            "422/422 [==============================] - 145s 343ms/step - loss: 0.3640 - accuracy: 0.8432\n",
            "Epoch 4/5\n",
            "422/422 [==============================] - 143s 340ms/step - loss: 0.3504 - accuracy: 0.8487\n",
            "Epoch 5/5\n",
            "422/422 [==============================] - 143s 340ms/step - loss: 0.3412 - accuracy: 0.8520\n",
            "Score for fold 2: loss of 0.3514702618122101; accuracy of 85.26002764701843%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/5\n",
            "422/422 [==============================] - 150s 340ms/step - loss: 0.4542 - accuracy: 0.7972\n",
            "Epoch 2/5\n",
            "422/422 [==============================] - 143s 340ms/step - loss: 0.3818 - accuracy: 0.8340\n",
            "Epoch 3/5\n",
            "422/422 [==============================] - 143s 340ms/step - loss: 0.3622 - accuracy: 0.8439\n",
            "Epoch 4/5\n",
            "422/422 [==============================] - 143s 338ms/step - loss: 0.3501 - accuracy: 0.8508\n",
            "Epoch 5/5\n",
            "422/422 [==============================] - 142s 336ms/step - loss: 0.3374 - accuracy: 0.8578\n",
            "Score for fold 3: loss of 0.3696215748786926; accuracy of 83.85499119758606%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/5\n",
            "422/422 [==============================] - 149s 336ms/step - loss: 0.4572 - accuracy: 0.7931\n",
            "Epoch 2/5\n",
            "422/422 [==============================] - 142s 335ms/step - loss: 0.3861 - accuracy: 0.8316\n",
            "Epoch 3/5\n",
            "422/422 [==============================] - 143s 338ms/step - loss: 0.3661 - accuracy: 0.8426\n",
            "Epoch 4/5\n",
            "422/422 [==============================] - 142s 336ms/step - loss: 0.3532 - accuracy: 0.8475\n",
            "Epoch 5/5\n",
            "422/422 [==============================] - 143s 339ms/step - loss: 0.3403 - accuracy: 0.8528\n",
            "Score for fold 4: loss of 0.3422115743160248; accuracy of 85.95443367958069%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/5\n",
            "422/422 [==============================] - 152s 342ms/step - loss: 0.4583 - accuracy: 0.7927\n",
            "Epoch 2/5\n",
            "422/422 [==============================] - 145s 342ms/step - loss: 0.3857 - accuracy: 0.8335\n",
            "Epoch 3/5\n",
            "422/422 [==============================] - 145s 344ms/step - loss: 0.3679 - accuracy: 0.8421\n",
            "Epoch 4/5\n",
            "422/422 [==============================] - 145s 343ms/step - loss: 0.3512 - accuracy: 0.8505\n",
            "Epoch 5/5\n",
            "422/422 [==============================] - 144s 342ms/step - loss: 0.3411 - accuracy: 0.8545\n",
            "Score for fold 5: loss of 0.322182297706604; accuracy of 86.31944060325623%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 6 ...\n",
            "Epoch 1/5\n",
            "422/422 [==============================] - 149s 337ms/step - loss: 0.4513 - accuracy: 0.7949\n",
            "Epoch 2/5\n",
            "422/422 [==============================] - 141s 334ms/step - loss: 0.3818 - accuracy: 0.8345\n",
            "Epoch 3/5\n",
            "422/422 [==============================] - 141s 334ms/step - loss: 0.3649 - accuracy: 0.8422\n",
            "Epoch 4/5\n",
            "422/422 [==============================] - 141s 334ms/step - loss: 0.3520 - accuracy: 0.8492\n",
            "Epoch 5/5\n",
            "422/422 [==============================] - 212s 502ms/step - loss: 0.3411 - accuracy: 0.8542\n",
            "Score for fold 6: loss of 0.3379674553871155; accuracy of 85.1255714893341%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 7 ...\n",
            "Epoch 1/5\n",
            "422/422 [==============================] - 285s 640ms/step - loss: 0.4531 - accuracy: 0.7947\n",
            "Epoch 2/5\n",
            "422/422 [==============================] - 270s 640ms/step - loss: 0.3822 - accuracy: 0.8361\n",
            "Epoch 3/5\n",
            "422/422 [==============================] - 270s 641ms/step - loss: 0.3647 - accuracy: 0.8455\n",
            "Epoch 4/5\n",
            "422/422 [==============================] - 270s 640ms/step - loss: 0.3477 - accuracy: 0.8511\n",
            "Epoch 5/5\n",
            "422/422 [==============================] - 270s 640ms/step - loss: 0.3389 - accuracy: 0.8562\n",
            "Score for fold 7: loss of 0.3580697774887085; accuracy of 85.08720993995667%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 8 ...\n",
            "Epoch 1/5\n",
            "422/422 [==============================] - 285s 639ms/step - loss: 0.4569 - accuracy: 0.7931\n",
            "Epoch 2/5\n",
            "422/422 [==============================] - 270s 639ms/step - loss: 0.3867 - accuracy: 0.8326\n",
            "Epoch 3/5\n",
            "422/422 [==============================] - 270s 640ms/step - loss: 0.3657 - accuracy: 0.8429\n",
            "Epoch 4/5\n",
            "422/422 [==============================] - 272s 644ms/step - loss: 0.3532 - accuracy: 0.8494\n",
            "Epoch 5/5\n",
            "422/422 [==============================] - 270s 639ms/step - loss: 0.3423 - accuracy: 0.8535\n",
            "Score for fold 8: loss of 0.35732316970825195; accuracy of 85.19389033317566%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 9 ...\n",
            "Epoch 1/5\n",
            "422/422 [==============================] - 285s 641ms/step - loss: 0.4547 - accuracy: 0.7952\n",
            "Epoch 2/5\n",
            "422/422 [==============================] - 271s 641ms/step - loss: 0.3863 - accuracy: 0.8328\n",
            "Epoch 3/5\n",
            "422/422 [==============================] - 271s 642ms/step - loss: 0.3651 - accuracy: 0.8425\n",
            "Epoch 4/5\n",
            "422/422 [==============================] - 273s 648ms/step - loss: 0.3525 - accuracy: 0.8491\n",
            "Epoch 5/5\n",
            "422/422 [==============================] - 271s 642ms/step - loss: 0.3416 - accuracy: 0.8548\n",
            "Score for fold 9: loss of 0.33815765380859375; accuracy of 85.14779210090637%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 10 ...\n",
            "Epoch 1/5\n",
            "422/422 [==============================] - 287s 642ms/step - loss: 0.4574 - accuracy: 0.7924\n",
            "Epoch 2/5\n",
            "422/422 [==============================] - 271s 641ms/step - loss: 0.3848 - accuracy: 0.8325\n",
            "Epoch 3/5\n",
            "422/422 [==============================] - 271s 643ms/step - loss: 0.3615 - accuracy: 0.8453\n",
            "Epoch 4/5\n",
            "422/422 [==============================] - 272s 643ms/step - loss: 0.3495 - accuracy: 0.8508\n",
            "Epoch 5/5\n",
            "422/422 [==============================] - 271s 643ms/step - loss: 0.3414 - accuracy: 0.8548\n",
            "Score for fold 10: loss of 0.3500238060951233; accuracy of 85.41389107704163%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.3173454701900482 - Accuracy: 86.56666874885559%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.32625967264175415 - Accuracy: 86.56666874885559%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.32167190313339233 - Accuracy: 86.56666874885559%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.32052621245384216 - Accuracy: 86.83333396911621%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.3238447308540344 - Accuracy: 86.76666617393494%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 6 - Loss: 0.319640576839447 - Accuracy: 86.79999709129333%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 7 - Loss: 0.3226678967475891 - Accuracy: 86.2333357334137%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 8 - Loss: 0.3233823776245117 - Accuracy: 86.2333357334137%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 9 - Loss: 0.3447829484939575 - Accuracy: 86.13333106040955%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 10 - Loss: 0.3056170344352722 - Accuracy: 86.63333058357239%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 11 - Loss: 0.33110311627388 - Accuracy: 86.00000143051147%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 12 - Loss: 0.32338619232177734 - Accuracy: 87.1999979019165%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 13 - Loss: 0.3306342661380768 - Accuracy: 86.83333396911621%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 14 - Loss: 0.3305692672729492 - Accuracy: 86.56666874885559%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 15 - Loss: 0.29719942808151245 - Accuracy: 87.33333349227905%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 16 - Loss: 0.3485431671142578 - Accuracy: 84.60000157356262%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 17 - Loss: 0.3160696029663086 - Accuracy: 87.1999979019165%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 18 - Loss: 0.33856335282325745 - Accuracy: 86.79999709129333%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 19 - Loss: 0.34366917610168457 - Accuracy: 85.56666374206543%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 20 - Loss: 0.34393367171287537 - Accuracy: 86.2999975681305%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 21 - Loss: 0.32331371307373047 - Accuracy: 86.16666793823242%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 22 - Loss: 0.30470460653305054 - Accuracy: 87.8666639328003%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 23 - Loss: 0.33505740761756897 - Accuracy: 86.19999885559082%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 24 - Loss: 0.33538293838500977 - Accuracy: 85.29999852180481%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 25 - Loss: 0.3097512125968933 - Accuracy: 87.5333309173584%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 26 - Loss: 0.32734572887420654 - Accuracy: 86.43333315849304%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 27 - Loss: 0.31838682293891907 - Accuracy: 87.1999979019165%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 28 - Loss: 0.3152604103088379 - Accuracy: 87.1999979019165%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 29 - Loss: 0.3276544511318207 - Accuracy: 86.46666407585144%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 30 - Loss: 0.3311815857887268 - Accuracy: 86.19999885559082%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 31 - Loss: 0.3423159718513489 - Accuracy: 86.12943291664124%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 32 - Loss: 0.3514702618122101 - Accuracy: 85.26002764701843%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 33 - Loss: 0.3696215748786926 - Accuracy: 83.85499119758606%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 34 - Loss: 0.3422115743160248 - Accuracy: 85.95443367958069%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 35 - Loss: 0.322182297706604 - Accuracy: 86.31944060325623%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 36 - Loss: 0.3379674553871155 - Accuracy: 85.1255714893341%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 37 - Loss: 0.3580697774887085 - Accuracy: 85.08720993995667%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 38 - Loss: 0.35732316970825195 - Accuracy: 85.19389033317566%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 39 - Loss: 0.33815765380859375 - Accuracy: 85.14779210090637%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 40 - Loss: 0.3500238060951233 - Accuracy: 85.41389107704163%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 86.2446665763855 (+- 0.8348011885466524)\n",
            "> Loss: 0.33066981211304664\n",
            "------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "for train, test in kfold.split(inputs, targets):\n",
        "\n",
        "  # Define the model architecture\n",
        "    model=Sequential()\n",
        "    embedding_layer=Embedding(vocab_size,\n",
        "                        EMBEDDING_DIM,\n",
        "                        embeddings_initializer=Constant(embedding_matrix),\n",
        "                        input_length=60,\n",
        "                        trainable=False)  \n",
        "    model.add(embedding_layer)\n",
        "    model.add(Bidirectional(GRU(units=128, return_sequences = True, dropout=0.25, recurrent_dropout=0.25)))\n",
        "#     model.add(Bidirectional(GRU(units=128, return_sequences = True, dropout=0.25, recurrent_dropout=0.25)))\n",
        "#     model.add(attention())\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dense(1,activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "  # model.summary()\n",
        "\n",
        "  # Generate a print\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  # Fit data to model\n",
        "    history = model.fit(inputs[train], targets[train],\n",
        "              batch_size=batch_size,\n",
        "              epochs=no_epochs,\n",
        "              verbose=verbosity)\n",
        "#     model_path = \"content/models_word2vec/\" + str(fold_no) + \"model\"\n",
        "    \n",
        "#     # convert the history.history dict to a pandas DataFrame:     \n",
        "#     hist_df = pd.DataFrame(history.history) \n",
        "\n",
        "#     # save to json:  \n",
        "#     hist_json_file = \"content/History_word2vec/\" + str(fold_no) + 'history.json' \n",
        "#     with open(hist_json_file, mode='w') as f:\n",
        "#         hist_df.to_json(f)\n",
        "            \n",
        "#     #save model\n",
        "#     model.save(model_path)\n",
        "    \n",
        "  # Generate generalization metrics\n",
        "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
        "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "  # Increase fold number\n",
        "    fold_no = fold_no + 1\n",
        "\n",
        "  # == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bi-LSTM(word2vec)"
      ],
      "metadata": {
        "id": "WQQ_lMBqhx3i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WM6dLiJig3Fc",
        "outputId": "904890d4-5405-460c-be94-e4ebf973e527"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/5\n",
            "422/422 [==============================] - 321s 722ms/step - loss: 0.4529 - accuracy: 0.7921\n",
            "Epoch 2/5\n",
            "422/422 [==============================] - 306s 726ms/step - loss: 0.3916 - accuracy: 0.8279\n",
            "Epoch 3/5\n",
            "422/422 [==============================] - 305s 722ms/step - loss: 0.3707 - accuracy: 0.8399\n",
            "Epoch 4/5\n",
            "422/422 [==============================] - 304s 721ms/step - loss: 0.3527 - accuracy: 0.8484\n",
            "Epoch 5/5\n",
            "422/422 [==============================] - 305s 722ms/step - loss: 0.3396 - accuracy: 0.8518\n",
            "Score for fold 1: loss of 0.34680941700935364; accuracy of 85.84110736846924%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/5\n",
            "422/422 [==============================] - 318s 717ms/step - loss: 0.4493 - accuracy: 0.7971\n",
            "Epoch 2/5\n",
            "422/422 [==============================] - 303s 719ms/step - loss: 0.3912 - accuracy: 0.8294\n",
            "Epoch 3/5\n",
            "422/422 [==============================] - 303s 718ms/step - loss: 0.3664 - accuracy: 0.8405\n",
            "Epoch 4/5\n",
            "422/422 [==============================] - 304s 719ms/step - loss: 0.3560 - accuracy: 0.8478\n",
            "Epoch 5/5\n",
            "422/422 [==============================] - 303s 718ms/step - loss: 0.3392 - accuracy: 0.8541\n",
            "Score for fold 2: loss of 0.3499753773212433; accuracy of 84.37665104866028%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/5\n",
            "422/422 [==============================] - 319s 719ms/step - loss: 0.4463 - accuracy: 0.7972\n",
            "Epoch 2/5\n",
            "422/422 [==============================] - 304s 720ms/step - loss: 0.3939 - accuracy: 0.8286\n",
            "Epoch 3/5\n",
            "422/422 [==============================] - 305s 724ms/step - loss: 0.3722 - accuracy: 0.8374\n",
            "Epoch 4/5\n",
            "422/422 [==============================] - 304s 719ms/step - loss: 0.3544 - accuracy: 0.8482\n",
            "Epoch 5/5\n",
            "422/422 [==============================] - 304s 720ms/step - loss: 0.3397 - accuracy: 0.8549\n",
            "Score for fold 3: loss of 0.35822880268096924; accuracy of 85.34055948257446%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/5\n",
            "422/422 [==============================] - 327s 736ms/step - loss: 0.4463 - accuracy: 0.7986\n",
            "Epoch 2/5\n",
            "422/422 [==============================] - 307s 727ms/step - loss: 0.3894 - accuracy: 0.8279\n",
            "Epoch 3/5\n",
            "422/422 [==============================] - 305s 723ms/step - loss: 0.3675 - accuracy: 0.8404\n",
            "Epoch 4/5\n",
            "422/422 [==============================] - 305s 722ms/step - loss: 0.3539 - accuracy: 0.8451\n",
            "Epoch 5/5\n",
            "422/422 [==============================] - 305s 722ms/step - loss: 0.3396 - accuracy: 0.8538\n",
            "Score for fold 4: loss of 0.34422558546066284; accuracy of 85.30832529067993%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/5\n",
            "422/422 [==============================] - 321s 725ms/step - loss: 0.4422 - accuracy: 0.8009\n",
            "Epoch 2/5\n",
            "422/422 [==============================] - 306s 725ms/step - loss: 0.3929 - accuracy: 0.8273\n",
            "Epoch 3/5\n",
            "422/422 [==============================] - 306s 725ms/step - loss: 0.3704 - accuracy: 0.8379\n",
            "Epoch 4/5\n",
            "422/422 [==============================] - 314s 745ms/step - loss: 0.3548 - accuracy: 0.8467\n",
            "Epoch 5/5\n",
            "422/422 [==============================] - 304s 719ms/step - loss: 0.3387 - accuracy: 0.8569\n",
            "Score for fold 5: loss of 0.3368319571018219; accuracy of 85.36500930786133%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 6 ...\n",
            "Epoch 1/5\n",
            "422/422 [==============================] - 323s 725ms/step - loss: 0.4433 - accuracy: 0.8014\n",
            "Epoch 2/5\n",
            "422/422 [==============================] - 304s 720ms/step - loss: 0.3869 - accuracy: 0.8316\n",
            "Epoch 3/5\n",
            "422/422 [==============================] - 304s 719ms/step - loss: 0.3656 - accuracy: 0.8410\n",
            "Epoch 4/5\n",
            "422/422 [==============================] - 235s 556ms/step - loss: 0.3501 - accuracy: 0.8491\n",
            "Epoch 5/5\n",
            "422/422 [==============================] - 216s 512ms/step - loss: 0.3377 - accuracy: 0.8549\n",
            "Score for fold 6: loss of 0.3653387427330017; accuracy of 85.32109260559082%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 7 ...\n",
            "Epoch 1/5\n",
            "422/422 [==============================] - 205s 462ms/step - loss: 0.4415 - accuracy: 0.8027\n",
            "Epoch 2/5\n",
            "422/422 [==============================] - 181s 429ms/step - loss: 0.3865 - accuracy: 0.8325\n",
            "Epoch 3/5\n",
            "422/422 [==============================] - 167s 396ms/step - loss: 0.3693 - accuracy: 0.8414\n",
            "Epoch 4/5\n",
            "422/422 [==============================] - 177s 418ms/step - loss: 0.3535 - accuracy: 0.8474\n",
            "Epoch 5/5\n",
            "422/422 [==============================] - 163s 386ms/step - loss: 0.3379 - accuracy: 0.8556\n",
            "Score for fold 7: loss of 0.3632947504520416; accuracy of 84.256112575531%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 8 ...\n",
            "Epoch 1/5\n",
            "422/422 [==============================] - 168s 379ms/step - loss: 0.4423 - accuracy: 0.8015\n",
            "Epoch 2/5\n",
            "422/422 [==============================] - 164s 388ms/step - loss: 0.3838 - accuracy: 0.8313\n",
            "Epoch 3/5\n",
            "422/422 [==============================] - 162s 384ms/step - loss: 0.3636 - accuracy: 0.8420\n",
            "Epoch 4/5\n",
            "422/422 [==============================] - 159s 376ms/step - loss: 0.3484 - accuracy: 0.8495\n",
            "Epoch 5/5\n",
            "422/422 [==============================] - 159s 376ms/step - loss: 0.3344 - accuracy: 0.8562\n",
            "Score for fold 8: loss of 0.3792581856250763; accuracy of 84.25887823104858%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 9 ...\n",
            "Epoch 1/5\n",
            "422/422 [==============================] - 169s 381ms/step - loss: 0.4490 - accuracy: 0.7979\n",
            "Epoch 2/5\n",
            "422/422 [==============================] - 160s 379ms/step - loss: 0.3927 - accuracy: 0.8273\n",
            "Epoch 3/5\n",
            "422/422 [==============================] - 160s 379ms/step - loss: 0.3677 - accuracy: 0.8393\n",
            "Epoch 4/5\n",
            "422/422 [==============================] - 160s 378ms/step - loss: 0.3511 - accuracy: 0.8487\n",
            "Epoch 5/5\n",
            "422/422 [==============================] - 160s 380ms/step - loss: 0.3387 - accuracy: 0.8547\n",
            "Score for fold 9: loss of 0.3489225208759308; accuracy of 85.45111417770386%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 10 ...\n",
            "Epoch 1/5\n",
            "422/422 [==============================] - 167s 377ms/step - loss: 0.4438 - accuracy: 0.7978\n",
            "Epoch 2/5\n",
            "422/422 [==============================] - 159s 378ms/step - loss: 0.3877 - accuracy: 0.8297\n",
            "Epoch 3/5\n",
            "422/422 [==============================] - 161s 381ms/step - loss: 0.3667 - accuracy: 0.8409\n",
            "Epoch 4/5\n",
            "422/422 [==============================] - 175s 415ms/step - loss: 0.3513 - accuracy: 0.8477\n",
            "Epoch 5/5\n",
            "422/422 [==============================] - 181s 430ms/step - loss: 0.3387 - accuracy: 0.8561\n",
            "Score for fold 10: loss of 0.3610544800758362; accuracy of 84.80667471885681%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.3173454701900482 - Accuracy: 86.56666874885559%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.32625967264175415 - Accuracy: 86.56666874885559%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.32167190313339233 - Accuracy: 86.56666874885559%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.32052621245384216 - Accuracy: 86.83333396911621%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.3238447308540344 - Accuracy: 86.76666617393494%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 6 - Loss: 0.319640576839447 - Accuracy: 86.79999709129333%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 7 - Loss: 0.3226678967475891 - Accuracy: 86.2333357334137%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 8 - Loss: 0.3233823776245117 - Accuracy: 86.2333357334137%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 9 - Loss: 0.3447829484939575 - Accuracy: 86.13333106040955%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 10 - Loss: 0.3056170344352722 - Accuracy: 86.63333058357239%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 11 - Loss: 0.33110311627388 - Accuracy: 86.00000143051147%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 12 - Loss: 0.32338619232177734 - Accuracy: 87.1999979019165%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 13 - Loss: 0.3306342661380768 - Accuracy: 86.83333396911621%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 14 - Loss: 0.3305692672729492 - Accuracy: 86.56666874885559%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 15 - Loss: 0.29719942808151245 - Accuracy: 87.33333349227905%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 16 - Loss: 0.3485431671142578 - Accuracy: 84.60000157356262%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 17 - Loss: 0.3160696029663086 - Accuracy: 87.1999979019165%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 18 - Loss: 0.33856335282325745 - Accuracy: 86.79999709129333%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 19 - Loss: 0.34366917610168457 - Accuracy: 85.56666374206543%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 20 - Loss: 0.34393367171287537 - Accuracy: 86.2999975681305%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 21 - Loss: 0.32331371307373047 - Accuracy: 86.16666793823242%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 22 - Loss: 0.30470460653305054 - Accuracy: 87.8666639328003%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 23 - Loss: 0.33505740761756897 - Accuracy: 86.19999885559082%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 24 - Loss: 0.33538293838500977 - Accuracy: 85.29999852180481%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 25 - Loss: 0.3097512125968933 - Accuracy: 87.5333309173584%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 26 - Loss: 0.32734572887420654 - Accuracy: 86.43333315849304%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 27 - Loss: 0.31838682293891907 - Accuracy: 87.1999979019165%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 28 - Loss: 0.3152604103088379 - Accuracy: 87.1999979019165%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 29 - Loss: 0.3276544511318207 - Accuracy: 86.46666407585144%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 30 - Loss: 0.3311815857887268 - Accuracy: 86.19999885559082%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 31 - Loss: 0.3423159718513489 - Accuracy: 86.12943291664124%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 32 - Loss: 0.3514702618122101 - Accuracy: 85.26002764701843%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 33 - Loss: 0.3696215748786926 - Accuracy: 83.85499119758606%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 34 - Loss: 0.3422115743160248 - Accuracy: 85.95443367958069%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 35 - Loss: 0.322182297706604 - Accuracy: 86.31944060325623%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 36 - Loss: 0.3379674553871155 - Accuracy: 85.1255714893341%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 37 - Loss: 0.3580697774887085 - Accuracy: 85.08720993995667%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 38 - Loss: 0.35732316970825195 - Accuracy: 85.19389033317566%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 39 - Loss: 0.33815765380859375 - Accuracy: 85.14779210090637%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 40 - Loss: 0.3500238060951233 - Accuracy: 85.41389107704163%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 41 - Loss: 0.34680941700935364 - Accuracy: 85.84110736846924%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 42 - Loss: 0.3499753773212433 - Accuracy: 84.37665104866028%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 43 - Loss: 0.35822880268096924 - Accuracy: 85.34055948257446%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 44 - Loss: 0.34422558546066284 - Accuracy: 85.30832529067993%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 45 - Loss: 0.3368319571018219 - Accuracy: 85.36500930786133%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 46 - Loss: 0.3653387427330017 - Accuracy: 85.32109260559082%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 47 - Loss: 0.3632947504520416 - Accuracy: 84.256112575531%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 48 - Loss: 0.3792581856250763 - Accuracy: 84.25887823104858%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 49 - Loss: 0.3489225208759308 - Accuracy: 85.45111417770386%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 50 - Loss: 0.3610544800758362 - Accuracy: 84.80667471885681%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 86.00224375724792 (+- 0.9220173604515025)\n",
            "> Loss: 0.3356146460771561\n",
            "------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "for train, test in kfold.split(inputs, targets):\n",
        "\n",
        "  # Define the model architecture\n",
        "    model=Sequential()\n",
        "    embedding_layer=Embedding(vocab_size,\n",
        "                        EMBEDDING_DIM,\n",
        "                        embeddings_initializer=Constant(embedding_matrix),\n",
        "                        input_length=60,\n",
        "                        trainable=False)  \n",
        "    model.add(embedding_layer)\n",
        "    model.add(Bidirectional(LSTM(units=128, return_sequences = True, dropout=0.25, recurrent_dropout=0.25)))\n",
        "#     model.add(Bidirectional(GRU(units=128, return_sequences = True, dropout=0.25, recurrent_dropout=0.25)))\n",
        "#     model.add(attention())\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dense(1,activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "  # model.summary()\n",
        "\n",
        "  # Generate a print\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  # Fit data to model\n",
        "    history = model.fit(inputs[train], targets[train],\n",
        "              batch_size=batch_size,\n",
        "              epochs=no_epochs,\n",
        "              verbose=verbosity)\n",
        "#     model_path = \"content/models_word2vec/\" + str(fold_no) + \"model\"\n",
        "    \n",
        "#     # convert the history.history dict to a pandas DataFrame:     \n",
        "#     hist_df = pd.DataFrame(history.history) \n",
        "\n",
        "#     # save to json:  \n",
        "#     hist_json_file = \"content/History_word2vec/\" + str(fold_no) + 'history.json' \n",
        "#     with open(hist_json_file, mode='w') as f:\n",
        "#         hist_df.to_json(f)\n",
        "            \n",
        "#     #save model\n",
        "#     model.save(model_path)\n",
        "    \n",
        "  # Generate generalization metrics\n",
        "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
        "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "  # Increase fold number\n",
        "    fold_no = fold_no + 1\n",
        "\n",
        "  # == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "k_fold_proposed_model_Bangla.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}